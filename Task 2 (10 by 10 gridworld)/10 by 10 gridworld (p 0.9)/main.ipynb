{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b05e71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Omega-automaton states (including the trap state): 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<csrl.oaa.oaa at 0x28edcbe7c40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 0\n",
      "Transition function: [\n",
      "  {(): 0, ('a',): 1, ('b',): 0, ('c',): 2, ('a', 'b'): 0, ('a', 'c'): 2, ('b', 'c'): 2, ('a', 'b', 'c'): 2},\n",
      "  {(): 1, ('a',): 1, ('b',): 0, ('c',): 2, ('a', 'b'): 0, ('a', 'c'): 2, ('b', 'c'): 2, ('a', 'b', 'c'): 2},\n",
      "  {(): 2, ('a',): 2, ('b',): 2, ('c',): 2, ('a', 'b'): 2, ('a', 'c'): 2, ('b', 'c'): 2, ('a', 'b', 'c'): 2}\n",
      "]\n",
      "Acceptance: [\n",
      "  {(): [None], ('a',): [True], ('b',): [None], ('c',): [None], ('a', 'b'): [None], ('a', 'c'): [None], ('b', 'c'): [None], ('a', 'b', 'c'): [None]},\n",
      "  {(): [None], ('a',): [None], ('b',): [True], ('c',): [None], ('a', 'b'): [None], ('a', 'c'): [None], ('b', 'c'): [None], ('a', 'b', 'c'): [None]},\n",
      "  {(): [None], ('a',): [None], ('b',): [None], ('c',): [None], ('a', 'b'): [None], ('a', 'c'): [None], ('b', 'c'): [None], ('a', 'b', 'c'): [None]}\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import manually defined automata\n",
    "%matplotlib inline\n",
    "from csrl.pomdp import GridPOMDP\n",
    "from csrl.oaa import oaa\n",
    "from csrl import ControlSynthesis\n",
    "import numpy as np \n",
    "\n",
    "oa=oaa()\n",
    "\n",
    "# LTL Specification\n",
    "#ltl = 'GF(a & Fb) & G!c' ### goes to 'a' then 'b' recurrently, gallobly ! c\n",
    "print('Number of Omega-automaton states (including the trap state):',oa.shape[1])\n",
    "display(oa)\n",
    "\n",
    "print('Initial state:',oa.q0)\n",
    "print('Transition function: ['),print(*['  '+str(t) for t in oa.delta],sep=',\\n'),print(']')\n",
    "print('Acceptance: ['),print(*['  '+str(t) for t in oa.acc],sep=',\\n'),print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bb76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JUNCHA~1\\AppData\\Local\\Temp/ipykernel_8712/1065089555.py:29: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ],dtype=np.object)\n"
     ]
    }
   ],
   "source": [
    "# POMDP Description\n",
    "shape = (10,10)\n",
    "# E: Empty, T: Trap, B: Obstacle\n",
    "structure = np.array([\n",
    "['B',  'B',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['B',  'B',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'B',  'B'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'B',  'B']\n",
    "])\n",
    "\n",
    "# Labels of the states\n",
    "label = np.array([\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       ('b',),       ('b',)],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       ('b',),       ('b',)],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       ('c',),       ('c',),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       ('c',),       ('c',),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[('a',),       ('a',),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[('a',),       ('a',),       (),       (),       (),       (),       (),       (),       (),       ()]\n",
    "],dtype=np.object)\n",
    "# Colors of the labels\n",
    "lcmap={\n",
    "    ('a',):'lightgreen',\n",
    "    ('b',):'lightgreen',\n",
    "    ('c',):'pink'\n",
    "}\n",
    "grid_pomdp = GridPOMDP(shape=shape,structure=structure,label=label,lcmap=lcmap,figsize=5)  # Use figsize=4 for smaller figures\n",
    "\n",
    "# Construct the product MDP\n",
    "csrl = ControlSynthesis(grid_pomdp,oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6820a7e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csrl.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1396d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])]],\n",
       "\n",
       "        [[list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])]],\n",
       "\n",
       "        [[list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])],\n",
       "         [list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3]), list([0, 1, 2, 3]), list([0, 1, 2, 3]),\n",
       "          list([0, 1, 2, 3])]]]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csrl.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee8ea50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START state: (0, 0, 9, 5)\n",
      "episode: 0/15000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/15000, steps: 180, e: 1.0\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2/15000, steps: 265, e: 1.0\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3/15000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4/15000, steps: 182, e: 1.0\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5/15000, steps: 180, e: 1.0\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6/15000, steps: 52, e: 1.0\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 7/15000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8/15000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9/15000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10/15000, steps: 83, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 11/15000, steps: 120, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12/15000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 13/15000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14/15000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 15/15000, steps: 68, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 16/15000, steps: 96, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 17/15000, steps: 52, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 18/15000, steps: 88, e: 0.99\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 19/15000, steps: 113, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 20/15000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 21/15000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 22/15000, steps: 218, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 23/15000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 24/15000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 25/15000, steps: 58, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 26/15000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 27/15000, steps: 126, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 28/15000, steps: 143, e: 0.98\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 29/15000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 30/15000, steps: 52, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 31/15000, steps: 180, e: 0.98\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 32/15000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 33/15000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 34/15000, steps: 65, e: 0.97\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 35/15000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 36/15000, steps: 78, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 37/15000, steps: 282, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 38/15000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 39/15000, steps: 92, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 40/15000, steps: 69, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 41/15000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 42/15000, steps: 108, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 43/15000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 44/15000, steps: 52, e: 0.97\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 45/15000, steps: 121, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 46/15000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 47/15000, steps: 86, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 48/15000, steps: 100, e: 0.96\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 49/15000, steps: 60, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 50/15000, steps: 224, e: 0.96\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 51/15000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 52/15000, steps: 387, e: 0.96\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 53/15000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 54/15000, steps: 129, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 55/15000, steps: 74, e: 0.96\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 56/15000, steps: 61, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 57/15000, steps: 52, e: 0.96\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 58/15000, steps: 224, e: 0.95\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 59/15000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 60/15000, steps: 66, e: 0.95\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 61/15000, steps: 96, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 62/15000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 63/15000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 64/15000, steps: 193, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 65/15000, steps: 79, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 66/15000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 67/15000, steps: 66, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 68/15000, steps: 56, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 69/15000, steps: 52, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 70/15000, steps: 577, e: 0.95\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 71/15000, steps: 93, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 72/15000, steps: 186, e: 0.94\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 73/15000, steps: 208, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 74/15000, steps: 57, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 75/15000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 76/15000, steps: 193, e: 0.94\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 77/15000, steps: 81, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 78/15000, steps: 509, e: 0.94\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 79/15000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 80/15000, steps: 56, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 81/15000, steps: 188, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 82/15000, steps: 58, e: 0.94\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 83/15000, steps: 52, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 84/15000, steps: 70, e: 0.94\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 85/15000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 86/15000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 87/15000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 88/15000, steps: 129, e: 0.93\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 89/15000, steps: 76, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 90/15000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 91/15000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 92/15000, steps: 152, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 93/15000, steps: 112, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 94/15000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 95/15000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 96/15000, steps: 144, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 97/15000, steps: 52, e: 0.93\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 98/15000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 99/15000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 100/15000, steps: 84, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 101/15000, steps: 84, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 102/15000, steps: 188, e: 0.92\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 103/15000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 104/15000, steps: 72, e: 0.92\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 105/15000, steps: 220, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 106/15000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 107/15000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 108/15000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 109/15000, steps: 59, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 110/15000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 111/15000, steps: 52, e: 0.92\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 112/15000, steps: 77, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 113/15000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 114/15000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 115/15000, steps: 275, e: 0.91\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 116/15000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 117/15000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 118/15000, steps: 178, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 119/15000, steps: 169, e: 0.91\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 120/15000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 121/15000, steps: 226, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 122/15000, steps: 147, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 123/15000, steps: 52, e: 0.91\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 124/15000, steps: 224, e: 0.91\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 125/15000, steps: 228, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 126/15000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 127/15000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 128/15000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 129/15000, steps: 265, e: 0.9\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 130/15000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 131/15000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 132/15000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 133/15000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 134/15000, steps: 90, e: 0.9\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 135/15000, steps: 52, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 136/15000, steps: 229, e: 0.9\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 137/15000, steps: 85, e: 0.9\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 138/15000, steps: 239, e: 0.9\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 139/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 140/15000, steps: 71, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 141/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 142/15000, steps: 227, e: 0.89\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 143/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 144/15000, steps: 57, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 145/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 146/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 147/15000, steps: 128, e: 0.89\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 148/15000, steps: 200, e: 0.89\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 149/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 150/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 151/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 152/15000, steps: 52, e: 0.89\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 153/15000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 154/15000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 155/15000, steps: 105, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 156/15000, steps: 145, e: 0.88\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 157/15000, steps: 81, e: 0.88\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 158/15000, steps: 283, e: 0.88\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 159/15000, steps: 373, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 160/15000, steps: 133, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 161/15000, steps: 178, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 162/15000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 163/15000, steps: 215, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 164/15000, steps: 446, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 165/15000, steps: 52, e: 0.88\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 166/15000, steps: 90, e: 0.88\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 167/15000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 168/15000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 169/15000, steps: 62, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 170/15000, steps: 199, e: 0.87\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 171/15000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 172/15000, steps: 90, e: 0.87\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 173/15000, steps: 161, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 174/15000, steps: 61, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 175/15000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 176/15000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 177/15000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 178/15000, steps: 168, e: 0.87\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 179/15000, steps: 79, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 180/15000, steps: 52, e: 0.87\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 181/15000, steps: 85, e: 0.87\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 182/15000, steps: 66, e: 0.86\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 183/15000, steps: 256, e: 0.86\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 184/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 185/15000, steps: 263, e: 0.86\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 186/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 187/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 188/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 189/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 190/15000, steps: 58, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 191/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 192/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 193/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 194/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 195/15000, steps: 52, e: 0.86\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 196/15000, steps: 74, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 197/15000, steps: 173, e: 0.85\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 198/15000, steps: 55, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 199/15000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 200/15000, steps: 219, e: 0.85\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 201/15000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 202/15000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 203/15000, steps: 410, e: 0.85\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 204/15000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 205/15000, steps: 93, e: 0.85\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 206/15000, steps: 396, e: 0.85\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 207/15000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 208/15000, steps: 135, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 209/15000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 210/15000, steps: 52, e: 0.85\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 211/15000, steps: 98, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 212/15000, steps: 66, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 213/15000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 214/15000, steps: 162, e: 0.84\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 215/15000, steps: 142, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 216/15000, steps: 181, e: 0.84\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 217/15000, steps: 111, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 218/15000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 219/15000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 220/15000, steps: 101, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 221/15000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 222/15000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 223/15000, steps: 214, e: 0.84\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 224/15000, steps: 52, e: 0.84\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 225/15000, steps: 76, e: 0.84\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 226/15000, steps: 59, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 227/15000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 228/15000, steps: 135, e: 0.83\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 229/15000, steps: 252, e: 0.83\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 230/15000, steps: 111, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 231/15000, steps: 119, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 232/15000, steps: 87, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 233/15000, steps: 138, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 234/15000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 235/15000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 236/15000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 237/15000, steps: 136, e: 0.83\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 238/15000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 239/15000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 240/15000, steps: 52, e: 0.83\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 241/15000, steps: 559, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 242/15000, steps: 61, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 243/15000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 244/15000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 245/15000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 246/15000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 247/15000, steps: 91, e: 0.82\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 248/15000, steps: 80, e: 0.82\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 249/15000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 250/15000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 251/15000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 252/15000, steps: 212, e: 0.82\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 253/15000, steps: 52, e: 0.82\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 254/15000, steps: 118, e: 0.82\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 255/15000, steps: 112, e: 0.82\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 256/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 257/15000, steps: 60, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 258/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 259/15000, steps: 211, e: 0.81\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 260/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 261/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 262/15000, steps: 96, e: 0.81\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 263/15000, steps: 66, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 264/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 265/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 266/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 267/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 268/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 269/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 270/15000, steps: 239, e: 0.81\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 271/15000, steps: 52, e: 0.81\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 272/15000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 273/15000, steps: 213, e: 0.8\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 274/15000, steps: 329, e: 0.8\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 275/15000, steps: 54, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 276/15000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 277/15000, steps: 84, e: 0.8\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 278/15000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 279/15000, steps: 52, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 280/15000, steps: 151, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 281/15000, steps: 357, e: 0.8\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 282/15000, steps: 174, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 283/15000, steps: 192, e: 0.8\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 284/15000, steps: 198, e: 0.8\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 285/15000, steps: 149, e: 0.8\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 286/15000, steps: 335, e: 0.8\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 287/15000, steps: 323, e: 0.79\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 288/15000, steps: 169, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 289/15000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 290/15000, steps: 365, e: 0.79\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 291/15000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 292/15000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 293/15000, steps: 508, e: 0.79\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 294/15000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 295/15000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 296/15000, steps: 61, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 297/15000, steps: 68, e: 0.79\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 298/15000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 299/15000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 300/15000, steps: 76, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 301/15000, steps: 52, e: 0.79\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 302/15000, steps: 93, e: 0.79\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 303/15000, steps: 99, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 304/15000, steps: 355, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 305/15000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 306/15000, steps: 268, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 307/15000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 308/15000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 309/15000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 310/15000, steps: 121, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 311/15000, steps: 59, e: 0.78\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 312/15000, steps: 58, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 313/15000, steps: 600, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 314/15000, steps: 52, e: 0.78\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 315/15000, steps: 106, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 316/15000, steps: 208, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 317/15000, steps: 91, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 318/15000, steps: 139, e: 0.78\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 319/15000, steps: 416, e: 0.77\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 320/15000, steps: 274, e: 0.77\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 321/15000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 322/15000, steps: 600, e: 0.77\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 323/15000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 324/15000, steps: 82, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 325/15000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 326/15000, steps: 239, e: 0.77\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 327/15000, steps: 182, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 328/15000, steps: 365, e: 0.77\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 329/15000, steps: 71, e: 0.77\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 330/15000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 331/15000, steps: 63, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 332/15000, steps: 66, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 333/15000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 334/15000, steps: 52, e: 0.77\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 335/15000, steps: 204, e: 0.76\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 336/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 337/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 338/15000, steps: 147, e: 0.76\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 339/15000, steps: 224, e: 0.76\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 340/15000, steps: 439, e: 0.76\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 341/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 342/15000, steps: 79, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 343/15000, steps: 255, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 344/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 345/15000, steps: 412, e: 0.76\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 346/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 347/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 348/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 349/15000, steps: 213, e: 0.76\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 350/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 351/15000, steps: 52, e: 0.76\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 352/15000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 353/15000, steps: 316, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 354/15000, steps: 98, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 355/15000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 356/15000, steps: 600, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 357/15000, steps: 283, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 358/15000, steps: 347, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 359/15000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 360/15000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 361/15000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 362/15000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 363/15000, steps: 381, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 364/15000, steps: 307, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 365/15000, steps: 589, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 366/15000, steps: 52, e: 0.75\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 367/15000, steps: 221, e: 0.75\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 368/15000, steps: 309, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 369/15000, steps: 253, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 370/15000, steps: 515, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 371/15000, steps: 95, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 372/15000, steps: 251, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 373/15000, steps: 143, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 374/15000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 375/15000, steps: 283, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 376/15000, steps: 148, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 377/15000, steps: 72, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 378/15000, steps: 122, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 379/15000, steps: 212, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 380/15000, steps: 166, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 381/15000, steps: 52, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 382/15000, steps: 416, e: 0.74\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 383/15000, steps: 233, e: 0.74\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 384/15000, steps: 204, e: 0.74\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 385/15000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 386/15000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 387/15000, steps: 335, e: 0.73\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 388/15000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 389/15000, steps: 162, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 390/15000, steps: 190, e: 0.73\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 391/15000, steps: 463, e: 0.73\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 392/15000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 393/15000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 394/15000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 395/15000, steps: 55, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 396/15000, steps: 56, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 397/15000, steps: 365, e: 0.73\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 398/15000, steps: 99, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 399/15000, steps: 52, e: 0.73\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 400/15000, steps: 600, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 401/15000, steps: 78, e: 0.73\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 402/15000, steps: 73, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 403/15000, steps: 74, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 404/15000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 405/15000, steps: 131, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 406/15000, steps: 268, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 407/15000, steps: 284, e: 0.72\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 408/15000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 409/15000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 410/15000, steps: 64, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 411/15000, steps: 320, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 412/15000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 413/15000, steps: 278, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 414/15000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 415/15000, steps: 537, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 416/15000, steps: 77, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 417/15000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 418/15000, steps: 452, e: 0.72\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 419/15000, steps: 52, e: 0.72\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 420/15000, steps: 133, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 421/15000, steps: 149, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 422/15000, steps: 117, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 423/15000, steps: 173, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 424/15000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 425/15000, steps: 68, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 426/15000, steps: 126, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 427/15000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 428/15000, steps: 66, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 429/15000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 430/15000, steps: 214, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 431/15000, steps: 99, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 432/15000, steps: 135, e: 0.71\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 433/15000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 434/15000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 435/15000, steps: 52, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 436/15000, steps: 485, e: 0.71\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 437/15000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 438/15000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 439/15000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 440/15000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 441/15000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 442/15000, steps: 86, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 443/15000, steps: 287, e: 0.7\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 444/15000, steps: 111, e: 0.7\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 445/15000, steps: 600, e: 0.7\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 446/15000, steps: 286, e: 0.7\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 447/15000, steps: 80, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 448/15000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 449/15000, steps: 95, e: 0.7\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 450/15000, steps: 52, e: 0.7\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 451/15000, steps: 513, e: 0.7\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 452/15000, steps: 239, e: 0.7\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 453/15000, steps: 94, e: 0.7\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 454/15000, steps: 367, e: 0.7\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 455/15000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 456/15000, steps: 56, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 457/15000, steps: 90, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 458/15000, steps: 75, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 459/15000, steps: 137, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 460/15000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 461/15000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 462/15000, steps: 97, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 463/15000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 464/15000, steps: 85, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 465/15000, steps: 84, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 466/15000, steps: 130, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 467/15000, steps: 122, e: 0.69\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 468/15000, steps: 520, e: 0.69\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 469/15000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 470/15000, steps: 580, e: 0.69\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 471/15000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 472/15000, steps: 52, e: 0.69\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 473/15000, steps: 278, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 474/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 475/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 476/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 477/15000, steps: 104, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 478/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 479/15000, steps: 600, e: 0.68\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 480/15000, steps: 183, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 481/15000, steps: 72, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 482/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 483/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 484/15000, steps: 295, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 485/15000, steps: 78, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 486/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 487/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 488/15000, steps: 584, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 489/15000, steps: 65, e: 0.68\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 490/15000, steps: 85, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 491/15000, steps: 52, e: 0.68\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 492/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 493/15000, steps: 103, e: 0.67\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 494/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 495/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 496/15000, steps: 56, e: 0.67\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 497/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 498/15000, steps: 148, e: 0.67\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 499/15000, steps: 75, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 500/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 501/15000, steps: 208, e: 0.67\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 502/15000, steps: 192, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 503/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 504/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 505/15000, steps: 423, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 506/15000, steps: 422, e: 0.67\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 507/15000, steps: 361, e: 0.67\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 508/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 509/15000, steps: 52, e: 0.67\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 510/15000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 511/15000, steps: 131, e: 0.66\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 512/15000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 513/15000, steps: 74, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 514/15000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 515/15000, steps: 591, e: 0.66\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 516/15000, steps: 150, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 517/15000, steps: 216, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 518/15000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 519/15000, steps: 221, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 520/15000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 521/15000, steps: 209, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 522/15000, steps: 505, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 523/15000, steps: 210, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 524/15000, steps: 372, e: 0.66\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 525/15000, steps: 172, e: 0.66\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 526/15000, steps: 52, e: 0.66\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 527/15000, steps: 411, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 528/15000, steps: 84, e: 0.66\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 529/15000, steps: 61, e: 0.65\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 530/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 531/15000, steps: 279, e: 0.65\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 532/15000, steps: 74, e: 0.65\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 533/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 534/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 535/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 536/15000, steps: 486, e: 0.65\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 537/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 538/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 539/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 540/15000, steps: 600, e: 0.65\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 541/15000, steps: 600, e: 0.65\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 542/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 543/15000, steps: 187, e: 0.65\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 544/15000, steps: 199, e: 0.65\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 545/15000, steps: 285, e: 0.65\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 546/15000, steps: 406, e: 0.65\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 547/15000, steps: 96, e: 0.65\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 548/15000, steps: 52, e: 0.65\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 549/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 550/15000, steps: 89, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 551/15000, steps: 306, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 552/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 553/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 554/15000, steps: 161, e: 0.64\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 555/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 556/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 557/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 558/15000, steps: 119, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 559/15000, steps: 54, e: 0.64\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 560/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 561/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 562/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 563/15000, steps: 427, e: 0.64\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 564/15000, steps: 64, e: 0.64\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 565/15000, steps: 64, e: 0.64\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 566/15000, steps: 52, e: 0.64\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 567/15000, steps: 276, e: 0.64\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 568/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 569/15000, steps: 77, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 570/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 571/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 572/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 573/15000, steps: 96, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 574/15000, steps: 86, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 575/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 576/15000, steps: 147, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 577/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 578/15000, steps: 138, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 579/15000, steps: 76, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 580/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 581/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 582/15000, steps: 194, e: 0.63\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 583/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 584/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 585/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 586/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 587/15000, steps: 52, e: 0.63\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 588/15000, steps: 91, e: 0.62\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 589/15000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 590/15000, steps: 600, e: 0.62\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 591/15000, steps: 375, e: 0.62\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 592/15000, steps: 463, e: 0.62\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 593/15000, steps: 159, e: 0.62\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 594/15000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 595/15000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 596/15000, steps: 600, e: 0.62\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 597/15000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 598/15000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 599/15000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 600/15000, steps: 102, e: 0.62\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 601/15000, steps: 268, e: 0.62\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 602/15000, steps: 600, e: 0.62\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 603/15000, steps: 221, e: 0.62\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 604/15000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 605/15000, steps: 299, e: 0.62\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 606/15000, steps: 463, e: 0.62\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 607/15000, steps: 52, e: 0.62\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 608/15000, steps: 399, e: 0.61\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 609/15000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 610/15000, steps: 206, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 611/15000, steps: 64, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 612/15000, steps: 185, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 613/15000, steps: 248, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 614/15000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 615/15000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 616/15000, steps: 600, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 617/15000, steps: 600, e: 0.61\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 618/15000, steps: 449, e: 0.61\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 619/15000, steps: 457, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 620/15000, steps: 308, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 621/15000, steps: 72, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 622/15000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 623/15000, steps: 215, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 624/15000, steps: 288, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 625/15000, steps: 52, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 626/15000, steps: 91, e: 0.61\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 627/15000, steps: 166, e: 0.61\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 628/15000, steps: 90, e: 0.61\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 629/15000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 630/15000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 631/15000, steps: 410, e: 0.6\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 632/15000, steps: 283, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 633/15000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 634/15000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 635/15000, steps: 225, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 636/15000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 637/15000, steps: 600, e: 0.6\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 638/15000, steps: 176, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 639/15000, steps: 600, e: 0.6\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 640/15000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 641/15000, steps: 351, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 642/15000, steps: 227, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 643/15000, steps: 290, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 644/15000, steps: 600, e: 0.6\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 645/15000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 646/15000, steps: 52, e: 0.6\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 647/15000, steps: 92, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 648/15000, steps: 600, e: 0.6\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 649/15000, steps: 600, e: 0.59\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 650/15000, steps: 115, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 651/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 652/15000, steps: 600, e: 0.59\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 653/15000, steps: 167, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 654/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 655/15000, steps: 126, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 656/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 657/15000, steps: 600, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 658/15000, steps: 600, e: 0.59\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 659/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 660/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 661/15000, steps: 71, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 662/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 663/15000, steps: 481, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 664/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 665/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 666/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 667/15000, steps: 464, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 668/15000, steps: 594, e: 0.59\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 669/15000, steps: 196, e: 0.59\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 670/15000, steps: 52, e: 0.59\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 671/15000, steps: 282, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 672/15000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 673/15000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 674/15000, steps: 248, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 675/15000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 676/15000, steps: 600, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 677/15000, steps: 600, e: 0.58\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 678/15000, steps: 284, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 679/15000, steps: 68, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 680/15000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 681/15000, steps: 131, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 682/15000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 683/15000, steps: 132, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 684/15000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 685/15000, steps: 414, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 686/15000, steps: 52, e: 0.58\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 687/15000, steps: 455, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 688/15000, steps: 600, e: 0.58\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 689/15000, steps: 600, e: 0.58\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 690/15000, steps: 104, e: 0.58\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 691/15000, steps: 410, e: 0.58\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 692/15000, steps: 96, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 693/15000, steps: 67, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 694/15000, steps: 205, e: 0.57\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 695/15000, steps: 103, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 696/15000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 697/15000, steps: 600, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 698/15000, steps: 524, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 699/15000, steps: 340, e: 0.57\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 700/15000, steps: 66, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 701/15000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 702/15000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 703/15000, steps: 93, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 704/15000, steps: 131, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 705/15000, steps: 256, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 706/15000, steps: 83, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 707/15000, steps: 52, e: 0.57\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 708/15000, steps: 205, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 709/15000, steps: 600, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 710/15000, steps: 64, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 711/15000, steps: 97, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 712/15000, steps: 388, e: 0.57\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 713/15000, steps: 427, e: 0.57\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 714/15000, steps: 81, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 715/15000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 716/15000, steps: 530, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 717/15000, steps: 274, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 718/15000, steps: 302, e: 0.56\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 719/15000, steps: 600, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 720/15000, steps: 529, e: 0.56\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 721/15000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 722/15000, steps: 62, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 723/15000, steps: 586, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 724/15000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 725/15000, steps: 70, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 726/15000, steps: 181, e: 0.56\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 727/15000, steps: 122, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 728/15000, steps: 230, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 729/15000, steps: 113, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 730/15000, steps: 456, e: 0.56\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 731/15000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 732/15000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 733/15000, steps: 600, e: 0.56\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 734/15000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 735/15000, steps: 52, e: 0.56\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 736/15000, steps: 337, e: 0.55\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 737/15000, steps: 423, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 738/15000, steps: 600, e: 0.55\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 739/15000, steps: 187, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 740/15000, steps: 600, e: 0.55\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 741/15000, steps: 130, e: 0.55\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 742/15000, steps: 311, e: 0.55\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 743/15000, steps: 60, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 744/15000, steps: 91, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 745/15000, steps: 380, e: 0.55\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 746/15000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 747/15000, steps: 420, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 748/15000, steps: 489, e: 0.55\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 749/15000, steps: 501, e: 0.55\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 750/15000, steps: 57, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 751/15000, steps: 111, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 752/15000, steps: 75, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 753/15000, steps: 505, e: 0.55\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 754/15000, steps: 63, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 755/15000, steps: 271, e: 0.55\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 756/15000, steps: 52, e: 0.55\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 757/15000, steps: 238, e: 0.55\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 758/15000, steps: 113, e: 0.55\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 759/15000, steps: 359, e: 0.54\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 760/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 761/15000, steps: 53, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 762/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 763/15000, steps: 251, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 764/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 765/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 766/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 767/15000, steps: 210, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 768/15000, steps: 227, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 769/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 770/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 771/15000, steps: 422, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 772/15000, steps: 514, e: 0.54\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 773/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 774/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 775/15000, steps: 52, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 776/15000, steps: 194, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 777/15000, steps: 600, e: 0.54\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 778/15000, steps: 482, e: 0.54\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 779/15000, steps: 427, e: 0.54\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 780/15000, steps: 427, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 781/15000, steps: 592, e: 0.54\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 782/15000, steps: 440, e: 0.53\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 783/15000, steps: 600, e: 0.53\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 784/15000, steps: 355, e: 0.53\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 785/15000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 786/15000, steps: 231, e: 0.53\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 787/15000, steps: 57, e: 0.53\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 788/15000, steps: 451, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 789/15000, steps: 600, e: 0.53\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 790/15000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 791/15000, steps: 600, e: 0.53\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 792/15000, steps: 225, e: 0.53\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 793/15000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 794/15000, steps: 161, e: 0.53\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 795/15000, steps: 171, e: 0.53\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 796/15000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 797/15000, steps: 466, e: 0.53\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 798/15000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 799/15000, steps: 78, e: 0.53\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 800/15000, steps: 310, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 801/15000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 802/15000, steps: 600, e: 0.53\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 803/15000, steps: 600, e: 0.53\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 804/15000, steps: 434, e: 0.53\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 805/15000, steps: 52, e: 0.53\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 806/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 807/15000, steps: 277, e: 0.52\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 808/15000, steps: 53, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 809/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 810/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 811/15000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 812/15000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 813/15000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 814/15000, steps: 567, e: 0.52\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 815/15000, steps: 219, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 816/15000, steps: 84, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 817/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 818/15000, steps: 170, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 819/15000, steps: 436, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 820/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 821/15000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 822/15000, steps: 277, e: 0.52\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 823/15000, steps: 66, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 824/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 825/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 826/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 827/15000, steps: 600, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 828/15000, steps: 106, e: 0.52\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 829/15000, steps: 52, e: 0.52\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 830/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 831/15000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 832/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 833/15000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 834/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 835/15000, steps: 99, e: 0.51\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 836/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 837/15000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 838/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 839/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 840/15000, steps: 327, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 841/15000, steps: 570, e: 0.51\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 842/15000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 843/15000, steps: 406, e: 0.51\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 844/15000, steps: 570, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 845/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 846/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 847/15000, steps: 296, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 848/15000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 849/15000, steps: 581, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 850/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 851/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 852/15000, steps: 600, e: 0.51\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 853/15000, steps: 52, e: 0.51\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 854/15000, steps: 133, e: 0.5\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 855/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 856/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 857/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 858/15000, steps: 115, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 859/15000, steps: 287, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 860/15000, steps: 72, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 861/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 862/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 863/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 864/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 865/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 866/15000, steps: 53, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 867/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 868/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 869/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 870/15000, steps: 141, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 871/15000, steps: 561, e: 0.5\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 872/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 873/15000, steps: 298, e: 0.5\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 874/15000, steps: 557, e: 0.5\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 875/15000, steps: 528, e: 0.5\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 876/15000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 877/15000, steps: 52, e: 0.5\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 878/15000, steps: 600, e: 0.5\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 879/15000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 880/15000, steps: 485, e: 0.49\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 881/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 882/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 883/15000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 884/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 885/15000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 886/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 887/15000, steps: 106, e: 0.49\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 888/15000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 889/15000, steps: 119, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 890/15000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 891/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 892/15000, steps: 580, e: 0.49\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 893/15000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 894/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 895/15000, steps: 325, e: 0.49\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 896/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 897/15000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 898/15000, steps: 52, e: 0.49\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 899/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 900/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 901/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 902/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 903/15000, steps: 600, e: 0.49\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 904/15000, steps: 65, e: 0.49\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 905/15000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 906/15000, steps: 134, e: 0.48\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 907/15000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 908/15000, steps: 591, e: 0.48\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 909/15000, steps: 166, e: 0.48\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 910/15000, steps: 596, e: 0.48\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 911/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 912/15000, steps: 223, e: 0.48\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 913/15000, steps: 411, e: 0.48\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 914/15000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 915/15000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 916/15000, steps: 52, e: 0.48\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 917/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 918/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 919/15000, steps: 198, e: 0.48\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 920/15000, steps: 362, e: 0.48\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 921/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 922/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 923/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 924/15000, steps: 107, e: 0.48\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 925/15000, steps: 380, e: 0.48\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 926/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 927/15000, steps: 62, e: 0.48\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 928/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 929/15000, steps: 502, e: 0.48\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 930/15000, steps: 600, e: 0.48\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 931/15000, steps: 337, e: 0.47\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 932/15000, steps: 592, e: 0.47\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 933/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 934/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 935/15000, steps: 80, e: 0.47\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 936/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 937/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 938/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 939/15000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 940/15000, steps: 263, e: 0.47\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 941/15000, steps: 91, e: 0.47\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 942/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 943/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 944/15000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 945/15000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 946/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 947/15000, steps: 401, e: 0.47\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 948/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 949/15000, steps: 52, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 950/15000, steps: 70, e: 0.47\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 951/15000, steps: 113, e: 0.47\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 952/15000, steps: 460, e: 0.47\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 953/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 954/15000, steps: 155, e: 0.47\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 955/15000, steps: 101, e: 0.47\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 956/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 957/15000, steps: 600, e: 0.47\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 958/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 959/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 960/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 961/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 962/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 963/15000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 964/15000, steps: 299, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 965/15000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 966/15000, steps: 62, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 967/15000, steps: 135, e: 0.46\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 968/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 969/15000, steps: 542, e: 0.46\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 970/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 971/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 972/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 973/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 974/15000, steps: 54, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 975/15000, steps: 345, e: 0.46\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 976/15000, steps: 319, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 977/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 978/15000, steps: 251, e: 0.46\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 979/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 980/15000, steps: 68, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 981/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 982/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 983/15000, steps: 600, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 984/15000, steps: 52, e: 0.46\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 985/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 986/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 987/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 988/15000, steps: 515, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 989/15000, steps: 148, e: 0.45\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 990/15000, steps: 551, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 991/15000, steps: 552, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 992/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 993/15000, steps: 96, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 994/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 995/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 996/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 997/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 998/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 999/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1000/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1001/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1002/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1003/15000, steps: 465, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1004/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1005/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1006/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1007/15000, steps: 70, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1008/15000, steps: 52, e: 0.45\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1009/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1010/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1011/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1012/15000, steps: 600, e: 0.45\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1013/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 1014/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1015/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1016/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1017/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1018/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1019/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1020/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1021/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1022/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1023/15000, steps: 212, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1024/15000, steps: 560, e: 0.44\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1025/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1026/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1027/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1028/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1029/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1030/15000, steps: 189, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1031/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1032/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1033/15000, steps: 59, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1034/15000, steps: 600, e: 0.44\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1035/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1036/15000, steps: 307, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1037/15000, steps: 530, e: 0.44\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1038/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1039/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1040/15000, steps: 52, e: 0.44\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1041/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1042/15000, steps: 507, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1043/15000, steps: 135, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1044/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1045/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1046/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1047/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1048/15000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1049/15000, steps: 383, e: 0.43\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1050/15000, steps: 206, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1051/15000, steps: 121, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1052/15000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1053/15000, steps: 252, e: 0.43\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1054/15000, steps: 467, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1055/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1056/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1057/15000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1058/15000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1059/15000, steps: 493, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1060/15000, steps: 103, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1061/15000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1062/15000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1063/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1064/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1065/15000, steps: 149, e: 0.43\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1066/15000, steps: 600, e: 0.43\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1067/15000, steps: 177, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1068/15000, steps: 110, e: 0.43\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1069/15000, steps: 52, e: 0.43\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1070/15000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1071/15000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1072/15000, steps: 600, e: 0.42\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1073/15000, steps: 334, e: 0.42\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1074/15000, steps: 261, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1075/15000, steps: 337, e: 0.42\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1076/15000, steps: 475, e: 0.42\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1077/15000, steps: 464, e: 0.42\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1078/15000, steps: 62, e: 0.42\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1079/15000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1080/15000, steps: 600, e: 0.42\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1081/15000, steps: 406, e: 0.42\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1082/15000, steps: 435, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1083/15000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1084/15000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1085/15000, steps: 88, e: 0.42\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1086/15000, steps: 89, e: 0.42\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1087/15000, steps: 376, e: 0.42\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1088/15000, steps: 600, e: 0.42\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1089/15000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1090/15000, steps: 600, e: 0.42\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1091/15000, steps: 600, e: 0.42\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1092/15000, steps: 575, e: 0.42\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1093/15000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1094/15000, steps: 87, e: 0.42\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1095/15000, steps: 130, e: 0.42\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1096/15000, steps: 600, e: 0.42\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1097/15000, steps: 260, e: 0.42\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1098/15000, steps: 52, e: 0.42\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1099/15000, steps: 142, e: 0.42\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1100/15000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1101/15000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1102/15000, steps: 266, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1103/15000, steps: 61, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1104/15000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1105/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1106/15000, steps: 322, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1107/15000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1108/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1109/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1110/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1111/15000, steps: 587, e: 0.41\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1112/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1113/15000, steps: 546, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1114/15000, steps: 57, e: 0.41\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1115/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1116/15000, steps: 208, e: 0.41\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1117/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1118/15000, steps: 419, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1119/15000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1120/15000, steps: 309, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1121/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1122/15000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1123/15000, steps: 270, e: 0.41\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1124/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1125/15000, steps: 321, e: 0.41\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1126/15000, steps: 600, e: 0.41\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 1127/15000, steps: 52, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1128/15000, steps: 109, e: 0.41\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1129/15000, steps: 349, e: 0.41\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1130/15000, steps: 448, e: 0.4\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1131/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1132/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1133/15000, steps: 139, e: 0.4\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1134/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1135/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1136/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1137/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1138/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1139/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1140/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1141/15000, steps: 514, e: 0.4\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1142/15000, steps: 329, e: 0.4\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1143/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1144/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1145/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1146/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1147/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1148/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1149/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1150/15000, steps: 448, e: 0.4\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1151/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1152/15000, steps: 198, e: 0.4\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1153/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1154/15000, steps: 179, e: 0.4\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1155/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1156/15000, steps: 600, e: 0.4\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1157/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1158/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1159/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1160/15000, steps: 399, e: 0.4\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1161/15000, steps: 52, e: 0.4\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1162/15000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1163/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1164/15000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1165/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1166/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1167/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1168/15000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1169/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1170/15000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1171/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1172/15000, steps: 500, e: 0.39\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1173/15000, steps: 71, e: 0.39\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1174/15000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1175/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1176/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1177/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1178/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1179/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1180/15000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1181/15000, steps: 498, e: 0.39\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1182/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1183/15000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1184/15000, steps: 151, e: 0.39\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1185/15000, steps: 137, e: 0.39\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1186/15000, steps: 90, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1187/15000, steps: 576, e: 0.39\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1188/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1189/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1190/15000, steps: 600, e: 0.39\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1191/15000, steps: 52, e: 0.39\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1192/15000, steps: 62, e: 0.39\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1193/15000, steps: 347, e: 0.39\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1194/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1195/15000, steps: 434, e: 0.38\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1196/15000, steps: 62, e: 0.38\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1197/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1198/15000, steps: 136, e: 0.38\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1199/15000, steps: 252, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1200/15000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1201/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1202/15000, steps: 206, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1203/15000, steps: 63, e: 0.38\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1204/15000, steps: 266, e: 0.38\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1205/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1206/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1207/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1208/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1209/15000, steps: 266, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1210/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1211/15000, steps: 385, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1212/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1213/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1214/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1215/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1216/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 1217/15000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1218/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1219/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1220/15000, steps: 470, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1221/15000, steps: 560, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1222/15000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1223/15000, steps: 52, e: 0.38\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1224/15000, steps: 461, e: 0.38\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1225/15000, steps: 399, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1226/15000, steps: 600, e: 0.38\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1227/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1228/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1229/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1230/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1231/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1232/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1233/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1234/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1235/15000, steps: 361, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 1236/15000, steps: 128, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1237/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1238/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1239/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1240/15000, steps: 427, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1241/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1242/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1243/15000, steps: 60, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1244/15000, steps: 62, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1245/15000, steps: 416, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1246/15000, steps: 326, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1247/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1248/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1249/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1250/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1251/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1252/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1253/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1254/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1255/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1256/15000, steps: 52, e: 0.37\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1257/15000, steps: 196, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1258/15000, steps: 600, e: 0.37\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1259/15000, steps: 150, e: 0.37\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1260/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1261/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1262/15000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1263/15000, steps: 109, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1264/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1265/15000, steps: 407, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1266/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1267/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1268/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1269/15000, steps: 256, e: 0.36\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1270/15000, steps: 479, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1271/15000, steps: 57, e: 0.36\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1272/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1273/15000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1274/15000, steps: 564, e: 0.36\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1275/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1276/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1277/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1278/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1279/15000, steps: 178, e: 0.36\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1280/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1281/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1282/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1283/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1284/15000, steps: 73, e: 0.36\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1285/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1286/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1287/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1288/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1289/15000, steps: 469, e: 0.36\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1290/15000, steps: 600, e: 0.36\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1291/15000, steps: 444, e: 0.36\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1292/15000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1293/15000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1294/15000, steps: 52, e: 0.36\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1295/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1296/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1297/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1298/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1299/15000, steps: 381, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1300/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1301/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1302/15000, steps: 74, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1303/15000, steps: 334, e: 0.35\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1304/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1305/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1306/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1307/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1308/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1309/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1310/15000, steps: 94, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1311/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1312/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1313/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1314/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1315/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1316/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1317/15000, steps: 111, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1318/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1319/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1320/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1321/15000, steps: 265, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1322/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1323/15000, steps: 52, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1324/15000, steps: 207, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1325/15000, steps: 426, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1326/15000, steps: 122, e: 0.35\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1327/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1328/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1329/15000, steps: 427, e: 0.35\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1330/15000, steps: 600, e: 0.35\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1331/15000, steps: 157, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1332/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1333/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1334/15000, steps: 169, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1335/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1336/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1337/15000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1338/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1339/15000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1340/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1341/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1342/15000, steps: 300, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 1343/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1344/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1345/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1346/15000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1347/15000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1348/15000, steps: 456, e: 0.34\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1349/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 1350/15000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1351/15000, steps: 104, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1352/15000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1353/15000, steps: 325, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1354/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1355/15000, steps: 264, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 1356/15000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1357/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1358/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1359/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1360/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1361/15000, steps: 52, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1362/15000, steps: 596, e: 0.34\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1363/15000, steps: 130, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1364/15000, steps: 408, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1365/15000, steps: 565, e: 0.34\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1366/15000, steps: 168, e: 0.34\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1367/15000, steps: 600, e: 0.34\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1368/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1369/15000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1370/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1371/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1372/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1373/15000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1374/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1375/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1376/15000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1377/15000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1378/15000, steps: 261, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1379/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1380/15000, steps: 504, e: 0.33\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1381/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1382/15000, steps: 308, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1383/15000, steps: 354, e: 0.33\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1384/15000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1385/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1386/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1387/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1388/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1389/15000, steps: 460, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1390/15000, steps: 366, e: 0.33\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1391/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1392/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1393/15000, steps: 215, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1394/15000, steps: 130, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1395/15000, steps: 52, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1396/15000, steps: 481, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1397/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1398/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1399/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1400/15000, steps: 250, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1401/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1402/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1403/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1404/15000, steps: 600, e: 0.33\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1405/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1406/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1407/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1408/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1409/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1410/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1411/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1412/15000, steps: 470, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1413/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1414/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1415/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1416/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1417/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1418/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1419/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1420/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1421/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1422/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1423/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1424/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1425/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1426/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1427/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1428/15000, steps: 59, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1429/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1430/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1431/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1432/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1433/15000, steps: 389, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1434/15000, steps: 66, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1435/15000, steps: 446, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1436/15000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1437/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1438/15000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1439/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 1440/15000, steps: 52, e: 0.32\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1441/15000, steps: 466, e: 0.32\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 1442/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1443/15000, steps: 600, e: 0.32\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1444/15000, steps: 259, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1445/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1446/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1447/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1448/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1449/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1450/15000, steps: 182, e: 0.31\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1451/15000, steps: 159, e: 0.31\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1452/15000, steps: 115, e: 0.31\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1453/15000, steps: 259, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1454/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1455/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1456/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1457/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1458/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1459/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1460/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1461/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1462/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1463/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1464/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1465/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1466/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1467/15000, steps: 204, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1468/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1469/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1470/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1471/15000, steps: 250, e: 0.31\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1472/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1473/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1474/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1475/15000, steps: 322, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1476/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1477/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1478/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1479/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1480/15000, steps: 52, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1481/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1482/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1483/15000, steps: 600, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1484/15000, steps: 126, e: 0.31\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1485/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1486/15000, steps: 249, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1487/15000, steps: 111, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1488/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1489/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1490/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1491/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1492/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1493/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1494/15000, steps: 568, e: 0.3\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1495/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1496/15000, steps: 139, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1497/15000, steps: 87, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1498/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1499/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1500/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1501/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1502/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1503/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1504/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1505/15000, steps: 64, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1506/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1507/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1508/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1509/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1510/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1511/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1512/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1513/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1514/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1515/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1516/15000, steps: 424, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 1517/15000, steps: 82, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1518/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1519/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1520/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1521/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1522/15000, steps: 256, e: 0.3\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1523/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1524/15000, steps: 600, e: 0.3\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1525/15000, steps: 52, e: 0.3\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1526/15000, steps: 160, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1527/15000, steps: 59, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1528/15000, steps: 66, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1529/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1530/15000, steps: 78, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1531/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1532/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1533/15000, steps: 176, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1534/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1535/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1536/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1537/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1538/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1539/15000, steps: 228, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1540/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1541/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1542/15000, steps: 222, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1543/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1544/15000, steps: 71, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1545/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1546/15000, steps: 120, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1547/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1548/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1549/15000, steps: 477, e: 0.29\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 1550/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1551/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1552/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1553/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1554/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1555/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1556/15000, steps: 148, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1557/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1558/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1559/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1560/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1561/15000, steps: 187, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1562/15000, steps: 61, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1563/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1564/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1565/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1566/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 1567/15000, steps: 52, e: 0.29\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1568/15000, steps: 600, e: 0.29\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1569/15000, steps: 255, e: 0.29\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1570/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1571/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1572/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1573/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1574/15000, steps: 194, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1575/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1576/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1577/15000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1578/15000, steps: 281, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1579/15000, steps: 126, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1580/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1581/15000, steps: 61, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1582/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1583/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1584/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1585/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1586/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1587/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1588/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1589/15000, steps: 351, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1590/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 1591/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1592/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 1593/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1594/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1595/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1596/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1597/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1598/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1599/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1600/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1601/15000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1602/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1603/15000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1604/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1605/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1606/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1607/15000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 1608/15000, steps: 373, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1609/15000, steps: 60, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 1610/15000, steps: 52, e: 0.28\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1611/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1612/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1613/15000, steps: 600, e: 0.28\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1614/15000, steps: 460, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1615/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1616/15000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1617/15000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1618/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1619/15000, steps: 258, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1620/15000, steps: 191, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1621/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1622/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1623/15000, steps: 495, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1624/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1625/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1626/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1627/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1628/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1629/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1630/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1631/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1632/15000, steps: 413, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1633/15000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1634/15000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1635/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1636/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1637/15000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1638/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1639/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1640/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1641/15000, steps: 263, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1642/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1643/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1644/15000, steps: 544, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1645/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1646/15000, steps: 590, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1647/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1648/15000, steps: 435, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1649/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1650/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1651/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1652/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1653/15000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1654/15000, steps: 78, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1655/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1656/15000, steps: 161, e: 0.27\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1657/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1658/15000, steps: 461, e: 0.27\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1659/15000, steps: 600, e: 0.27\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1660/15000, steps: 52, e: 0.27\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1661/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1662/15000, steps: 599, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1663/15000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1664/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1665/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1666/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1667/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1668/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1669/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1670/15000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1671/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1672/15000, steps: 316, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1673/15000, steps: 110, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1674/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1675/15000, steps: 158, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1676/15000, steps: 78, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1677/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1678/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1679/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1680/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1681/15000, steps: 316, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1682/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1683/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1684/15000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 1685/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1686/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1687/15000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1688/15000, steps: 91, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1689/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1690/15000, steps: 162, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1691/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1692/15000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1693/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1694/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1695/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1696/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1697/15000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1698/15000, steps: 456, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1699/15000, steps: 52, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1700/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1701/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 1702/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1703/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1704/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1705/15000, steps: 145, e: 0.26\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1706/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1707/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1708/15000, steps: 600, e: 0.26\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1709/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1710/15000, steps: 542, e: 0.25\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1711/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1712/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1713/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1714/15000, steps: 340, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1715/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1716/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 1717/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1718/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1719/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1720/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1721/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1722/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1723/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 1724/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1725/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 1726/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1727/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1728/15000, steps: 121, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1729/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1730/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1731/15000, steps: 435, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1732/15000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1733/15000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1734/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1735/15000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1736/15000, steps: 59, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1737/15000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1738/15000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1739/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1740/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1741/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1742/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1743/15000, steps: 338, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 1744/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1745/15000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1746/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1747/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1748/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1749/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1750/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1751/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1752/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1753/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1754/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1755/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1756/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1757/15000, steps: 600, e: 0.25\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1758/15000, steps: 52, e: 0.25\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1759/15000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1760/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1761/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1762/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1763/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1764/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1765/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1766/15000, steps: 590, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1767/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1768/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1769/15000, steps: 107, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1770/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1771/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 1772/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 1773/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1774/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1775/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1776/15000, steps: 190, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1777/15000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1778/15000, steps: 460, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1779/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1780/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1781/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1782/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1783/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1784/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1785/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1786/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1787/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1788/15000, steps: 131, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1789/15000, steps: 119, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1790/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1791/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1792/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1793/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1794/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1795/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1796/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1797/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1798/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1799/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1800/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1801/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1802/15000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1803/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1804/15000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1805/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 1806/15000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1807/15000, steps: 458, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1808/15000, steps: 600, e: 0.24\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1809/15000, steps: 308, e: 0.24\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1810/15000, steps: 52, e: 0.24\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1811/15000, steps: 408, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1812/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1813/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1814/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1815/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 1816/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1817/15000, steps: 401, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1818/15000, steps: 115, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1819/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1820/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1821/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1822/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1823/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 1824/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1825/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1826/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1827/15000, steps: 189, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1828/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 1829/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1830/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 1831/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1832/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1833/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1834/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1835/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 1836/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1837/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1838/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1839/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1840/15000, steps: 256, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1841/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1842/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1843/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1844/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1845/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1846/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1847/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1848/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1849/15000, steps: 210, e: 0.23\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1850/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1851/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1852/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1853/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1854/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 1855/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 1856/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1857/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 1858/15000, steps: 52, e: 0.23\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 1859/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1860/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 1861/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1862/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1863/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 1864/15000, steps: 600, e: 0.23\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 1865/15000, steps: 169, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1866/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1867/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1868/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1869/15000, steps: 174, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1870/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1871/15000, steps: 79, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 1872/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1873/15000, steps: 202, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1874/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 1875/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1876/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1877/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 1878/15000, steps: 509, e: 0.22\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1879/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1880/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1881/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1882/15000, steps: 236, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1883/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1884/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1885/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 1886/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 1887/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 1888/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 1889/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1890/15000, steps: 359, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1891/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1892/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 1893/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 1894/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1895/15000, steps: 435, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1896/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 1897/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1898/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 1899/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 1900/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1901/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1902/15000, steps: 406, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1903/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1904/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1905/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 1906/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1907/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 1908/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 1909/15000, steps: 52, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1910/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1911/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1912/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1913/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1914/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1915/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1916/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1917/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1918/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1919/15000, steps: 149, e: 0.22\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 1920/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1921/15000, steps: 600, e: 0.22\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1922/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1923/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1924/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1925/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1926/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 1927/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 1928/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 1929/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1930/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1931/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 1932/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 1933/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 1934/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 1935/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1936/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 1937/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 1938/15000, steps: 243, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1939/15000, steps: 565, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 1940/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 1941/15000, steps: 500, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 1942/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1943/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1944/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1945/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 1946/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 1947/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 1948/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 1949/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1950/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1951/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 1952/15000, steps: 67, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 1953/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 1954/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1955/15000, steps: 309, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1956/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1957/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 1958/15000, steps: 216, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1959/15000, steps: 112, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1960/15000, steps: 542, e: 0.21\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 1961/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1962/15000, steps: 558, e: 0.21\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 1963/15000, steps: 581, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 1964/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1965/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 1966/15000, steps: 79, e: 0.21\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 1967/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1968/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 1969/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1970/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1971/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 1972/15000, steps: 562, e: 0.21\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 1973/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 1974/15000, steps: 519, e: 0.21\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 1975/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 1976/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 1977/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 1978/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 1979/15000, steps: 52, e: 0.21\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1980/15000, steps: 600, e: 0.21\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 1981/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 1982/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 1983/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 1984/15000, steps: 208, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 1985/15000, steps: 279, e: 0.2\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 1986/15000, steps: 460, e: 0.2\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 1987/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 1988/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 1989/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 1990/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 1991/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 1992/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 1993/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 1994/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 1995/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1996/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 1997/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 1998/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 1999/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2000/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2001/15000, steps: 248, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2002/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2003/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2004/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2005/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2006/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2007/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2008/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2009/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2010/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2011/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2012/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2013/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2014/15000, steps: 457, e: 0.2\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2015/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2016/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2017/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2018/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2019/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2020/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2021/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2022/15000, steps: 439, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2023/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2024/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2025/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2026/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2027/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2028/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2029/15000, steps: 441, e: 0.2\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2030/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2031/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2032/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2033/15000, steps: 492, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 2034/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2035/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2036/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2037/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2038/15000, steps: 210, e: 0.2\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2039/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2040/15000, steps: 243, e: 0.2\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2041/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2042/15000, steps: 52, e: 0.2\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2043/15000, steps: 600, e: 0.2\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2044/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2045/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2046/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2047/15000, steps: 573, e: 0.19\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2048/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2049/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2050/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2051/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2052/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2053/15000, steps: 309, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2054/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2055/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2056/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2057/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2058/15000, steps: 519, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2059/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2060/15000, steps: 182, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2061/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2062/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2063/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2064/15000, steps: 105, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2065/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2066/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2067/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2068/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2069/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2070/15000, steps: 304, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2071/15000, steps: 539, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2072/15000, steps: 129, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2073/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2074/15000, steps: 406, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2075/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2076/15000, steps: 356, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2077/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2078/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2079/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2080/15000, steps: 105, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2081/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2082/15000, steps: 74, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2083/15000, steps: 223, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2084/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2085/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2086/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2087/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2088/15000, steps: 567, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2089/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2090/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2091/15000, steps: 544, e: 0.19\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2092/15000, steps: 349, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2093/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2094/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2095/15000, steps: 107, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2096/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2097/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2098/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 2099/15000, steps: 113, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2100/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2101/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2102/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 2103/15000, steps: 52, e: 0.19\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2104/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2105/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2106/15000, steps: 516, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2107/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2108/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2109/15000, steps: 600, e: 0.19\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2110/15000, steps: 359, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2111/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2112/15000, steps: 333, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2113/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2114/15000, steps: 152, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2115/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 2116/15000, steps: 232, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2117/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2118/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2119/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2120/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2121/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2122/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 2123/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2124/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2125/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2126/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2127/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2128/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2129/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2130/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2131/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2132/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 2133/15000, steps: 275, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2134/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2135/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2136/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2137/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2138/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2139/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2140/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2141/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2142/15000, steps: 352, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2143/15000, steps: 355, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2144/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2145/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 2146/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2147/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2148/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2149/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2150/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 2151/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2152/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2153/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2154/15000, steps: 439, e: 0.18\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2155/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2156/15000, steps: 83, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2157/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2158/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2159/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2160/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2161/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2162/15000, steps: 394, e: 0.18\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2163/15000, steps: 470, e: 0.18\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2164/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2165/15000, steps: 181, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2166/15000, steps: 80, e: 0.18\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2167/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2168/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2169/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 2170/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2171/15000, steps: 307, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2172/15000, steps: 464, e: 0.18\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2173/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2174/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2175/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2176/15000, steps: 52, e: 0.18\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2177/15000, steps: 600, e: 0.18\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2178/15000, steps: 128, e: 0.18\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2179/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2180/15000, steps: 285, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2181/15000, steps: 78, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2182/15000, steps: 134, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2183/15000, steps: 181, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2184/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2185/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2186/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2187/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2188/15000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2189/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2190/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2191/15000, steps: 429, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 2192/15000, steps: 107, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 2193/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2194/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2195/15000, steps: 67, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 2196/15000, steps: 361, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2197/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2198/15000, steps: 355, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2199/15000, steps: 281, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2200/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2201/15000, steps: 161, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2202/15000, steps: 202, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2203/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2204/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2205/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2206/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2207/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2208/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2209/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2210/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2211/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2212/15000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2213/15000, steps: 301, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2214/15000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2215/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2216/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2217/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2218/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2219/15000, steps: 258, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2220/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2221/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2222/15000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2223/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2224/15000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2225/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2226/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2227/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2228/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2229/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2230/15000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2231/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2232/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2233/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2234/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2235/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2236/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2237/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 2238/15000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2239/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2240/15000, steps: 134, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2241/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2242/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2243/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2244/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2245/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2246/15000, steps: 236, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2247/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2248/15000, steps: 56, e: 0.17\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2249/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2250/15000, steps: 208, e: 0.17\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2251/15000, steps: 600, e: 0.17\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2252/15000, steps: 52, e: 0.17\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2253/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2254/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2255/15000, steps: 370, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 2256/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2257/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 2258/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2259/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2260/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2261/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2262/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2263/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2264/15000, steps: 575, e: 0.16\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 2265/15000, steps: 314, e: 0.16\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2266/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2267/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2268/15000, steps: 81, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2269/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2270/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2271/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2272/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2273/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2274/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2275/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2276/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2277/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2278/15000, steps: 566, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2279/15000, steps: 127, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2280/15000, steps: 331, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2281/15000, steps: 143, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2282/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2283/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2284/15000, steps: 53, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2285/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2286/15000, steps: 292, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2287/15000, steps: 364, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2288/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2289/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 2290/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2291/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2292/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2293/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2294/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2295/15000, steps: 62, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2296/15000, steps: 54, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2297/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2298/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2299/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2300/15000, steps: 478, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2301/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2302/15000, steps: 131, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2303/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2304/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2305/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2306/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2307/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2308/15000, steps: 337, e: 0.16\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2309/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2310/15000, steps: 581, e: 0.16\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2311/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2312/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2313/15000, steps: 240, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2314/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2315/15000, steps: 362, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2316/15000, steps: 108, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2317/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2318/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2319/15000, steps: 53, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2320/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2321/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2322/15000, steps: 238, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 2323/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2324/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2325/15000, steps: 117, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2326/15000, steps: 52, e: 0.16\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2327/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2328/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2329/15000, steps: 600, e: 0.16\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2330/15000, steps: 313, e: 0.16\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2331/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2332/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2333/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2334/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2335/15000, steps: 344, e: 0.15\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2336/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2337/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2338/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2339/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2340/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2341/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2342/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2343/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2344/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2345/15000, steps: 307, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2346/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2347/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2348/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2349/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2350/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2351/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2352/15000, steps: 571, e: 0.15\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2353/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2354/15000, steps: 338, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2355/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2356/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2357/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2358/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2359/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2360/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2361/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2362/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2363/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2364/15000, steps: 515, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2365/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2366/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2367/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2368/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2369/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2370/15000, steps: 566, e: 0.15\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2371/15000, steps: 457, e: 0.15\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2372/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2373/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2374/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2375/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2376/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2377/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2378/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2379/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2380/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2381/15000, steps: 396, e: 0.15\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2382/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2383/15000, steps: 572, e: 0.15\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2384/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2385/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2386/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2387/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2388/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2389/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2390/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2391/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2392/15000, steps: 382, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2393/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2394/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2395/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2396/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2397/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2398/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2399/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2400/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2401/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2402/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2403/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2404/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2405/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2406/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2407/15000, steps: 52, e: 0.15\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2408/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2409/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2410/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2411/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2412/15000, steps: 600, e: 0.15\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2413/15000, steps: 155, e: 0.15\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2414/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2415/15000, steps: 115, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2416/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2417/15000, steps: 459, e: 0.14\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2418/15000, steps: 72, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2419/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2420/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2421/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2422/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2423/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2424/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2425/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2426/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2427/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2428/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2429/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2430/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2431/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2432/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2433/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2434/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2435/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2436/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 2437/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2438/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2439/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2440/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2441/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2442/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 2443/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2444/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2445/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2446/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2447/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2448/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2449/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2450/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2451/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2452/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2453/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2454/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2455/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2456/15000, steps: 501, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2457/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2458/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2459/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2460/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2461/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2462/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2463/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2464/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2465/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2466/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2467/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2468/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2469/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2470/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2471/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2472/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2473/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2474/15000, steps: 107, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2475/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2476/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 2477/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2478/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2479/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2480/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2481/15000, steps: 574, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2482/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2483/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2484/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2485/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2486/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2487/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2488/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2489/15000, steps: 83, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2490/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2491/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2492/15000, steps: 471, e: 0.14\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2493/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2494/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2495/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2496/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2497/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2498/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2499/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2500/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2501/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2502/15000, steps: 52, e: 0.14\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2503/15000, steps: 600, e: 0.14\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2504/15000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2505/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2506/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2507/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2508/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2509/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2510/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2511/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2512/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 2513/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2514/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2515/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2516/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2517/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2518/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2519/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2520/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2521/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2522/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2523/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2524/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2525/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2526/15000, steps: 420, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2527/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2528/15000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2529/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2530/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2531/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 2532/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2533/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2534/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2535/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2536/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2537/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2538/15000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2539/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2540/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2541/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2542/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2543/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2544/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2545/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2546/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2547/15000, steps: 359, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2548/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2549/15000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2550/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2551/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2552/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2553/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2554/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2555/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2556/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2557/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2558/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2559/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2560/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 2561/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2562/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2563/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2564/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2565/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 2566/15000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2567/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2568/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2569/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2570/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2571/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2572/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2573/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2574/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2575/15000, steps: 210, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2576/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2577/15000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2578/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2579/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2580/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2581/15000, steps: 142, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2582/15000, steps: 184, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2583/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2584/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2585/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2586/15000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2587/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2588/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2589/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2590/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2591/15000, steps: 52, e: 0.13\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2592/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2593/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2594/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2595/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2596/15000, steps: 514, e: 0.13\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2597/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2598/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2599/15000, steps: 600, e: 0.13\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2600/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2601/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2602/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2603/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2604/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2605/15000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2606/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2607/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2608/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2609/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2610/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2611/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2612/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2613/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2614/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2615/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2616/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2617/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2618/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2619/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2620/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2621/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2622/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2623/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2624/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2625/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2626/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2627/15000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2628/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2629/15000, steps: 155, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2630/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2631/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2632/15000, steps: 511, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2633/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2634/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2635/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2636/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2637/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2638/15000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2639/15000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2640/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2641/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2642/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2643/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2644/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 2645/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2646/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2647/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2648/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2649/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2650/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2651/15000, steps: 106, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2652/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2653/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2654/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2655/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2656/15000, steps: 312, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2657/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 2658/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2659/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2660/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2661/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2662/15000, steps: 408, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2663/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2664/15000, steps: 139, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2665/15000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 2666/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2667/15000, steps: 76, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2668/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 2669/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2670/15000, steps: 84, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2671/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2672/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2673/15000, steps: 166, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2674/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2675/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2676/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2677/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2678/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2679/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 2680/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2681/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2682/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2683/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2684/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2685/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2686/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2687/15000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2688/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2689/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2690/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2691/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2692/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2693/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2694/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2695/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2696/15000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2697/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2698/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2699/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2700/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2701/15000, steps: 52, e: 0.12\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2702/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2703/15000, steps: 600, e: 0.12\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2704/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2705/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2706/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2707/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2708/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2709/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2710/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2711/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2712/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 2713/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2714/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2715/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2716/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2717/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 2718/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2719/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2720/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2721/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2722/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2723/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 2724/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2725/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2726/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2727/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2728/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2729/15000, steps: 329, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2730/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2731/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2732/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2733/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2734/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2735/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2736/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2737/15000, steps: 286, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 2738/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2739/15000, steps: 466, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2740/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2741/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2742/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2743/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2744/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2745/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2746/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2747/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2748/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2749/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2750/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 2751/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2752/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 2753/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2754/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2755/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2756/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2757/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 2758/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2759/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2760/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2761/15000, steps: 447, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2762/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2763/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2764/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2765/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2766/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2767/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2768/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2769/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2770/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2771/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2772/15000, steps: 412, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2773/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2774/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 2775/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2776/15000, steps: 585, e: 0.11\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2777/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2778/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2779/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2780/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2781/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2782/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2783/15000, steps: 306, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2784/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2785/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 2786/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2787/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2788/15000, steps: 128, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2789/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2790/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2791/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2792/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2793/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2794/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2795/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2796/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2797/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2798/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2799/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2800/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2801/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 2802/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2803/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2804/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2805/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 2806/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2807/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 2808/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2809/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2810/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2811/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2812/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2813/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2814/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2815/15000, steps: 117, e: 0.11\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2816/15000, steps: 600, e: 0.11\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2817/15000, steps: 52, e: 0.11\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2818/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2819/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2820/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2821/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2822/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2823/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2824/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2825/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 2826/15000, steps: 109, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 2827/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 2828/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2829/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2830/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2831/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2832/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2833/15000, steps: 471, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2834/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2835/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2836/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2837/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2838/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2839/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2840/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2841/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 2842/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2843/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 2844/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2845/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2846/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2847/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 2848/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2849/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2850/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2851/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2852/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 2853/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 2854/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2855/15000, steps: 108, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2856/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2857/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2858/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2859/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2860/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2861/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2862/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 2863/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2864/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2865/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2866/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2867/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2868/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2869/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2870/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2871/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2872/15000, steps: 227, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2873/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2874/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2875/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 2876/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2877/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2878/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2879/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2880/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2881/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 2882/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 2883/15000, steps: 600, e: 0.1\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2884/15000, steps: 52, e: 0.1\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2885/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2886/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 2887/15000, steps: 383, e: 0.099\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 2888/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2889/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 2890/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 2891/15000, steps: 52, e: 0.099\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2892/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2893/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 2894/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2895/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2896/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2897/15000, steps: 600, e: 0.099\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2898/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 2899/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 2900/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 2901/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 2902/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2903/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2904/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 2905/15000, steps: 52, e: 0.098\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2906/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 2907/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2908/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2909/15000, steps: 600, e: 0.098\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2910/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 2911/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2912/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2913/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2914/15000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2915/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 2916/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2917/15000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2918/15000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2919/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2920/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2921/15000, steps: 52, e: 0.097\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 2922/15000, steps: 600, e: 0.097\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2923/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 2924/15000, steps: 56, e: 0.096\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 2925/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 2926/15000, steps: 52, e: 0.096\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2927/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 2928/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2929/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2930/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2931/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 2932/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2933/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2934/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2935/15000, steps: 600, e: 0.096\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 2936/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2937/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 2938/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 2939/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 2940/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 2941/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 2942/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 2943/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2944/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2945/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2946/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 2947/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 2948/15000, steps: 600, e: 0.095\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2949/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 2950/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 2951/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2952/15000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 2953/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2954/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2955/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 2956/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 2957/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 2958/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 2959/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 2960/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 2961/15000, steps: 600, e: 0.094\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 2962/15000, steps: 52, e: 0.094\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 2963/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 2964/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 2965/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 2966/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2967/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2968/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2969/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2970/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 2971/15000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 2972/15000, steps: 124, e: 0.093\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 2973/15000, steps: 52, e: 0.093\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 2974/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 2975/15000, steps: 600, e: 0.093\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 2976/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 2977/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2978/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 2979/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 2980/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 2981/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 2982/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 2983/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 2984/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 2985/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 2986/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 2987/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 2988/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 2989/15000, steps: 600, e: 0.092\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 2990/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 2991/15000, steps: 52, e: 0.091\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 2992/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 2993/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 2994/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 2995/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 2996/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 2997/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 2998/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 2999/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3000/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3001/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3002/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3003/15000, steps: 600, e: 0.091\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3004/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3005/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3006/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3007/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3008/15000, steps: 582, e: 0.09\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3009/15000, steps: 122, e: 0.09\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3010/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3011/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3012/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3013/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3014/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3015/15000, steps: 320, e: 0.09\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3016/15000, steps: 600, e: 0.09\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3017/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3018/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3019/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3020/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3021/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3022/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3023/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3024/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3025/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3026/15000, steps: 52, e: 0.089\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3027/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3028/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3029/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3030/15000, steps: 600, e: 0.089\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3031/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3032/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3033/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3034/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3035/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 3036/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3037/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3038/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3039/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3040/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3041/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3042/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 3043/15000, steps: 52, e: 0.088\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3044/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3045/15000, steps: 600, e: 0.088\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3046/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3047/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3048/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3049/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3050/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3051/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3052/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3053/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3054/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3055/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3056/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3057/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3058/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3059/15000, steps: 600, e: 0.087\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3060/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3061/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3062/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3063/15000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3064/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3065/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3066/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3067/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3068/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3069/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3070/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3071/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3072/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 3073/15000, steps: 52, e: 0.086\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3074/15000, steps: 600, e: 0.086\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3075/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3076/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3077/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3078/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3079/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3080/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3081/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3082/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3083/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3084/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3085/15000, steps: 248, e: 0.085\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3086/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3087/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3088/15000, steps: 600, e: 0.085\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3089/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3090/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3091/15000, steps: 458, e: 0.084\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3092/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3093/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3094/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3095/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3096/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3097/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3098/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3099/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3100/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3101/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3102/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3103/15000, steps: 600, e: 0.084\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3104/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3105/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3106/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3107/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3108/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3109/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3110/15000, steps: 52, e: 0.083\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3111/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3112/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3113/15000, steps: 173, e: 0.083\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3114/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3115/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3116/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3117/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3118/15000, steps: 600, e: 0.083\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3119/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3120/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3121/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3122/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3123/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3124/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3125/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3126/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3127/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3128/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3129/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3130/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3131/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3132/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3133/15000, steps: 600, e: 0.082\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3134/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3135/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3136/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3137/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3138/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3139/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3140/15000, steps: 52, e: 0.081\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3141/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3142/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3143/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3144/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3145/15000, steps: 114, e: 0.081\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3146/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3147/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3148/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3149/15000, steps: 600, e: 0.081\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3150/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3151/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3152/15000, steps: 386, e: 0.08\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3153/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3154/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3155/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3156/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 3157/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3158/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3159/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3160/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3161/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3162/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3163/15000, steps: 600, e: 0.08\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3164/15000, steps: 52, e: 0.08\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3165/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3166/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3167/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3168/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3169/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3170/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3171/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3172/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3173/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3174/15000, steps: 52, e: 0.079\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3175/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3176/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3177/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3178/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3179/15000, steps: 214, e: 0.079\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3180/15000, steps: 600, e: 0.079\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3181/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3182/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3183/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3184/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3185/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3186/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3187/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3188/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3189/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3190/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3191/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3192/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3193/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3194/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3195/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3196/15000, steps: 600, e: 0.078\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3197/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3198/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3199/15000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3200/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3201/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3202/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3203/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3204/15000, steps: 52, e: 0.077\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3205/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 3206/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3207/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3208/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3209/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3210/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3211/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3212/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3213/15000, steps: 600, e: 0.077\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3214/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3215/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3216/15000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3217/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3218/15000, steps: 52, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3219/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3220/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3221/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3222/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3223/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3224/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3225/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3226/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3227/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3228/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3229/15000, steps: 600, e: 0.076\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3230/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3231/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3232/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3233/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3234/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3235/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3236/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3237/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3238/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3239/15000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3240/15000, steps: 305, e: 0.075\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3241/15000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3242/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3243/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3244/15000, steps: 52, e: 0.075\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3245/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3246/15000, steps: 600, e: 0.075\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3247/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3248/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3249/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3250/15000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3251/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3252/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3253/15000, steps: 52, e: 0.074\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3254/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3255/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3256/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3257/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3258/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3259/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3260/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3261/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3262/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3263/15000, steps: 600, e: 0.074\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3264/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3265/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3266/15000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 3267/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3268/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3269/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3270/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3271/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3272/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3273/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3274/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3275/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3276/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3277/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3278/15000, steps: 52, e: 0.073\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3279/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3280/15000, steps: 600, e: 0.073\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3281/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3282/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3283/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3284/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3285/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3286/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3287/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3288/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3289/15000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3290/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3291/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3292/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3293/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3294/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3295/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3296/15000, steps: 52, e: 0.072\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3297/15000, steps: 600, e: 0.072\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3298/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3299/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3300/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3301/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3302/15000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3303/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3304/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3305/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3306/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3307/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3308/15000, steps: 52, e: 0.071\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3309/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3310/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 3311/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3312/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3313/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3314/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3315/15000, steps: 600, e: 0.071\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3316/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3317/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3318/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3319/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3320/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3321/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3322/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3323/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3324/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3325/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3326/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3327/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3328/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3329/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3330/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3331/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3332/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3333/15000, steps: 600, e: 0.07\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3334/15000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3335/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3336/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3337/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3338/15000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3339/15000, steps: 546, e: 0.069\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3340/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3341/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3342/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3343/15000, steps: 52, e: 0.069\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3344/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3345/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3346/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3347/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3348/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3349/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3350/15000, steps: 600, e: 0.069\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3351/15000, steps: 307, e: 0.069\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3352/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3353/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3354/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3355/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3356/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3357/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3358/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3359/15000, steps: 406, e: 0.068\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3360/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3361/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3362/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3363/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3364/15000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3365/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3366/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3367/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3368/15000, steps: 600, e: 0.068\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3369/15000, steps: 52, e: 0.068\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3370/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3371/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3372/15000, steps: 200, e: 0.067\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3373/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3374/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3375/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3376/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3377/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3378/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3379/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3380/15000, steps: 112, e: 0.067\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3381/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3382/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3383/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3384/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3385/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3386/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3387/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3388/15000, steps: 600, e: 0.067\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3389/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3390/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3391/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3392/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3393/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3394/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3395/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3396/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3397/15000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3398/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3399/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3400/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3401/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3402/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3403/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3404/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3405/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3406/15000, steps: 52, e: 0.066\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3407/15000, steps: 600, e: 0.066\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3408/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3409/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3410/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 3411/15000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3412/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3413/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3414/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3415/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3416/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3417/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3418/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3419/15000, steps: 278, e: 0.065\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3420/15000, steps: 52, e: 0.065\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3421/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3422/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3423/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3424/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3425/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3426/15000, steps: 600, e: 0.065\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3427/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 3428/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3429/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3430/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3431/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3432/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3433/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3434/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3435/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3436/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3437/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3438/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3439/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3440/15000, steps: 52, e: 0.064\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3441/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3442/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3443/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3444/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3445/15000, steps: 600, e: 0.064\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3446/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3447/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3448/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3449/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3450/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3451/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3452/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3453/15000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3454/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3455/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3456/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3457/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3458/15000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3459/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3460/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3461/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3462/15000, steps: 52, e: 0.063\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3463/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3464/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3465/15000, steps: 600, e: 0.063\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3466/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3467/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3468/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3469/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 3470/15000, steps: 594, e: 0.062\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3471/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3472/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3473/15000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3474/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3475/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3476/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3477/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3478/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3479/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3480/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3481/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3482/15000, steps: 52, e: 0.062\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3483/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3484/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3485/15000, steps: 600, e: 0.062\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3486/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3487/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3488/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3489/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3490/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3491/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3492/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3493/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3494/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3495/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3496/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3497/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3498/15000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3499/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3500/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3501/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3502/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3503/15000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3504/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3505/15000, steps: 600, e: 0.061\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3506/15000, steps: 52, e: 0.061\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3507/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3508/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3509/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3510/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3511/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3512/15000, steps: 360, e: 0.06\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3513/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3514/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3515/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3516/15000, steps: 511, e: 0.06\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3517/15000, steps: 61, e: 0.06\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3518/15000, steps: 161, e: 0.06\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3519/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3520/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3521/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3522/15000, steps: 52, e: 0.06\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3523/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3524/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3525/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3526/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3527/15000, steps: 600, e: 0.06\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3528/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3529/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3530/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3531/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3532/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3533/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3534/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3535/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3536/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3537/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3538/15000, steps: 356, e: 0.059\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3539/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3540/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3541/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3542/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3543/15000, steps: 52, e: 0.059\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3544/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3545/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3546/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3547/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 3548/15000, steps: 600, e: 0.059\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3549/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3550/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3551/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3552/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3553/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3554/15000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3555/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3556/15000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3557/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3558/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3559/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3560/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3561/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3562/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3563/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3564/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3565/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3566/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3567/15000, steps: 600, e: 0.058\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3568/15000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3569/15000, steps: 52, e: 0.058\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3570/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3571/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3572/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3573/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3574/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3575/15000, steps: 60, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3576/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3577/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3578/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3579/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3580/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3581/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3582/15000, steps: 52, e: 0.057\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3583/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3584/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3585/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3586/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3587/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3588/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3589/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3590/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3591/15000, steps: 600, e: 0.057\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3592/15000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3593/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3594/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3595/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 3596/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3597/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3598/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3599/15000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3600/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3601/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3602/15000, steps: 524, e: 0.056\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3603/15000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3604/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3605/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3606/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3607/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3608/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3609/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3610/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3611/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3612/15000, steps: 52, e: 0.056\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3613/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3614/15000, steps: 600, e: 0.056\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3615/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3616/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3617/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3618/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3619/15000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3620/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3621/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3622/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3623/15000, steps: 52, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3624/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3625/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3626/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3627/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3628/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3629/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3630/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3631/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3632/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3633/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3634/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3635/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3636/15000, steps: 600, e: 0.055\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3637/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3638/15000, steps: 301, e: 0.054\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3639/15000, steps: 57, e: 0.054\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3640/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3641/15000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3642/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3643/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3644/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3645/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3646/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3647/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3648/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3649/15000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3650/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3651/15000, steps: 52, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3652/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3653/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3654/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3655/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3656/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3657/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3658/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3659/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 3660/15000, steps: 600, e: 0.054\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3661/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3662/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3663/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3664/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3665/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3666/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3667/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3668/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3669/15000, steps: 52, e: 0.053\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3670/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3671/15000, steps: 164, e: 0.053\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3672/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3673/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3674/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3675/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3676/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3677/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3678/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3679/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3680/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3681/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3682/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3683/15000, steps: 600, e: 0.053\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3684/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3685/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3686/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3687/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3688/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3689/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3690/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3691/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3692/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3693/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3694/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3695/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3696/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3697/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3698/15000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3699/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3700/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3701/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3702/15000, steps: 52, e: 0.052\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 3703/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3704/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3705/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3706/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3707/15000, steps: 600, e: 0.052\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3708/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3709/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3710/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3711/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3712/15000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3713/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3714/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3715/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3716/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3717/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3718/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3719/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3720/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3721/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3722/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3723/15000, steps: 52, e: 0.051\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3724/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3725/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3726/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 3727/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3728/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3729/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3730/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3731/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3732/15000, steps: 600, e: 0.051\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3733/15000, steps: 52, e: 0.05\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3734/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3735/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3736/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3737/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3738/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3739/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3740/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3741/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3742/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 3743/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3744/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3745/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3746/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3747/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3748/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3749/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3750/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3751/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3752/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3753/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3754/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3755/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3756/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3757/15000, steps: 600, e: 0.05\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3758/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3759/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3760/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3761/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3762/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3763/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3764/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 3765/15000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3766/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3767/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3768/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3769/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3770/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3771/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3772/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3773/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3774/15000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 3775/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3776/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3777/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 3778/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 3779/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3780/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3781/15000, steps: 600, e: 0.049\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3782/15000, steps: 52, e: 0.049\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3783/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3784/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3785/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 3786/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3787/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3788/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3789/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3790/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3791/15000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3792/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3793/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 3794/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3795/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 3796/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3797/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3798/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3799/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3800/15000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3801/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3802/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3803/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3804/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3805/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3806/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 3807/15000, steps: 52, e: 0.048\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3808/15000, steps: 600, e: 0.048\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3809/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 3810/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 3811/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3812/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3813/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3814/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3815/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 3816/15000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3817/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3818/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3819/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3820/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3821/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3822/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3823/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3824/15000, steps: 52, e: 0.047\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3825/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3826/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3827/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3828/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3829/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3830/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3831/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3832/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3833/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3834/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3835/15000, steps: 600, e: 0.047\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3836/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3837/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3838/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3839/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3840/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 3841/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3842/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3843/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3844/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3845/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3846/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3847/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 3848/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 3849/15000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3850/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3851/15000, steps: 52, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3852/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3853/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3854/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3855/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3856/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 3857/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3858/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3859/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3860/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3861/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3862/15000, steps: 600, e: 0.046\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 3863/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3864/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3865/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 3866/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3867/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3868/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 3869/15000, steps: 95, e: 0.045\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3870/15000, steps: 57, e: 0.045\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3871/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3872/15000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3873/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3874/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3875/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3876/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3877/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3878/15000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3879/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3880/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3881/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 3882/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 3883/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3884/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 3885/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3886/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3887/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3888/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 3889/15000, steps: 600, e: 0.045\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3890/15000, steps: 52, e: 0.045\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 3891/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 3892/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 3893/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 3894/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 3895/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3896/15000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3897/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 3898/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 3899/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 3900/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3901/15000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3902/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 3903/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3904/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3905/15000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3906/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3907/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 3908/15000, steps: 52, e: 0.044\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 3909/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 3910/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 3911/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 3912/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3913/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3914/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3915/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3916/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3917/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 3918/15000, steps: 600, e: 0.044\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 3919/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3920/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3921/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3922/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3923/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 3924/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3925/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 3926/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3927/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3928/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3929/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 3930/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3931/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 3932/15000, steps: 235, e: 0.043\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3933/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 3934/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3935/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3936/15000, steps: 52, e: 0.043\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 3937/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3938/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3939/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 3940/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3941/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3942/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3943/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 3944/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 3945/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 3946/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3947/15000, steps: 600, e: 0.043\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 3948/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3949/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3950/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 3951/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3952/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 3953/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3954/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 3955/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 3956/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 3957/15000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 3958/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3959/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 3960/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3961/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3962/15000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 3963/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 3964/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3965/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 3966/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3967/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 3968/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 3969/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 3970/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 3971/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 3972/15000, steps: 52, e: 0.042\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 3973/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 3974/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3975/15000, steps: 428, e: 0.042\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3976/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 3977/15000, steps: 600, e: 0.042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 3978/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 3979/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3980/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 3981/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3982/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 3983/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 3984/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3985/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 3986/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 3987/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 3988/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 3989/15000, steps: 277, e: 0.041\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 3990/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 3991/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 3992/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 3993/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 3994/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 3995/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 3996/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 3997/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 3998/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 3999/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4000/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4001/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4002/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4003/15000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4004/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4005/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4006/15000, steps: 52, e: 0.041\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4007/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4008/15000, steps: 600, e: 0.041\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4009/15000, steps: 52, e: 0.04\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4010/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4011/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4012/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4013/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4014/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4015/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4016/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4017/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4018/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4019/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4020/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4021/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4022/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4023/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4024/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4025/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4026/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4027/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4028/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4029/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4030/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4031/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4032/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4033/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4034/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4035/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4036/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4037/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4038/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4039/15000, steps: 600, e: 0.04\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4040/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4041/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4042/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4043/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4044/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4045/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4046/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4047/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4048/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4049/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4050/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4051/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4052/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4053/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4054/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4055/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4056/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4057/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4058/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4059/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4060/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4061/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4062/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4063/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4064/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4065/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4066/15000, steps: 113, e: 0.039\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4067/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4068/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4069/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4070/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4071/15000, steps: 600, e: 0.039\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4072/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4073/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4074/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4075/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4076/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4077/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4078/15000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4079/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4080/15000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4081/15000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4082/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4083/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4084/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4085/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4086/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4087/15000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4088/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4089/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4090/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4091/15000, steps: 461, e: 0.038\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4092/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4093/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4094/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4095/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4096/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4097/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4098/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4099/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4100/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4101/15000, steps: 52, e: 0.038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4102/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4103/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4104/15000, steps: 600, e: 0.038\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4105/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4106/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4107/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4108/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4109/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4110/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4111/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4112/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4113/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4114/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4115/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4116/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4117/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4118/15000, steps: 440, e: 0.037\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4119/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4120/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4121/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4122/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4123/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4124/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4125/15000, steps: 52, e: 0.037\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4126/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4127/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4128/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4129/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4130/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4131/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4132/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4133/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4134/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4135/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4136/15000, steps: 52, e: 0.037\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4137/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4138/15000, steps: 600, e: 0.037\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4139/15000, steps: 256, e: 0.036\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4140/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4141/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4142/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4143/15000, steps: 461, e: 0.036\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4144/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4145/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4146/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4147/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 4148/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4149/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4150/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4151/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4152/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4153/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4154/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4155/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4156/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4157/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4158/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4159/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4160/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4161/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4162/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4163/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4164/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4165/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4166/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4167/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4168/15000, steps: 52, e: 0.036\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4169/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4170/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4171/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4172/15000, steps: 600, e: 0.036\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4173/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4174/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4175/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4176/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4177/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4178/15000, steps: 52, e: 0.035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4179/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4180/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4181/15000, steps: 52, e: 0.035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4182/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4183/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4184/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4185/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4186/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4187/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4188/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4189/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4190/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4191/15000, steps: 52, e: 0.035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4192/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4193/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4194/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4195/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4196/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4197/15000, steps: 543, e: 0.035\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4198/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4199/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4200/15000, steps: 188, e: 0.035\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4201/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4202/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4203/15000, steps: 52, e: 0.035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4204/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4205/15000, steps: 52, e: 0.035\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4206/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4207/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4208/15000, steps: 600, e: 0.035\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4209/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4210/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4211/15000, steps: 546, e: 0.034\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4212/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4213/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4214/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4215/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4216/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4217/15000, steps: 52, e: 0.034\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4218/15000, steps: 98, e: 0.034\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4219/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4220/15000, steps: 52, e: 0.034\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4221/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4222/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4223/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4224/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4225/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4226/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4227/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4228/15000, steps: 52, e: 0.034\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4229/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4230/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4231/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4232/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4233/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4234/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4235/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4236/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4237/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4238/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4239/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4240/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4241/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4242/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4243/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4244/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4245/15000, steps: 600, e: 0.034\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4246/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4247/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4248/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4249/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4250/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4251/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4252/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4253/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4254/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4255/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4256/15000, steps: 52, e: 0.033\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4257/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4258/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4259/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4260/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4261/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4262/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4263/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4264/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4265/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4266/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4267/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4268/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4269/15000, steps: 52, e: 0.033\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4270/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4271/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4272/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4273/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4274/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4275/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4276/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4277/15000, steps: 147, e: 0.033\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4278/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4279/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4280/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4281/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4282/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4283/15000, steps: 600, e: 0.033\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4284/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4285/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4286/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4287/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4288/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4289/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4290/15000, steps: 52, e: 0.032\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4291/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4292/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4293/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4294/15000, steps: 326, e: 0.032\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4295/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4296/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4297/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4298/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4299/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4300/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4301/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4302/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4303/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4304/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4305/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4306/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4307/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4308/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4309/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4310/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4311/15000, steps: 52, e: 0.032\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4312/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4313/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4314/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4315/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4316/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4317/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4318/15000, steps: 52, e: 0.032\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4319/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4320/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4321/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4322/15000, steps: 600, e: 0.032\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4323/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4324/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4325/15000, steps: 52, e: 0.031\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4326/15000, steps: 479, e: 0.031\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4327/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4328/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 4329/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4330/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4331/15000, steps: 52, e: 0.031\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4332/15000, steps: 52, e: 0.031\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4333/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4334/15000, steps: 52, e: 0.031\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4335/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4336/15000, steps: 52, e: 0.031\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4337/15000, steps: 455, e: 0.031\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4338/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4339/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4340/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4341/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4342/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4343/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4344/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4345/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4346/15000, steps: 108, e: 0.031\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4347/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4348/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4349/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4350/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4351/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4352/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4353/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4354/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4355/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4356/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4357/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4358/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4359/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4360/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4361/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4362/15000, steps: 600, e: 0.031\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4363/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4364/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4365/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4366/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4367/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4368/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4369/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4370/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4371/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4372/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4373/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4374/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4375/15000, steps: 52, e: 0.03\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4376/15000, steps: 52, e: 0.03\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4377/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4378/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 4379/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4380/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4381/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4382/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4383/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4384/15000, steps: 52, e: 0.03\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4385/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4386/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4387/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4388/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4389/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4390/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4391/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4392/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4393/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4394/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4395/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4396/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4397/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4398/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4399/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4400/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4401/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4402/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4403/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4404/15000, steps: 600, e: 0.03\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4405/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4406/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4407/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4408/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4409/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4410/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4411/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4412/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4413/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4414/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4415/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4416/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4417/15000, steps: 52, e: 0.029\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4418/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4419/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4420/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4421/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4422/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4423/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4424/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4425/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4426/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4427/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4428/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4429/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4430/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4431/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4432/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4433/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4434/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4435/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4436/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4437/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4438/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4439/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4440/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4441/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4442/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4443/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4444/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4445/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4446/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4447/15000, steps: 600, e: 0.029\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4448/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4449/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4450/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4451/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4452/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4453/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4454/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4455/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4456/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4457/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4458/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4459/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4460/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4461/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4462/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4463/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4464/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4465/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4466/15000, steps: 52, e: 0.028\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4467/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4468/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4469/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4470/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4471/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4472/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4473/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4474/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4475/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4476/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4477/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4478/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4479/15000, steps: 347, e: 0.028\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4480/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4481/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4482/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4483/15000, steps: 52, e: 0.028\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4484/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4485/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4486/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4487/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4488/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4489/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 4490/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4491/15000, steps: 600, e: 0.028\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4492/15000, steps: 52, e: 0.027\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4493/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4494/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4495/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4496/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4497/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4498/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4499/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4500/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4501/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4502/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4503/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4504/15000, steps: 360, e: 0.027\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4505/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4506/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4507/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4508/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4509/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4510/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4511/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4512/15000, steps: 52, e: 0.027\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4513/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4514/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4515/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4516/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4517/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4518/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4519/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4520/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4521/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4522/15000, steps: 52, e: 0.027\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4523/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4524/15000, steps: 76, e: 0.027\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4525/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4526/15000, steps: 155, e: 0.027\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4527/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4528/15000, steps: 563, e: 0.027\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4529/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4530/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4531/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4532/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4533/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4534/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4535/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4536/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4537/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4538/15000, steps: 600, e: 0.027\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4539/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4540/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4541/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4542/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4543/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4544/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4545/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4546/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4547/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4548/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4549/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4550/15000, steps: 52, e: 0.026\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4551/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4552/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4553/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4554/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4555/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4556/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4557/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4558/15000, steps: 338, e: 0.026\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 4559/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4560/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4561/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4562/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4563/15000, steps: 52, e: 0.026\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4564/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4565/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4566/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4567/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4568/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4569/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4570/15000, steps: 52, e: 0.026\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4571/15000, steps: 52, e: 0.026\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4572/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4573/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4574/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4575/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4576/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4577/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4578/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4579/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4580/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4581/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4582/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4583/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4584/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4585/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4586/15000, steps: 600, e: 0.026\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4587/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4588/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4589/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4590/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4591/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4592/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4593/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4594/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4595/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4596/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4597/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4598/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4599/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4600/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4601/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4602/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4603/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4604/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4605/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4606/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4607/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4608/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4609/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4610/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4611/15000, steps: 52, e: 0.025\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4612/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4613/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4614/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4615/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4616/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4617/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4618/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4619/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4620/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4621/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4622/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4623/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4624/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4625/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4626/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4627/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4628/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4629/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4630/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4631/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4632/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4633/15000, steps: 52, e: 0.025\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4634/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4635/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4636/15000, steps: 600, e: 0.025\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4637/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4638/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4639/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4640/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4641/15000, steps: 52, e: 0.024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4642/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4643/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4644/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4645/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4646/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4647/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4648/15000, steps: 52, e: 0.024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4649/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4650/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4651/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 4652/15000, steps: 343, e: 0.024\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4653/15000, steps: 52, e: 0.024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4654/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4655/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4656/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4657/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4658/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4659/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4660/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4661/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4662/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4663/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4664/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4665/15000, steps: 52, e: 0.024\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4666/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4667/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4668/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4669/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4670/15000, steps: 52, e: 0.024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4671/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4672/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4673/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4674/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4675/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4676/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4677/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4678/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4679/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4680/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4681/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4682/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4683/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4684/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 4685/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4686/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4687/15000, steps: 600, e: 0.024\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4688/15000, steps: 332, e: 0.024\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4689/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4690/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4691/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4692/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4693/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4694/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4695/15000, steps: 52, e: 0.023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4696/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4697/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4698/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4699/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4700/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4701/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4702/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4703/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4704/15000, steps: 290, e: 0.023\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4705/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4706/15000, steps: 52, e: 0.023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4707/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4708/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4709/15000, steps: 369, e: 0.023\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4710/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4711/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4712/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4713/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4714/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4715/15000, steps: 478, e: 0.023\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4716/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4717/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4718/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4719/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4720/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4721/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4722/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4723/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 4724/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4725/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4726/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4727/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4728/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4729/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 4730/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4731/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4732/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4733/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4734/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 4735/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4736/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4737/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4738/15000, steps: 52, e: 0.023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4739/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4740/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4741/15000, steps: 600, e: 0.023\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4742/15000, steps: 52, e: 0.023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4743/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4744/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4745/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4746/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4747/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4748/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4749/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4750/15000, steps: 52, e: 0.022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4751/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4752/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4753/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4754/15000, steps: 262, e: 0.022\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4755/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4756/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4757/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 4758/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4759/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4760/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4761/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 4762/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4763/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 4764/15000, steps: 52, e: 0.022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 4765/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4766/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 4767/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4768/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4769/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4770/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4771/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4772/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4773/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4774/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4775/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4776/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4777/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4778/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4779/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4780/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4781/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 4782/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4783/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4784/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4785/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4786/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4787/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 4788/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4789/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4790/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 4791/15000, steps: 52, e: 0.022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4792/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 4793/15000, steps: 52, e: 0.022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4794/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4795/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4796/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4797/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4798/15000, steps: 600, e: 0.022\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4799/15000, steps: 52, e: 0.022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4800/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4801/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4802/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4803/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4804/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4805/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4806/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4807/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4808/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4809/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4810/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4811/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4812/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4813/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4814/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4815/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4816/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4817/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4818/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4819/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4820/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4821/15000, steps: 52, e: 0.021\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4822/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4823/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4824/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4825/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4826/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4827/15000, steps: 52, e: 0.021\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4828/15000, steps: 52, e: 0.021\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4829/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 4830/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4831/15000, steps: 52, e: 0.021\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4832/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4833/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4834/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4835/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4836/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4837/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4838/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4839/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 4840/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4841/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4842/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4843/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4844/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4845/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 4846/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4847/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4848/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4849/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4850/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4851/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4852/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4853/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4854/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4855/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4856/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4857/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4858/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4859/15000, steps: 600, e: 0.021\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 4860/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4861/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4862/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4863/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4864/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 4865/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 4866/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4867/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 4868/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4869/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4870/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4871/15000, steps: 60, e: 0.02\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4872/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4873/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4874/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4875/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4876/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 4877/15000, steps: 52, e: 0.02\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4878/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 4879/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4880/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4881/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4882/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4883/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 4884/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4885/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 4886/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4887/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4888/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4889/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4890/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4891/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4892/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 4893/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4894/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4895/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4896/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 4897/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4898/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4899/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4900/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 4901/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4902/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 4903/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4904/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 4905/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 4906/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 4907/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4908/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 4909/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4910/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4911/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4912/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4913/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4914/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4915/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 4916/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4917/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 4918/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4919/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4920/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 4921/15000, steps: 600, e: 0.02\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 4922/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 4923/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4924/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 4925/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4926/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 4927/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4928/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 4929/15000, steps: 52, e: 0.019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 4930/15000, steps: 52, e: 0.019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 4931/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 4932/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4933/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4934/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 4935/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 4936/15000, steps: 521, e: 0.019\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4937/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4938/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4939/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 4940/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 4941/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4942/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 4943/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 4944/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4945/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4946/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 4947/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4948/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4949/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 4950/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 4951/15000, steps: 52, e: 0.019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 4952/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4953/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 4954/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 4955/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 4956/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 4957/15000, steps: 52, e: 0.019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4958/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4959/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 4960/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 4961/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4962/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 4963/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 4964/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 4965/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 4966/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 4967/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4968/15000, steps: 52, e: 0.019\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 4969/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 4970/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4971/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 4972/15000, steps: 52, e: 0.019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4973/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4974/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 4975/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 4976/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 4977/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4978/15000, steps: 52, e: 0.019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 4979/15000, steps: 52, e: 0.019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 4980/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 4981/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 4982/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 4983/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4984/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4985/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 4986/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 4987/15000, steps: 600, e: 0.019\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 4988/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 4989/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 4990/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 4991/15000, steps: 52, e: 0.018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 4992/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 4993/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 4994/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 4995/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 4996/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 4997/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 4998/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 4999/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5000/15000, steps: 168, e: 0.018\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5001/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5002/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5003/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5004/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5005/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5006/15000, steps: 52, e: 0.018\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5007/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5008/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5009/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5010/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5011/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5012/15000, steps: 52, e: 0.018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5013/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5014/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5015/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5016/15000, steps: 52, e: 0.018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5017/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5018/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5019/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5020/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5021/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5022/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5023/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5024/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5025/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5026/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5027/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5028/15000, steps: 52, e: 0.018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5029/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5030/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5031/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5032/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5033/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5034/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5035/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5036/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5037/15000, steps: 52, e: 0.018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5038/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5039/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5040/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5041/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5042/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5043/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5044/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5045/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5046/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5047/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5048/15000, steps: 52, e: 0.018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5049/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5050/15000, steps: 52, e: 0.018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5051/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5052/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5053/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5054/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5055/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5056/15000, steps: 600, e: 0.018\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5057/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5058/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5059/15000, steps: 151, e: 0.017\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5060/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5061/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5062/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5063/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5064/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5065/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5066/15000, steps: 52, e: 0.017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5067/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5068/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5069/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5070/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5071/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5072/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5073/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5074/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5075/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5076/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5077/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5078/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5079/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5080/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5081/15000, steps: 52, e: 0.017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5082/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5083/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5084/15000, steps: 52, e: 0.017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5085/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5086/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5087/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5088/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5089/15000, steps: 52, e: 0.017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5090/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5091/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5092/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5093/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5094/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5095/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5096/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5097/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5098/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5099/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5100/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5101/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5102/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5103/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5104/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5105/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5106/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5107/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5108/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5109/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5110/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5111/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5112/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5113/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5114/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5115/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5116/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5117/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5118/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5119/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5120/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5121/15000, steps: 52, e: 0.017\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5122/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5123/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5124/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5125/15000, steps: 377, e: 0.017\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5126/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5127/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5128/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5129/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5130/15000, steps: 600, e: 0.017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5131/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5132/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5133/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5134/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5135/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5136/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5137/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5138/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5139/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5140/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5141/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5142/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5143/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5144/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5145/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5146/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5147/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5148/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5149/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5150/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5151/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5152/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5153/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5154/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5155/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5156/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5157/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5158/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5159/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5160/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5161/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5162/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5163/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5164/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5165/15000, steps: 333, e: 0.016\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5166/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5167/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5168/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5169/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5170/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5171/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5172/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5173/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5174/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5175/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5176/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5177/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5178/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5179/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5180/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5181/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5182/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5183/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5184/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5185/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5186/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5187/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5188/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5189/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5190/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5191/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5192/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5193/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5194/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5195/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5196/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5197/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5198/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5199/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5200/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5201/15000, steps: 52, e: 0.016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5202/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5203/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5204/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5205/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5206/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5207/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5208/15000, steps: 600, e: 0.016\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5209/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5210/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5211/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5212/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5213/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5214/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5215/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5216/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5217/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5218/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5219/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5220/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5221/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5222/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5223/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5224/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5225/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5226/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5227/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5228/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5229/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5230/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5231/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5232/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5233/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5234/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5235/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5236/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5237/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5238/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5239/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5240/15000, steps: 52, e: 0.015\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5241/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5242/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5243/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5244/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5245/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5246/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5247/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5248/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5249/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5250/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5251/15000, steps: 134, e: 0.015\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5252/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5253/15000, steps: 52, e: 0.015\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5254/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5255/15000, steps: 52, e: 0.015\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5256/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5257/15000, steps: 52, e: 0.015\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5258/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5259/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5260/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5261/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5262/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5263/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5264/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5265/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5266/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5267/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5268/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5269/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5270/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5271/15000, steps: 232, e: 0.015\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5272/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5273/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5274/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5275/15000, steps: 52, e: 0.015\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5276/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5277/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5278/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5279/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5280/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5281/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5282/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5283/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5284/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5285/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5286/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5287/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5288/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5289/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5290/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5291/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5292/15000, steps: 600, e: 0.015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5293/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5294/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5295/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5296/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5297/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5298/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5299/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5300/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5301/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5302/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5303/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5304/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5305/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5306/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5307/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5308/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5309/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5310/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5311/15000, steps: 81, e: 0.014\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5312/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5313/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5314/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5315/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5316/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5317/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5318/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5319/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5320/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5321/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5322/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5323/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5324/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5325/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5326/15000, steps: 215, e: 0.014\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5327/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5328/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5329/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5330/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5331/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5332/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5333/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5334/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5335/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5336/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5337/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5338/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5339/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5340/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5341/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5342/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5343/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5344/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5345/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5346/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5347/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5348/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5349/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5350/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5351/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5352/15000, steps: 288, e: 0.014\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5353/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5354/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5355/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5356/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5357/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5358/15000, steps: 208, e: 0.014\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5359/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5360/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5361/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5362/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5363/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5364/15000, steps: 420, e: 0.014\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5365/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5366/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5367/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5368/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5369/15000, steps: 52, e: 0.014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5370/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5371/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5372/15000, steps: 204, e: 0.014\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5373/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5374/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5375/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5376/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5377/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5378/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5379/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5380/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5381/15000, steps: 600, e: 0.014\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5382/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5383/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5384/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5385/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5386/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5387/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5388/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5389/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5390/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5391/15000, steps: 153, e: 0.013\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5392/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5393/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5394/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5395/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5396/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5397/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5398/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5399/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5400/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5401/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5402/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5403/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5404/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5405/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5406/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5407/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5408/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5409/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5410/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5411/15000, steps: 562, e: 0.013\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5412/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5413/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5414/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5415/15000, steps: 338, e: 0.013\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5416/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5417/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5418/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5419/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5420/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5421/15000, steps: 511, e: 0.013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5422/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5423/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5424/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5425/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5426/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5427/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5428/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5429/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5430/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5431/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5432/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5433/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5434/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5435/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5436/15000, steps: 52, e: 0.013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5437/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5438/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5439/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5440/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5441/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5442/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5443/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5444/15000, steps: 214, e: 0.013\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5445/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5446/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5447/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5448/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5449/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5450/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5451/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5452/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5453/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5454/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5455/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5456/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5457/15000, steps: 52, e: 0.013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5458/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5459/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5460/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5461/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5462/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5463/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5464/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5465/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5466/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5467/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5468/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5469/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5470/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5471/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5472/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5473/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5474/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5475/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5476/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5477/15000, steps: 600, e: 0.013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5478/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5479/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5480/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5481/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5482/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5483/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5484/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5485/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5486/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5487/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5488/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5489/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5490/15000, steps: 209, e: 0.012\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5491/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5492/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5493/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5494/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5495/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5496/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5497/15000, steps: 52, e: 0.012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5498/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5499/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5500/15000, steps: 52, e: 0.012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5501/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5502/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5503/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5504/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5505/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5506/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5507/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5508/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5509/15000, steps: 52, e: 0.012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5510/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5511/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5512/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5513/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5514/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5515/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5516/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5517/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5518/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5519/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5520/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5521/15000, steps: 483, e: 0.012\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5522/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5523/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5524/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5525/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5526/15000, steps: 233, e: 0.012\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5527/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5528/15000, steps: 52, e: 0.012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5529/15000, steps: 493, e: 0.012\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5530/15000, steps: 52, e: 0.012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5531/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5532/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5533/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5534/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5535/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5536/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5537/15000, steps: 85, e: 0.012\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5538/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5539/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5540/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5541/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5542/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5543/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5544/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5545/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5546/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5547/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5548/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5549/15000, steps: 52, e: 0.012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5550/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5551/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5552/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5553/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5554/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5555/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5556/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5557/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5558/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5559/15000, steps: 358, e: 0.012\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5560/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5561/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5562/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5563/15000, steps: 471, e: 0.012\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5564/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5565/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5566/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5567/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5568/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5569/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5570/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5571/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5572/15000, steps: 593, e: 0.012\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5573/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5574/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5575/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5576/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5577/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5578/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5579/15000, steps: 397, e: 0.012\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5580/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5581/15000, steps: 600, e: 0.012\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5582/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5583/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5584/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5585/15000, steps: 52, e: 0.011\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5586/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5587/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5588/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5589/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5590/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5591/15000, steps: 389, e: 0.011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5592/15000, steps: 61, e: 0.011\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5593/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5594/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5595/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5596/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5597/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5598/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5599/15000, steps: 314, e: 0.011\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5600/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5601/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5602/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5603/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5604/15000, steps: 351, e: 0.011\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5605/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5606/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5607/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5608/15000, steps: 56, e: 0.011\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5609/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5610/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5611/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5612/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5613/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5614/15000, steps: 52, e: 0.011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5615/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5616/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5617/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5618/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5619/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5620/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5621/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5622/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5623/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5624/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5625/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5626/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5627/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5628/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5629/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5630/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5631/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5632/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5633/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5634/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5635/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5636/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5637/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5638/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5639/15000, steps: 350, e: 0.011\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5640/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5641/15000, steps: 52, e: 0.011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 5642/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5643/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5644/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5645/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5646/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5647/15000, steps: 223, e: 0.011\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5648/15000, steps: 52, e: 0.011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5649/15000, steps: 52, e: 0.011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5650/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5651/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5652/15000, steps: 52, e: 0.011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5653/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5654/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5655/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5656/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5657/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5658/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5659/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 5660/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5661/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5662/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5663/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5664/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5665/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5666/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5667/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5668/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5669/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5670/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5671/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5672/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5673/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5674/15000, steps: 52, e: 0.011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5675/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5676/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 5677/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5678/15000, steps: 150, e: 0.011\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5679/15000, steps: 79, e: 0.011\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5680/15000, steps: 182, e: 0.011\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 5681/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5682/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5683/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5684/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5685/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5686/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5687/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5688/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5689/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5690/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5691/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5692/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5693/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5694/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5695/15000, steps: 600, e: 0.011\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5696/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5697/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5698/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5699/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5700/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5701/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5702/15000, steps: 481, e: 0.01\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5703/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5704/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5705/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5706/15000, steps: 52, e: 0.01\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5707/15000, steps: 64, e: 0.01\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5708/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5709/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5710/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5711/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5712/15000, steps: 52, e: 0.01\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5713/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5714/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5715/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5716/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5717/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5718/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5719/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5720/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5721/15000, steps: 52, e: 0.01\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5722/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5723/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5724/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 5725/15000, steps: 52, e: 0.01\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5726/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5727/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5728/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5729/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5730/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5731/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5732/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5733/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5734/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5735/15000, steps: 184, e: 0.01\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5736/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5737/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5738/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5739/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5740/15000, steps: 52, e: 0.01\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5741/15000, steps: 397, e: 0.01\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5742/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5743/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5744/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5745/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5746/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5747/15000, steps: 457, e: 0.01\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5748/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5749/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5750/15000, steps: 52, e: 0.01\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5751/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5752/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5753/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 5754/15000, steps: 52, e: 0.01\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 5755/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5756/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5757/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5758/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5759/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5760/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5761/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5762/15000, steps: 600, e: 0.01\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 5763/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5764/15000, steps: 375, e: 0.0099\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5765/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5766/15000, steps: 291, e: 0.0099\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5767/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5768/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5769/15000, steps: 52, e: 0.0099\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5770/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5771/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5772/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 5773/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5774/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5775/15000, steps: 600, e: 0.0099\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5776/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5777/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5778/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5779/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5780/15000, steps: 52, e: 0.0098\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 5781/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5782/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5783/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 5784/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5785/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5786/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5787/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5788/15000, steps: 600, e: 0.0098\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 5789/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5790/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5791/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5792/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5793/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5794/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5795/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5796/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5797/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 5798/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5799/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5800/15000, steps: 600, e: 0.0097\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5801/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5802/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5803/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5804/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5805/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5806/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5807/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5808/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 5809/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5810/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5811/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5812/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5813/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5814/15000, steps: 600, e: 0.0096\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5815/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 5816/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5817/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5818/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5819/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5820/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5821/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5822/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5823/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5824/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5825/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5826/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5827/15000, steps: 600, e: 0.0095\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5828/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 5829/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5830/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 5831/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5832/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5833/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5834/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5835/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5836/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5837/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5838/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5839/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5840/15000, steps: 600, e: 0.0094\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5841/15000, steps: 52, e: 0.0093\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5842/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5843/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5844/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5845/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 5846/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5847/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5848/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5849/15000, steps: 52, e: 0.0093\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5850/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5851/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5852/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5853/15000, steps: 600, e: 0.0093\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5854/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 5855/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5856/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5857/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5858/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 5859/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 5860/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5861/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5862/15000, steps: 557, e: 0.0092\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 5863/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5864/15000, steps: 52, e: 0.0092\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 5865/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5866/15000, steps: 234, e: 0.0092\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 5867/15000, steps: 600, e: 0.0092\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5868/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5869/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5870/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5871/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5872/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5873/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5874/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5875/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 5876/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5877/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 5878/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5879/15000, steps: 52, e: 0.0091\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5880/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5881/15000, steps: 600, e: 0.0091\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5882/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5883/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 5884/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5885/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 5886/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 5887/15000, steps: 164, e: 0.009\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 5888/15000, steps: 235, e: 0.009\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5889/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5890/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 5891/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 5892/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 5893/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5894/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5895/15000, steps: 600, e: 0.009\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5896/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 5897/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 5898/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 5899/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5900/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5901/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5902/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 5903/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 5904/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5905/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5906/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 5907/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5908/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5909/15000, steps: 600, e: 0.0089\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 5910/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5911/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5912/15000, steps: 103, e: 0.0088\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5913/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5914/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5915/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5916/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5917/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5918/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5919/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5920/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 5921/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5922/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5923/15000, steps: 600, e: 0.0088\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5924/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5925/15000, steps: 467, e: 0.0087\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 5926/15000, steps: 194, e: 0.0087\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5927/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5928/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5929/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5930/15000, steps: 504, e: 0.0087\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5931/15000, steps: 242, e: 0.0087\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5932/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5933/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5934/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5935/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 5936/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 5937/15000, steps: 600, e: 0.0087\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5938/15000, steps: 52, e: 0.0086\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5939/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5940/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5941/15000, steps: 52, e: 0.0086\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 5942/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 5943/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 5944/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 5945/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5946/15000, steps: 52, e: 0.0086\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5947/15000, steps: 516, e: 0.0086\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 5948/15000, steps: 60, e: 0.0086\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5949/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 5950/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5951/15000, steps: 485, e: 0.0086\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 5952/15000, steps: 600, e: 0.0086\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5953/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 5954/15000, steps: 52, e: 0.0085\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 5955/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 5956/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 5957/15000, steps: 52, e: 0.0085\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 5958/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 5959/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5960/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 5961/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 5962/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 5963/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 5964/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5965/15000, steps: 600, e: 0.0085\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 5966/15000, steps: 230, e: 0.0085\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5967/15000, steps: 52, e: 0.0084\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5968/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 5969/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5970/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 5971/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5972/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 5973/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 5974/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 5975/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 5976/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 5977/15000, steps: 52, e: 0.0084\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5978/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 5979/15000, steps: 182, e: 0.0084\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5980/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 5981/15000, steps: 600, e: 0.0084\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 5982/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 5983/15000, steps: 594, e: 0.0083\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 5984/15000, steps: 52, e: 0.0083\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 5985/15000, steps: 74, e: 0.0083\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5986/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 5987/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 5988/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 5989/15000, steps: 141, e: 0.0083\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 5990/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 5991/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 5992/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 5993/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 5994/15000, steps: 94, e: 0.0083\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 5995/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 5996/15000, steps: 600, e: 0.0083\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 5997/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 5998/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 5999/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6000/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6001/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6002/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6003/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6004/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6005/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6006/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6007/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6008/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6009/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6010/15000, steps: 52, e: 0.0082\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6011/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6012/15000, steps: 600, e: 0.0082\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6013/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6014/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6015/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6016/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6017/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6018/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6019/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6020/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6021/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6022/15000, steps: 355, e: 0.0081\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6023/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6024/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6025/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6026/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6027/15000, steps: 600, e: 0.0081\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6028/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6029/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6030/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6031/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6032/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6033/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6034/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6035/15000, steps: 52, e: 0.008\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6036/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6037/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6038/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6039/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6040/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6041/15000, steps: 52, e: 0.008\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6042/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6043/15000, steps: 600, e: 0.008\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6044/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6045/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6046/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6047/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6048/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6049/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6050/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6051/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6052/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6053/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6054/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6055/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 6056/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6057/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6058/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6059/15000, steps: 600, e: 0.0079\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6060/15000, steps: 73, e: 0.0078\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6061/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6062/15000, steps: 52, e: 0.0078\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6063/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6064/15000, steps: 330, e: 0.0078\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6065/15000, steps: 52, e: 0.0078\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6066/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6067/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6068/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6069/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6070/15000, steps: 218, e: 0.0078\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6071/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6072/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6073/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6074/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6075/15000, steps: 600, e: 0.0078\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6076/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6077/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6078/15000, steps: 52, e: 0.0077\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6079/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6080/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6081/15000, steps: 408, e: 0.0077\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 6082/15000, steps: 52, e: 0.0077\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6083/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6084/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6085/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6086/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6087/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6088/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6089/15000, steps: 600, e: 0.0077\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6090/15000, steps: 194, e: 0.0077\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6091/15000, steps: 52, e: 0.0077\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6092/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6093/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6094/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6095/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6096/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6097/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6098/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6099/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6100/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6101/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6102/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6103/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6104/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6105/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6106/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6107/15000, steps: 600, e: 0.0076\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6108/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6109/15000, steps: 52, e: 0.0075\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6110/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6111/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6112/15000, steps: 369, e: 0.0075\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6113/15000, steps: 208, e: 0.0075\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6114/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6115/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6116/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6117/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6118/15000, steps: 52, e: 0.0075\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6119/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6120/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6121/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6122/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6123/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6124/15000, steps: 600, e: 0.0075\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6125/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6126/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6127/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6128/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6129/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6130/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6131/15000, steps: 352, e: 0.0074\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6132/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6133/15000, steps: 149, e: 0.0074\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6134/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6135/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6136/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6137/15000, steps: 52, e: 0.0074\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6138/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6139/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6140/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6141/15000, steps: 600, e: 0.0074\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6142/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6143/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6144/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6145/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6146/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6147/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6148/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6149/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6150/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6151/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6152/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6153/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6154/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6155/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6156/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6157/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6158/15000, steps: 600, e: 0.0073\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6159/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6160/15000, steps: 70, e: 0.0072\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6161/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6162/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6163/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 6164/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6165/15000, steps: 169, e: 0.0072\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6166/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6167/15000, steps: 274, e: 0.0072\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6168/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6169/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6170/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6171/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6172/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6173/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6174/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6175/15000, steps: 600, e: 0.0072\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6176/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6177/15000, steps: 52, e: 0.0071\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6178/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6179/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6180/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6181/15000, steps: 472, e: 0.0071\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6182/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6183/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6184/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6185/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6186/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6187/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 6188/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6189/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6190/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6191/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6192/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6193/15000, steps: 600, e: 0.0071\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6194/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6195/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6196/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6197/15000, steps: 52, e: 0.007\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6198/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6199/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6200/15000, steps: 52, e: 0.007\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6201/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6202/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6203/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6204/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6205/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6206/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6207/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6208/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6209/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6210/15000, steps: 600, e: 0.007\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6211/15000, steps: 52, e: 0.007\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6212/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6213/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6214/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6215/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6216/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6217/15000, steps: 52, e: 0.0069\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6218/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6219/15000, steps: 188, e: 0.0069\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6220/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6221/15000, steps: 52, e: 0.0069\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6222/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6223/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6224/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6225/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6226/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6227/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6228/15000, steps: 600, e: 0.0069\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6229/15000, steps: 52, e: 0.0069\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6230/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6231/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6232/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6233/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6234/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6235/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6236/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6237/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6238/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6239/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6240/15000, steps: 52, e: 0.0068\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6241/15000, steps: 52, e: 0.0068\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6242/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6243/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6244/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6245/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6246/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6247/15000, steps: 600, e: 0.0068\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6248/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6249/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6250/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6251/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6252/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6253/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6254/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6255/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6256/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6257/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6258/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6259/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6260/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6261/15000, steps: 455, e: 0.0067\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6262/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6263/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6264/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6265/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6266/15000, steps: 600, e: 0.0067\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6267/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6268/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6269/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6270/15000, steps: 52, e: 0.0066\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6271/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6272/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6273/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6274/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6275/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6276/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6277/15000, steps: 136, e: 0.0066\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6278/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6279/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6280/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6281/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6282/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6283/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6284/15000, steps: 600, e: 0.0066\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6285/15000, steps: 52, e: 0.0066\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6286/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6287/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6288/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6289/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6290/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6291/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6292/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6293/15000, steps: 278, e: 0.0065\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6294/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 6295/15000, steps: 52, e: 0.0065\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6296/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6297/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6298/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6299/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6300/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6301/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6302/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6303/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6304/15000, steps: 600, e: 0.0065\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6305/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6306/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6307/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6308/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6309/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6310/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6311/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6312/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6313/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6314/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6315/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6316/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6317/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 6318/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6319/15000, steps: 149, e: 0.0064\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6320/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6321/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6322/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6323/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6324/15000, steps: 600, e: 0.0064\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6325/15000, steps: 53, e: 0.0063\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6326/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6327/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6328/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6329/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6330/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6331/15000, steps: 52, e: 0.0063\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6332/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6333/15000, steps: 52, e: 0.0063\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6334/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6335/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 6336/15000, steps: 52, e: 0.0063\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6337/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6338/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6339/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6340/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6341/15000, steps: 562, e: 0.0063\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6342/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6343/15000, steps: 600, e: 0.0063\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6344/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6345/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6346/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6347/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6348/15000, steps: 402, e: 0.0062\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 6349/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6350/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6351/15000, steps: 159, e: 0.0062\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6352/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6353/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6354/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6355/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6356/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6357/15000, steps: 52, e: 0.0062\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6358/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6359/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6360/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6361/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6362/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6363/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6364/15000, steps: 600, e: 0.0062\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6365/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6366/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6367/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6368/15000, steps: 52, e: 0.0061\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6369/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6370/15000, steps: 69, e: 0.0061\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6371/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6372/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6373/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6374/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6375/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6376/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6377/15000, steps: 412, e: 0.0061\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6378/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6379/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6380/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6381/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6382/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6383/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 6384/15000, steps: 600, e: 0.0061\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6385/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6386/15000, steps: 52, e: 0.006\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6387/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6388/15000, steps: 52, e: 0.006\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6389/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6390/15000, steps: 272, e: 0.006\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6391/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6392/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6393/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6394/15000, steps: 554, e: 0.006\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6395/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6396/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6397/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6398/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6399/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6400/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6401/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6402/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6403/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6404/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6405/15000, steps: 600, e: 0.006\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6406/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6407/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6408/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6409/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6410/15000, steps: 52, e: 0.0059\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6411/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6412/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6413/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6414/15000, steps: 591, e: 0.0059\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6415/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6416/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6417/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6418/15000, steps: 52, e: 0.0059\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6419/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6420/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6421/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6422/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6423/15000, steps: 52, e: 0.0059\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6424/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6425/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6426/15000, steps: 600, e: 0.0059\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6427/15000, steps: 52, e: 0.0058\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6428/15000, steps: 72, e: 0.0058\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 6429/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6430/15000, steps: 52, e: 0.0058\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6431/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6432/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6433/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6434/15000, steps: 283, e: 0.0058\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6435/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6436/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6437/15000, steps: 52, e: 0.0058\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6438/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6439/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 6440/15000, steps: 52, e: 0.0058\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6441/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6442/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6443/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6444/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6445/15000, steps: 52, e: 0.0058\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6446/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6447/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6448/15000, steps: 600, e: 0.0058\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6449/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6450/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6451/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6452/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6453/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6454/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6455/15000, steps: 332, e: 0.0057\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6456/15000, steps: 52, e: 0.0057\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6457/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6458/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6459/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6460/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6461/15000, steps: 537, e: 0.0057\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6462/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6463/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6464/15000, steps: 52, e: 0.0057\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6465/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6466/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6467/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6468/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6469/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6470/15000, steps: 600, e: 0.0057\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6471/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6472/15000, steps: 327, e: 0.0056\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6473/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6474/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6475/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6476/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6477/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6478/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 6479/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6480/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6481/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6482/15000, steps: 190, e: 0.0056\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6483/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6484/15000, steps: 52, e: 0.0056\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6485/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6486/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6487/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6488/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6489/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6490/15000, steps: 52, e: 0.0056\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6491/15000, steps: 422, e: 0.0056\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6492/15000, steps: 600, e: 0.0056\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6493/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 6494/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6495/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6496/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6497/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6498/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6499/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6500/15000, steps: 52, e: 0.0055\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6501/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6502/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6503/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6504/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6505/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6506/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6507/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6508/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6509/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6510/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6511/15000, steps: 52, e: 0.0055\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6512/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6513/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6514/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6515/15000, steps: 600, e: 0.0055\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6516/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6517/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6518/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6519/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6520/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 6521/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6522/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6523/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6524/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6525/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6526/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6527/15000, steps: 254, e: 0.0054\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6528/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6529/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6530/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6531/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6532/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6533/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6534/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6535/15000, steps: 52, e: 0.0054\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6536/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6537/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6538/15000, steps: 600, e: 0.0054\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6539/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6540/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 6541/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6542/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6543/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6544/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6545/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6546/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 6547/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6548/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6549/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6550/15000, steps: 52, e: 0.0053\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6551/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6552/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6553/15000, steps: 52, e: 0.0053\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6554/15000, steps: 481, e: 0.0053\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6555/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6556/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6557/15000, steps: 486, e: 0.0053\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6558/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6559/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6560/15000, steps: 600, e: 0.0053\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6561/15000, steps: 52, e: 0.0053\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6562/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6563/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6564/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6565/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6566/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6567/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6568/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6569/15000, steps: 539, e: 0.0052\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6570/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6571/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6572/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6573/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6574/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6575/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6576/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6577/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6578/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6579/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6580/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6581/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6582/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6583/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6584/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6585/15000, steps: 600, e: 0.0052\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6586/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6587/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6588/15000, steps: 52, e: 0.0051\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6589/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6590/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6591/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6592/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6593/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6594/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6595/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6596/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6597/15000, steps: 87, e: 0.0051\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6598/15000, steps: 52, e: 0.0051\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6599/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6600/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6601/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6602/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6603/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6604/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6605/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6606/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6607/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6608/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6609/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6610/15000, steps: 600, e: 0.0051\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6611/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6612/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6613/15000, steps: 52, e: 0.005\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6614/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6615/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6616/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6617/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6618/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6619/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6620/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6621/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6622/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6623/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6624/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6625/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6626/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6627/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6628/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6629/15000, steps: 52, e: 0.005\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6630/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6631/15000, steps: 52, e: 0.005\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6632/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6633/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6634/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6635/15000, steps: 600, e: 0.005\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6636/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6637/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6638/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6639/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6640/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6641/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6642/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6643/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6644/15000, steps: 294, e: 0.0049\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6645/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6646/15000, steps: 478, e: 0.0049\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6647/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6648/15000, steps: 52, e: 0.0049\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6649/15000, steps: 336, e: 0.0049\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6650/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6651/15000, steps: 52, e: 0.0049\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6652/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6653/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6654/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6655/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6656/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6657/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6658/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6659/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6660/15000, steps: 600, e: 0.0049\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6661/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6662/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6663/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6664/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6665/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6666/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6667/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6668/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6669/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6670/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6671/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6672/15000, steps: 52, e: 0.0048\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6673/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6674/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 6675/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6676/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6677/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6678/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6679/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6680/15000, steps: 52, e: 0.0048\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6681/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6682/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6683/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6684/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6685/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6686/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6687/15000, steps: 600, e: 0.0048\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6688/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6689/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6690/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6691/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6692/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6693/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6694/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6695/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6696/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6697/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6698/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 6699/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6700/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6701/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6702/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6703/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6704/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6705/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6706/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6707/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 6708/15000, steps: 52, e: 0.0047\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6709/15000, steps: 52, e: 0.0047\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6710/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 6711/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6712/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6713/15000, steps: 600, e: 0.0047\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6714/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6715/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6716/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6717/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6718/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6719/15000, steps: 52, e: 0.0046\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6720/15000, steps: 52, e: 0.0046\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6721/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6722/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6723/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6724/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6725/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 6726/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6727/15000, steps: 52, e: 0.0046\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6728/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6729/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6730/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6731/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6732/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6733/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 6734/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6735/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6736/15000, steps: 345, e: 0.0046\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6737/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6738/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6739/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6740/15000, steps: 600, e: 0.0046\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6741/15000, steps: 52, e: 0.0045\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6742/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6743/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6744/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6745/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6746/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6747/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6748/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6749/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6750/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6751/15000, steps: 515, e: 0.0045\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6752/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 6753/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6754/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6755/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6756/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6757/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6758/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6759/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6760/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6761/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 6762/15000, steps: 52, e: 0.0045\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6763/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6764/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6765/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6766/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6767/15000, steps: 600, e: 0.0045\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6768/15000, steps: 52, e: 0.0045\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6769/15000, steps: 52, e: 0.0044\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6770/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6771/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6772/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6773/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 6774/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 6775/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6776/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6777/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6778/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6779/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6780/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6781/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6782/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6783/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6784/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6785/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6786/15000, steps: 52, e: 0.0044\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6787/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 6788/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6789/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6790/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6791/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6792/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6793/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6794/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6795/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 6796/15000, steps: 600, e: 0.0044\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6797/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6798/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6799/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6800/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6801/15000, steps: 52, e: 0.0043\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6802/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6803/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6804/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6805/15000, steps: 180, e: 0.0043\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 6806/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 6807/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6808/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6809/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6810/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6811/15000, steps: 385, e: 0.0043\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6812/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6813/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6814/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6815/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6816/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6817/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6818/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6819/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6820/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6821/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6822/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6823/15000, steps: 363, e: 0.0043\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6824/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6825/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6826/15000, steps: 600, e: 0.0043\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6827/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 6828/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6829/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6830/15000, steps: 234, e: 0.0042\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6831/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6832/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6833/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6834/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6835/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6836/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6837/15000, steps: 371, e: 0.0042\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6838/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6839/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6840/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6841/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6842/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 6843/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6844/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6845/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6846/15000, steps: 558, e: 0.0042\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6847/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6848/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 6849/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6850/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6851/15000, steps: 52, e: 0.0042\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6852/15000, steps: 68, e: 0.0042\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6853/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6854/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6855/15000, steps: 600, e: 0.0042\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6856/15000, steps: 52, e: 0.0041\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6857/15000, steps: 52, e: 0.0041\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 6858/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6859/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6860/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6861/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 6862/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6863/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6864/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6865/15000, steps: 563, e: 0.0041\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6866/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6867/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 6868/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6869/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6870/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6871/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6872/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6873/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 6874/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6875/15000, steps: 52, e: 0.0041\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 6876/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6877/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6878/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6879/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6880/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6881/15000, steps: 280, e: 0.0041\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6882/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6883/15000, steps: 544, e: 0.0041\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6884/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6885/15000, steps: 600, e: 0.0041\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 6886/15000, steps: 52, e: 0.0041\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6887/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6888/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 6889/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6890/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6891/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6892/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6893/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6894/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 6895/15000, steps: 52, e: 0.004\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 6896/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6897/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6898/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6899/15000, steps: 52, e: 0.004\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6900/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6901/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 6902/15000, steps: 52, e: 0.004\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6903/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6904/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6905/15000, steps: 52, e: 0.004\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6906/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 6907/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6908/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 6909/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6910/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6911/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6912/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6913/15000, steps: 227, e: 0.004\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6914/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6915/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6916/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 6917/15000, steps: 600, e: 0.004\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6918/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6919/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 6920/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 6921/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6922/15000, steps: 52, e: 0.0039\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6923/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6924/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6925/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6926/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6927/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 6928/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6929/15000, steps: 215, e: 0.0039\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 6930/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6931/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 6932/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 6933/15000, steps: 416, e: 0.0039\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6934/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6935/15000, steps: 52, e: 0.0039\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 6936/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6937/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 6938/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 6939/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6940/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6941/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 6942/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 6943/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 6944/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6945/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 6946/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 6947/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 6948/15000, steps: 600, e: 0.0039\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 6949/15000, steps: 52, e: 0.0039\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 6950/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 6951/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 6952/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6953/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 6954/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 6955/15000, steps: 52, e: 0.0038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 6956/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6957/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 6958/15000, steps: 52, e: 0.0038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 6959/15000, steps: 52, e: 0.0038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 6960/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 6961/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6962/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 6963/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 6964/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6965/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 6966/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 6967/15000, steps: 52, e: 0.0038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 6968/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 6969/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 6970/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6971/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 6972/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 6973/15000, steps: 265, e: 0.0038\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 6974/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 6975/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 6976/15000, steps: 52, e: 0.0038\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 6977/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 6978/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 6979/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 6980/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6981/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 6982/15000, steps: 600, e: 0.0038\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 6983/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 6984/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 6985/15000, steps: 52, e: 0.0037\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 6986/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 6987/15000, steps: 52, e: 0.0037\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6988/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 6989/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 6990/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6991/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 6992/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 6993/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 6994/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 6995/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 6996/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 6997/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 6998/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 6999/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7000/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7001/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7002/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7003/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7004/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7005/15000, steps: 484, e: 0.0037\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 7006/15000, steps: 258, e: 0.0037\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7007/15000, steps: 52, e: 0.0037\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7008/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7009/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7010/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7011/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7012/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7013/15000, steps: 299, e: 0.0037\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7014/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7015/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7016/15000, steps: 600, e: 0.0037\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 7017/15000, steps: 52, e: 0.0036\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7018/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 7019/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7020/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 7021/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7022/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7023/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7024/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7025/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7026/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7027/15000, steps: 328, e: 0.0036\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 7028/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7029/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7030/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7031/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7032/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7033/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7034/15000, steps: 577, e: 0.0036\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7035/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7036/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7037/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7038/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7039/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 7040/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7041/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7042/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7043/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7044/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7045/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7046/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7047/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7048/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7049/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7050/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7051/15000, steps: 600, e: 0.0036\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7052/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7053/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7054/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7055/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7056/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7057/15000, steps: 52, e: 0.0035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7058/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7059/15000, steps: 52, e: 0.0035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7060/15000, steps: 52, e: 0.0035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7061/15000, steps: 52, e: 0.0035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7062/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7063/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7064/15000, steps: 52, e: 0.0035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7065/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7066/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7067/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7068/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7069/15000, steps: 52, e: 0.0035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7070/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7071/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7072/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7073/15000, steps: 243, e: 0.0035\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7074/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7075/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7076/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7077/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7078/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7079/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7080/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7081/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7082/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7083/15000, steps: 52, e: 0.0035\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7084/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7085/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7086/15000, steps: 600, e: 0.0035\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7087/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7088/15000, steps: 474, e: 0.0034\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 7089/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7090/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7091/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7092/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7093/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7094/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7095/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 7096/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7097/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7098/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7099/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7100/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7101/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 7102/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7103/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7104/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7105/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7106/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7107/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7108/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7109/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7110/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7111/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7112/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7113/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7114/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7115/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7116/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7117/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7118/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7119/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 7120/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7121/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7122/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7123/15000, steps: 600, e: 0.0034\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7124/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7125/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7126/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 7127/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7128/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7129/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 7130/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7131/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 7132/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7133/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7134/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7135/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7136/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7137/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7138/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7139/15000, steps: 257, e: 0.0033\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7140/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7141/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7142/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7143/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7144/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7145/15000, steps: 52, e: 0.0033\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 7146/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7147/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 7148/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7149/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7150/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7151/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7152/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7153/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7154/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7155/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 7156/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7157/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 7158/15000, steps: 600, e: 0.0033\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7159/15000, steps: 72, e: 0.0033\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7160/15000, steps: 52, e: 0.0033\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7161/15000, steps: 52, e: 0.0033\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7162/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7163/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 7164/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 7165/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7166/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7167/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7168/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7169/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7170/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7171/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7172/15000, steps: 312, e: 0.0032\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7173/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7174/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7175/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7176/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7177/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7178/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 7179/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7180/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7181/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7182/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7183/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7184/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 7185/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7186/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7187/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 7188/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7189/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7190/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7191/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7192/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7193/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7194/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7195/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7196/15000, steps: 223, e: 0.0032\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7197/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 7198/15000, steps: 52, e: 0.0032\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7199/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7200/15000, steps: 600, e: 0.0032\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7201/15000, steps: 52, e: 0.0031\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7202/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7203/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7204/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7205/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7206/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7207/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7208/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7209/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 7210/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7211/15000, steps: 251, e: 0.0031\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7212/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7213/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7214/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7215/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7216/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7217/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7218/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7219/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7220/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7221/15000, steps: 52, e: 0.0031\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 7222/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7223/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7224/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7225/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 7226/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 7227/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 7228/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7229/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7230/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 7231/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7232/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7233/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7234/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7235/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7236/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7237/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 7238/15000, steps: 257, e: 0.0031\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7239/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7240/15000, steps: 600, e: 0.0031\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7241/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7242/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7243/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7244/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7245/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7246/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7247/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7248/15000, steps: 212, e: 0.003\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7249/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7250/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7251/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 7252/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7253/15000, steps: 116, e: 0.003\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7254/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7255/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7256/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7257/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 7258/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7259/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7260/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7261/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7262/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7263/15000, steps: 52, e: 0.003\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7264/15000, steps: 52, e: 0.003\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 7265/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7266/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7267/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7268/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7269/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7270/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7271/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7272/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7273/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7274/15000, steps: 52, e: 0.003\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7275/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7276/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7277/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7278/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 7279/15000, steps: 303, e: 0.003\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7280/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7281/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7282/15000, steps: 600, e: 0.003\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7283/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7284/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7285/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7286/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7287/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7288/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7289/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7290/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7291/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7292/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7293/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7294/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7295/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7296/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7297/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7298/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7299/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 7300/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7301/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 7302/15000, steps: 59, e: 0.0029\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7303/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7304/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7305/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7306/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7307/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7308/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7309/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7310/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7311/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7312/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7313/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7314/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7315/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7316/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7317/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7318/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7319/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7320/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7321/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7322/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7323/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 7324/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7325/15000, steps: 600, e: 0.0029\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 7326/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7327/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7328/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7329/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7330/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7331/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7332/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7333/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7334/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7335/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7336/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7337/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7338/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7339/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7340/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7341/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7342/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7343/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7344/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7345/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7346/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7347/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7348/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7349/15000, steps: 52, e: 0.0028\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7350/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7351/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7352/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7353/15000, steps: 335, e: 0.0028\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7354/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7355/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 7356/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7357/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7358/15000, steps: 52, e: 0.0028\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7359/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7360/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7361/15000, steps: 52, e: 0.0028\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7362/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7363/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 7364/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7365/15000, steps: 52, e: 0.0028\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7366/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7367/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7368/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 7369/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7370/15000, steps: 600, e: 0.0028\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7371/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 7372/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7373/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7374/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7375/15000, steps: 52, e: 0.0027\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7376/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7377/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7378/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7379/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7380/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7381/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7382/15000, steps: 96, e: 0.0027\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7383/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7384/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7385/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7386/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 7387/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7388/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7389/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7390/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7391/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7392/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7393/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7394/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7395/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7396/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7397/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7398/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7399/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 7400/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7401/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7402/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 7403/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7404/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7405/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 7406/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7407/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 7408/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7409/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7410/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7411/15000, steps: 52, e: 0.0027\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7412/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7413/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7414/15000, steps: 131, e: 0.0027\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7415/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7416/15000, steps: 600, e: 0.0027\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7417/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7418/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7419/15000, steps: 52, e: 0.0026\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7420/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7421/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7422/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7423/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7424/15000, steps: 582, e: 0.0026\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7425/15000, steps: 52, e: 0.0026\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7426/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7427/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7428/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7429/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7430/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7431/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7432/15000, steps: 52, e: 0.0026\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7433/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7434/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7435/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 7436/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7437/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7438/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 7439/15000, steps: 52, e: 0.0026\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7440/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7441/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7442/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7443/15000, steps: 52, e: 0.0026\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7444/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7445/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7446/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7447/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7448/15000, steps: 99, e: 0.0026\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7449/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 7450/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7451/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7452/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7453/15000, steps: 241, e: 0.0026\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7454/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7455/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7456/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7457/15000, steps: 594, e: 0.0026\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7458/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7459/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7460/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7461/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 7462/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7463/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7464/15000, steps: 600, e: 0.0026\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7465/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7466/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 7467/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7468/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7469/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7470/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7471/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7472/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7473/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7474/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 7475/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 7476/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7477/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7478/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7479/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7480/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7481/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7482/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 7483/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7484/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7485/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7486/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7487/15000, steps: 175, e: 0.0025\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7488/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7489/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7490/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7491/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7492/15000, steps: 52, e: 0.0025\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7493/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7494/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7495/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7496/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7497/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7498/15000, steps: 230, e: 0.0025\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7499/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7500/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7501/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 7502/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7503/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 7504/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 7505/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7506/15000, steps: 52, e: 0.0025\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7507/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7508/15000, steps: 52, e: 0.0025\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7509/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7510/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7511/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7512/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7513/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7514/15000, steps: 600, e: 0.0025\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7515/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7516/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7517/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7518/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7519/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7520/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7521/15000, steps: 52, e: 0.0024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7522/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7523/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7524/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7525/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7526/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7527/15000, steps: 52, e: 0.0024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7528/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7529/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7530/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7531/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7532/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7533/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7534/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7535/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7536/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7537/15000, steps: 52, e: 0.0024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7538/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7539/15000, steps: 52, e: 0.0024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7540/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7541/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7542/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7543/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7544/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7545/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7546/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7547/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 7548/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7549/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7550/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7551/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 7552/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7553/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 7554/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7555/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7556/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7557/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7558/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7559/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7560/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 7561/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7562/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7563/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7564/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7565/15000, steps: 600, e: 0.0024\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7566/15000, steps: 52, e: 0.0024\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7567/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 7568/15000, steps: 52, e: 0.0023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7569/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7570/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7571/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7572/15000, steps: 52, e: 0.0023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7573/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7574/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7575/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7576/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7577/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7578/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7579/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7580/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7581/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 7582/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7583/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7584/15000, steps: 461, e: 0.0023\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7585/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7586/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7587/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7588/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7589/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7590/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7591/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7592/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7593/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7594/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7595/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7596/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7597/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7598/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7599/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 7600/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7601/15000, steps: 52, e: 0.0023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7602/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7603/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7604/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7605/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7606/15000, steps: 52, e: 0.0023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 7607/15000, steps: 52, e: 0.0023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7608/15000, steps: 52, e: 0.0023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7609/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7610/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7611/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7612/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7613/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7614/15000, steps: 52, e: 0.0023\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 7615/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7616/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7617/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7618/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7619/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7620/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7621/15000, steps: 600, e: 0.0023\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7622/15000, steps: 302, e: 0.0022\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7623/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7624/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7625/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7626/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7627/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7628/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7629/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7630/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7631/15000, steps: 52, e: 0.0022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7632/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7633/15000, steps: 52, e: 0.0022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7634/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7635/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7636/15000, steps: 52, e: 0.0022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7637/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7638/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7639/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7640/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 7641/15000, steps: 248, e: 0.0022\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7642/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7643/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7644/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7645/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7646/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7647/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7648/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7649/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7650/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7651/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7652/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7653/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7654/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7655/15000, steps: 52, e: 0.0022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7656/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7657/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7658/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7659/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 7660/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7661/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7662/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7663/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7664/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7665/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7666/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7667/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7668/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 7669/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7670/15000, steps: 52, e: 0.0022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7671/15000, steps: 52, e: 0.0022\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7672/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7673/15000, steps: 413, e: 0.0022\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7674/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7675/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 7676/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7677/15000, steps: 600, e: 0.0022\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7678/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7679/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7680/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7681/15000, steps: 366, e: 0.0021\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7682/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 7683/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7684/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7685/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7686/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 7687/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 7688/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7689/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7690/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 7691/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 7692/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7693/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7694/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7695/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 7696/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 7697/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7698/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7699/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7700/15000, steps: 198, e: 0.0021\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 7701/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7702/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 7703/15000, steps: 52, e: 0.0021\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7704/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7705/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 7706/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7707/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7708/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7709/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7710/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 7711/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7712/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 7713/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7714/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7715/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7716/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7717/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7718/15000, steps: 52, e: 0.0021\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7719/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 7720/15000, steps: 52, e: 0.0021\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7721/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7722/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7723/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7724/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7725/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 7726/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7727/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 7728/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7729/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7730/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7731/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7732/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7733/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7734/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7735/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7736/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 7737/15000, steps: 600, e: 0.0021\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7738/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7739/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7740/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7741/15000, steps: 112, e: 0.002\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 7742/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 7743/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7744/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7745/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7746/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7747/15000, steps: 52, e: 0.002\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7748/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7749/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7750/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7751/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7752/15000, steps: 52, e: 0.002\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7753/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7754/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 7755/15000, steps: 586, e: 0.002\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7756/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7757/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7758/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7759/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7760/15000, steps: 168, e: 0.002\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7761/15000, steps: 517, e: 0.002\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7762/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7763/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7764/15000, steps: 501, e: 0.002\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7765/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7766/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7767/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7768/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7769/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7770/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7771/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7772/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 7773/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7774/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7775/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7776/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7777/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7778/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7779/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7780/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7781/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7782/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7783/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7784/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7785/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7786/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 7787/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 7788/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7789/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7790/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7791/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7792/15000, steps: 52, e: 0.002\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 7793/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 7794/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 7795/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 7796/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 7797/15000, steps: 52, e: 0.002\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7798/15000, steps: 52, e: 0.002\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7799/15000, steps: 600, e: 0.002\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7800/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 7801/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7802/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7803/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 7804/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 7805/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 7806/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7807/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7808/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7809/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7810/15000, steps: 52, e: 0.0019\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7811/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7812/15000, steps: 108, e: 0.0019\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7813/15000, steps: 52, e: 0.0019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7814/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7815/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7816/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7817/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7818/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 7819/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7820/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7821/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7822/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7823/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7824/15000, steps: 52, e: 0.0019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7825/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7826/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7827/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7828/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7829/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7830/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7831/15000, steps: 307, e: 0.0019\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7832/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7833/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7834/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7835/15000, steps: 255, e: 0.0019\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 7836/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7837/15000, steps: 379, e: 0.0019\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 7838/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7839/15000, steps: 430, e: 0.0019\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 7840/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7841/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 7842/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7843/15000, steps: 52, e: 0.0019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7844/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7845/15000, steps: 52, e: 0.0019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7846/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7847/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7848/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 7849/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7850/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7851/15000, steps: 52, e: 0.0019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7852/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7853/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7854/15000, steps: 52, e: 0.0019\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 7855/15000, steps: 172, e: 0.0019\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7856/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7857/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7858/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7859/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7860/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7861/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7862/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7863/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7864/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7865/15000, steps: 600, e: 0.0019\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7866/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7867/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7868/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7869/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7870/15000, steps: 414, e: 0.0018\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7871/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7872/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 7873/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7874/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 7875/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7876/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 7877/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7878/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 7879/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 7880/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 7881/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 7882/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 7883/15000, steps: 225, e: 0.0018\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7884/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7885/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 7886/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7887/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7888/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7889/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7890/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 7891/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7892/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7893/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7894/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7895/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7896/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 7897/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7898/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7899/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7900/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7901/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7902/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7903/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 7904/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7905/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7906/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7907/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7908/15000, steps: 227, e: 0.0018\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7909/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7910/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7911/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 7912/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 7913/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7914/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7915/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7916/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7917/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7918/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 7919/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 7920/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7921/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 7922/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 7923/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7924/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7925/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 7926/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7927/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7928/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7929/15000, steps: 52, e: 0.0018\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7930/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 7931/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 7932/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 7933/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7934/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 7935/15000, steps: 600, e: 0.0018\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7936/15000, steps: 143, e: 0.0017\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 7937/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 7938/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 7939/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7940/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 7941/15000, steps: 352, e: 0.0017\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 7942/15000, steps: 358, e: 0.0017\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 7943/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 7944/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7945/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 7946/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 7947/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7948/15000, steps: 52, e: 0.0017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7949/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7950/15000, steps: 52, e: 0.0017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 7951/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 7952/15000, steps: 52, e: 0.0017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7953/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 7954/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 7955/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7956/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 7957/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7958/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 7959/15000, steps: 52, e: 0.0017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 7960/15000, steps: 197, e: 0.0017\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 7961/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 7962/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 7963/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 7964/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7965/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 7966/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 7967/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 7968/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7969/15000, steps: 52, e: 0.0017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 7970/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 7971/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 7972/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 7973/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7974/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 7975/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 7976/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 7977/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 7978/15000, steps: 52, e: 0.0017\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 7979/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 7980/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 7981/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7982/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 7983/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 7984/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 7985/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 7986/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 7987/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 7988/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 7989/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 7990/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 7991/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 7992/15000, steps: 296, e: 0.0017\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 7993/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7994/15000, steps: 321, e: 0.0017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7995/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 7996/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 7997/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 7998/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 7999/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 8000/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8001/15000, steps: 283, e: 0.0017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8002/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8003/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8004/15000, steps: 455, e: 0.0017\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8005/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8006/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8007/15000, steps: 210, e: 0.0017\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8008/15000, steps: 600, e: 0.0017\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8009/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8010/15000, steps: 52, e: 0.0016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8011/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8012/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8013/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8014/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8015/15000, steps: 94, e: 0.0016\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8016/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8017/15000, steps: 449, e: 0.0016\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8018/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8019/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8020/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8021/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8022/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8023/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8024/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 8025/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8026/15000, steps: 229, e: 0.0016\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8027/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8028/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8029/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8030/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 8031/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8032/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8033/15000, steps: 52, e: 0.0016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8034/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8035/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8036/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8037/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8038/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8039/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8040/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8041/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8042/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8043/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8044/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8045/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8046/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 8047/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8048/15000, steps: 52, e: 0.0016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8049/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8050/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 8051/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8052/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8053/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8054/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8055/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8056/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 8057/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8058/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8059/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8060/15000, steps: 532, e: 0.0016\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8061/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8062/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8063/15000, steps: 345, e: 0.0016\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8064/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8065/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8066/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8067/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8068/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8069/15000, steps: 52, e: 0.0016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8070/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8071/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8072/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8073/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8074/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8075/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8076/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8077/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8078/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8079/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8080/15000, steps: 52, e: 0.0016\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8081/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8082/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8083/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8084/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8085/15000, steps: 585, e: 0.0016\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8086/15000, steps: 600, e: 0.0016\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8087/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8088/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8089/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8090/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8091/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8092/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8093/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8094/15000, steps: 323, e: 0.0015\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 8095/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8096/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8097/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 8098/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8099/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8100/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8101/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8102/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 8103/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 8104/15000, steps: 63, e: 0.0015\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8105/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8106/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8107/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 8108/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8109/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8110/15000, steps: 52, e: 0.0015\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8111/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8112/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8113/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8114/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8115/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8116/15000, steps: 52, e: 0.0015\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8117/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8118/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 8119/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8120/15000, steps: 52, e: 0.0015\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8121/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8122/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8123/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8124/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8125/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8126/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8127/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8128/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8129/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8130/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8131/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8132/15000, steps: 536, e: 0.0015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8133/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 8134/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8135/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8136/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8137/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8138/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8139/15000, steps: 52, e: 0.0015\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8140/15000, steps: 413, e: 0.0015\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8141/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8142/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8143/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8144/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 8145/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8146/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8147/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8148/15000, steps: 214, e: 0.0015\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8149/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 8150/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8151/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 8152/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8153/15000, steps: 162, e: 0.0015\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8154/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8155/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 8156/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8157/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8158/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8159/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8160/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8161/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8162/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8163/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8164/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8165/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8166/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8167/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8168/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8169/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 8170/15000, steps: 600, e: 0.0015\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 8171/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8172/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8173/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8174/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8175/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8176/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8177/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8178/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 8179/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8180/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8181/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 8182/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8183/15000, steps: 52, e: 0.0014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8184/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8185/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8186/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8187/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8188/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8189/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8190/15000, steps: 52, e: 0.0014\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8191/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8192/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8193/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8194/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8195/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8196/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 8197/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8198/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8199/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8200/15000, steps: 52, e: 0.0014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8201/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 8202/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8203/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8204/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8205/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8206/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8207/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8208/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8209/15000, steps: 52, e: 0.0014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8210/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8211/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8212/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8213/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8214/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8215/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 8216/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8217/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8218/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8219/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8220/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8221/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8222/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8223/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8224/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8225/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8226/15000, steps: 52, e: 0.0014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8227/15000, steps: 596, e: 0.0014\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8228/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8229/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8230/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8231/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8232/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8233/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8234/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8235/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8236/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 8237/15000, steps: 52, e: 0.0014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8238/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8239/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8240/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 8241/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8242/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8243/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8244/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8245/15000, steps: 159, e: 0.0014\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8246/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8247/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8248/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8249/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8250/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8251/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8252/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8253/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8254/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8255/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8256/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8257/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8258/15000, steps: 600, e: 0.0014\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8259/15000, steps: 52, e: 0.0014\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8260/15000, steps: 555, e: 0.0013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8261/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8262/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8263/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8264/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8265/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8266/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8267/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8268/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8269/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8270/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8271/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8272/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8273/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8274/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8275/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8276/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8277/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 8278/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8279/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8280/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8281/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 8282/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 8283/15000, steps: 246, e: 0.0013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8284/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8285/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8286/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8287/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8288/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8289/15000, steps: 110, e: 0.0013\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8290/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 8291/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8292/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8293/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8294/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8295/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 8296/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8297/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8298/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 8299/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8300/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8301/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8302/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 8303/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8304/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8305/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8306/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8307/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8308/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8309/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8310/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8311/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8312/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8313/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8314/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8315/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8316/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8317/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8318/15000, steps: 128, e: 0.0013\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8319/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8320/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 8321/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8322/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8323/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8324/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 8325/15000, steps: 183, e: 0.0013\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8326/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8327/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 8328/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8329/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8330/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8331/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 8332/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8333/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8334/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8335/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8336/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8337/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8338/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8339/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 8340/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8341/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8342/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8343/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8344/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8345/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 8346/15000, steps: 52, e: 0.0013\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8347/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8348/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 8349/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8350/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8351/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8352/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8353/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8354/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 8355/15000, steps: 600, e: 0.0013\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8356/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8357/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8358/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8359/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8360/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8361/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8362/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8363/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8364/15000, steps: 569, e: 0.0012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8365/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8366/15000, steps: 52, e: 0.0012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8367/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8368/15000, steps: 116, e: 0.0012\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8369/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8370/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8371/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8372/15000, steps: 52, e: 0.0012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8373/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8374/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8375/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8376/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8377/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8378/15000, steps: 411, e: 0.0012\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8379/15000, steps: 52, e: 0.0012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8380/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8381/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8382/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 8383/15000, steps: 589, e: 0.0012\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8384/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 8385/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8386/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8387/15000, steps: 101, e: 0.0012\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8388/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8389/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 8390/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8391/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8392/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 8393/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8394/15000, steps: 417, e: 0.0012\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8395/15000, steps: 52, e: 0.0012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8396/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 8397/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8398/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 8399/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8400/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8401/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8402/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8403/15000, steps: 204, e: 0.0012\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8404/15000, steps: 138, e: 0.0012\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8405/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 8406/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8407/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8408/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8409/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8410/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8411/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8412/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8413/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8414/15000, steps: 591, e: 0.0012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8415/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8416/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8417/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8418/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8419/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8420/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8421/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8422/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8423/15000, steps: 136, e: 0.0012\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8424/15000, steps: 52, e: 0.0012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8425/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8426/15000, steps: 194, e: 0.0012\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8427/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8428/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8429/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8430/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8431/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 8432/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8433/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8434/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8435/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8436/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8437/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8438/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8439/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 8440/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8441/15000, steps: 52, e: 0.0012\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8442/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8443/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8444/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8445/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8446/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8447/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8448/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8449/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8450/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8451/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 8452/15000, steps: 592, e: 0.0012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8453/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8454/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8455/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8456/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8457/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8458/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8459/15000, steps: 600, e: 0.0012\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8460/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8461/15000, steps: 310, e: 0.0011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8462/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 8463/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 8464/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 8465/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8466/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8467/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8468/15000, steps: 275, e: 0.0011\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8469/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8470/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8471/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8472/15000, steps: 155, e: 0.0011\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 8473/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8474/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8475/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8476/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8477/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8478/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8479/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8480/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8481/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8482/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8483/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8484/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8485/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8486/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8487/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8488/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8489/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8490/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8491/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8492/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 8493/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8494/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 8495/15000, steps: 544, e: 0.0011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8496/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8497/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8498/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8499/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8500/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8501/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 8502/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8503/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8504/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 8505/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 8506/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8507/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8508/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8509/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 8510/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8511/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8512/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8513/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8514/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8515/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8516/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8517/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8518/15000, steps: 431, e: 0.0011\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8519/15000, steps: 509, e: 0.0011\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8520/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8521/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8522/15000, steps: 237, e: 0.0011\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8523/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8524/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8525/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8526/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8527/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8528/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 8529/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 8530/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8531/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8532/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8533/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8534/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8535/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8536/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8537/15000, steps: 136, e: 0.0011\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8538/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8539/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8540/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8541/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8542/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8543/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8544/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 8545/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8546/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8547/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8548/15000, steps: 103, e: 0.0011\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8549/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 8550/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8551/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8552/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8553/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8554/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8555/15000, steps: 501, e: 0.0011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8556/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8557/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8558/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8559/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8560/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 8561/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8562/15000, steps: 504, e: 0.0011\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8563/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8564/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8565/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8566/15000, steps: 449, e: 0.0011\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8567/15000, steps: 554, e: 0.0011\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8568/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8569/15000, steps: 317, e: 0.0011\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8570/15000, steps: 52, e: 0.0011\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 8571/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8572/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8573/15000, steps: 600, e: 0.0011\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8574/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8575/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 8576/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8577/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8578/15000, steps: 396, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8579/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8580/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8581/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8582/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8583/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8584/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8585/15000, steps: 139, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8586/15000, steps: 465, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8587/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8588/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8589/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8590/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8591/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8592/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 8593/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8594/15000, steps: 269, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8595/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8596/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8597/15000, steps: 90, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8598/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8599/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8600/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8601/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 8602/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8603/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8604/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8605/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8606/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8607/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8608/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8609/15000, steps: 119, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8610/15000, steps: 585, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8611/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8612/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8613/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8614/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8615/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8616/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8617/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8618/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8619/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8620/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8621/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8622/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8623/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8624/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8625/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8626/15000, steps: 369, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8627/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8628/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8629/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8630/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8631/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8632/15000, steps: 359, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8633/15000, steps: 149, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8634/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8635/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 8636/15000, steps: 385, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8637/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8638/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8639/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 8640/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8641/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 8642/15000, steps: 519, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8643/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8644/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8645/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8646/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8647/15000, steps: 235, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8648/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8649/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8650/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8651/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8652/15000, steps: 110, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8653/15000, steps: 107, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8654/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8655/15000, steps: 442, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8656/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8657/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8658/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8659/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8660/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8661/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 8662/15000, steps: 384, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8663/15000, steps: 382, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8664/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8665/15000, steps: 197, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8666/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8667/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8668/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 8669/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 8670/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8671/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8672/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8673/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8674/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8675/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8676/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8677/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8678/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8679/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8680/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8681/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8682/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8683/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8684/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8685/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8686/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8687/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 8688/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8689/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 8690/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8691/15000, steps: 431, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8692/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8693/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8694/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8695/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8696/15000, steps: 306, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8697/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8698/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8699/15000, steps: 397, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8700/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8701/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8702/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8703/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8704/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8705/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8706/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 8707/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8708/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8709/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8710/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8711/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8712/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8713/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8714/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8715/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8716/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 8717/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8718/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8719/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8720/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8721/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8722/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8723/15000, steps: 117, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8724/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8725/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8726/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8727/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8728/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8729/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8730/15000, steps: 542, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8731/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8732/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 8733/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 8734/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 8735/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8736/15000, steps: 118, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8737/15000, steps: 73, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8738/15000, steps: 348, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8739/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8740/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8741/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8742/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8743/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8744/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8745/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8746/15000, steps: 221, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 8747/15000, steps: 302, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 8748/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 8749/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8750/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8751/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8752/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 8753/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8754/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8755/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8756/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 8757/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8758/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8759/15000, steps: 291, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8760/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8761/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8762/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8763/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8764/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8765/15000, steps: 562, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8766/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8767/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8768/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8769/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8770/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8771/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8772/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8773/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8774/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8775/15000, steps: 540, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8776/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8777/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8778/15000, steps: 165, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8779/15000, steps: 267, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8780/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8781/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8782/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8783/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8784/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8785/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8786/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8787/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 8788/15000, steps: 237, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8789/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8790/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8791/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8792/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 8793/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8794/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8795/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8796/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8797/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8798/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 8799/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8800/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8801/15000, steps: 495, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8802/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8803/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8804/15000, steps: 255, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 8805/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8806/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8807/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8808/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 8809/15000, steps: 512, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8810/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8811/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8812/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8813/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8814/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8815/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8816/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8817/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8818/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8819/15000, steps: 157, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 8820/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8821/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8822/15000, steps: 319, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8823/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8824/15000, steps: 184, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8825/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8826/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8827/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8828/15000, steps: 305, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8829/15000, steps: 154, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8830/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8831/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8832/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8833/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8834/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8835/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8836/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8837/15000, steps: 561, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8838/15000, steps: 228, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 8839/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8840/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8841/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 8842/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 8843/15000, steps: 230, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 8844/15000, steps: 303, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8845/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8846/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8847/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8848/15000, steps: 134, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8849/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8850/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8851/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8852/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8853/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 8854/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 8855/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8856/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 8857/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 8858/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8859/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 8860/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8861/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 8862/15000, steps: 500, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 8863/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8864/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 8865/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8866/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8867/15000, steps: 236, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 8868/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8869/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 8870/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8871/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8872/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8873/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8874/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8875/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8876/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8877/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 8878/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 8879/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 8880/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8881/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8882/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8883/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8884/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 8885/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8886/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8887/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8888/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8889/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8890/15000, steps: 487, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8891/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 8892/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8893/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8894/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 8895/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8896/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 8897/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8898/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8899/15000, steps: 282, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8900/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8901/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8902/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 8903/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 8904/15000, steps: 426, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 8905/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8906/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8907/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 8908/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8909/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 8910/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8911/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8912/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 8913/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8914/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 8915/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 8916/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 8917/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8918/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8919/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 8920/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 8921/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 8922/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8923/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8924/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 8925/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8926/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 8927/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8928/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8929/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8930/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 8931/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 8932/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 8933/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8934/15000, steps: 481, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 8935/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8936/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 8937/15000, steps: 382, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8938/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8939/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 8940/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8941/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8942/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 8943/15000, steps: 86, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 8944/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 8945/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 8946/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8947/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 8948/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8949/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8950/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 8951/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8952/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 8953/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 8954/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8955/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8956/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8957/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 8958/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 8959/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 8960/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 8961/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8962/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 8963/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 8964/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 8965/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 8966/15000, steps: 453, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8967/15000, steps: 60, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8968/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 8969/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 8970/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 8971/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8972/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 8973/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8974/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 8975/15000, steps: 214, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 8976/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8977/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 8978/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 8979/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 8980/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 8981/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 8982/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8983/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 8984/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8985/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 8986/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8987/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 8988/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 8989/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 8990/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 8991/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 8992/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 8993/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8994/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 8995/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 8996/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8997/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 8998/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 8999/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9000/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9001/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9002/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9003/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9004/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9005/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9006/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9007/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 9008/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9009/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9010/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9011/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9012/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9013/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9014/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9015/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9016/15000, steps: 236, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 9017/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9018/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9019/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9020/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9021/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9022/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9023/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9024/15000, steps: 61, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9025/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9026/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9027/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9028/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9029/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9030/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9031/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9032/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9033/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9034/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9035/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9036/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9037/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9038/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9039/15000, steps: 441, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9040/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9041/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9042/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9043/15000, steps: 592, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9044/15000, steps: 98, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9045/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9046/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9047/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9048/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9049/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9050/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9051/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9052/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9053/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9054/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9055/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9056/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9057/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9058/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9059/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9060/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9061/15000, steps: 313, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9062/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9063/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9064/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9065/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9066/15000, steps: 311, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 9067/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 9068/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9069/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 9070/15000, steps: 235, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9071/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9072/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9073/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9074/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9075/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9076/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9077/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9078/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9079/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 9080/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9081/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 9082/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9083/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9084/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9085/15000, steps: 290, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9086/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9087/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9088/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9089/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9090/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 9091/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9092/15000, steps: 126, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 9093/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9094/15000, steps: 497, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9095/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9096/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9097/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9098/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9099/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9100/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9101/15000, steps: 265, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9102/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9103/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9104/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9105/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9106/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9107/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9108/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9109/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9110/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 9111/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9112/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9113/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9114/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9115/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9116/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9117/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 9118/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9119/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9120/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9121/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9122/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9123/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 9124/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9125/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9126/15000, steps: 249, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9127/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9128/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9129/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 9130/15000, steps: 349, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9131/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9132/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 9133/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9134/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9135/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9136/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9137/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9138/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9139/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9140/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 9141/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9142/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9143/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9144/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9145/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9146/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9147/15000, steps: 542, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9148/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9149/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9150/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9151/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9152/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9153/15000, steps: 55, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9154/15000, steps: 109, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9155/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9156/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9157/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9158/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9159/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9160/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 9161/15000, steps: 138, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9162/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9163/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9164/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9165/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9166/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9167/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9168/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9169/15000, steps: 415, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9170/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9171/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9172/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9173/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9174/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9175/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9176/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9177/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9178/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9179/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9180/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9181/15000, steps: 356, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9182/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9183/15000, steps: 163, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9184/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9185/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9186/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9187/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9188/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 9189/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9190/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9191/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9192/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9193/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9194/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9195/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9196/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9197/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9198/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9199/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 9200/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9201/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9202/15000, steps: 569, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9203/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9204/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9205/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9206/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 9207/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9208/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9209/15000, steps: 99, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9210/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9211/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9212/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9213/15000, steps: 444, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9214/15000, steps: 136, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9215/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 9216/15000, steps: 527, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9217/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9218/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9219/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9220/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9221/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9222/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9223/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9224/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9225/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9226/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9227/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9228/15000, steps: 359, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9229/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9230/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9231/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9232/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9233/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9234/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9235/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9236/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9237/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9238/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9239/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9240/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9241/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9242/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9243/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9244/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 9245/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9246/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9247/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9248/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9249/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9250/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9251/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9252/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9253/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9254/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9255/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9256/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9257/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9258/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9259/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9260/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9261/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9262/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9263/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9264/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9265/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9266/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9267/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9268/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9269/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9270/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9271/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9272/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 9273/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9274/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9275/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9276/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 9277/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9278/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 9279/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9280/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9281/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9282/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9283/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 9284/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9285/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 9286/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9287/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9288/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9289/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9290/15000, steps: 60, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9291/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9292/15000, steps: 421, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9293/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 9294/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9295/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9296/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9297/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9298/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9299/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9300/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9301/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9302/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9303/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9304/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9305/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9306/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9307/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9308/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9309/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 9310/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9311/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 9312/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9313/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9314/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9315/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 9316/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 9317/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9318/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9319/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9320/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9321/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9322/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9323/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9324/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9325/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9326/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9327/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9328/15000, steps: 288, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9329/15000, steps: 510, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9330/15000, steps: 313, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9331/15000, steps: 372, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9332/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9333/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9334/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9335/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9336/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9337/15000, steps: 479, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9338/15000, steps: 509, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9339/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9340/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 9341/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 9342/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9343/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9344/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 9345/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9346/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9347/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9348/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9349/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 9350/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9351/15000, steps: 440, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9352/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9353/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9354/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9355/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9356/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9357/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9358/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9359/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9360/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9361/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9362/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9363/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9364/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 9365/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9366/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9367/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9368/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9369/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9370/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9371/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9372/15000, steps: 118, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9373/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9374/15000, steps: 155, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9375/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9376/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9377/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9378/15000, steps: 167, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9379/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9380/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9381/15000, steps: 414, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9382/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9383/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9384/15000, steps: 303, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9385/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9386/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9387/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9388/15000, steps: 300, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9389/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9390/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9391/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9392/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 9393/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9394/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9395/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9396/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9397/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9398/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9399/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9400/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9401/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9402/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 9403/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9404/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9405/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9406/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9407/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9408/15000, steps: 59, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9409/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9410/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9411/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 9412/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9413/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9414/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9415/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 9416/15000, steps: 103, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 9417/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9418/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9419/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9420/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9421/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 9422/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9423/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9424/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9425/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9426/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9427/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9428/15000, steps: 110, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9429/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9430/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9431/15000, steps: 233, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 9432/15000, steps: 253, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9433/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9434/15000, steps: 587, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 9435/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9436/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9437/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9438/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9439/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9440/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9441/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9442/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9443/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9444/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9445/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9446/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9447/15000, steps: 384, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9448/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9449/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 9450/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9451/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9452/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9453/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 9454/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9455/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 9456/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9457/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9458/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9459/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9460/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 9461/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9462/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9463/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9464/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9465/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9466/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9467/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 9468/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9469/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9470/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9471/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9472/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9473/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9474/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9475/15000, steps: 472, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9476/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9477/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9478/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9479/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9480/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9481/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9482/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9483/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9484/15000, steps: 548, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9485/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9486/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9487/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 9488/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9489/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9490/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9491/15000, steps: 423, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9492/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 9493/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9494/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9495/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9496/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9497/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9498/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9499/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9500/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9501/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9502/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9503/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9504/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9505/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9506/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9507/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9508/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9509/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9510/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9511/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 9512/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 9513/15000, steps: 172, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9514/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9515/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9516/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 9517/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9518/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9519/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9520/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9521/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9522/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9523/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9524/15000, steps: 135, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9525/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9526/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9527/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9528/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 9529/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 9530/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9531/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9532/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9533/15000, steps: 597, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9534/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9535/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9536/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9537/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9538/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9539/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 9540/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9541/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9542/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9543/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9544/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9545/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9546/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 9547/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9548/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9549/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9550/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9551/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9552/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 9553/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 9554/15000, steps: 239, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 9555/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9556/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9557/15000, steps: 315, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9558/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9559/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9560/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9561/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9562/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9563/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9564/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9565/15000, steps: 180, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9566/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 9567/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9568/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 9569/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9570/15000, steps: 162, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9571/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9572/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9573/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9574/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9575/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9576/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9577/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9578/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9579/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9580/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9581/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9582/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9583/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 9584/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9585/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9586/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9587/15000, steps: 316, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9588/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 9589/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9590/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9591/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 9592/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9593/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9594/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9595/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9596/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9597/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9598/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9599/15000, steps: 354, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 9600/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9601/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9602/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 9603/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 9604/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9605/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9606/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9607/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9608/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9609/15000, steps: 452, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9610/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9611/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9612/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9613/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9614/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9615/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9616/15000, steps: 316, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9617/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9618/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9619/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9620/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9621/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9622/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9623/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9624/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9625/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 9626/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 9627/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9628/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9629/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9630/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9631/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9632/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9633/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9634/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9635/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 9636/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9637/15000, steps: 237, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 9638/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9639/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9640/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9641/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9642/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9643/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9644/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9645/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9646/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9647/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9648/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9649/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9650/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9651/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9652/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9653/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 9654/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 9655/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 9656/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9657/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9658/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9659/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9660/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9661/15000, steps: 191, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9662/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9663/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9664/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9665/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 9666/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9667/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9668/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9669/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9670/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 9671/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9672/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 9673/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9674/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9675/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9676/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9677/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9678/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9679/15000, steps: 555, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9680/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9681/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9682/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 9683/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9684/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9685/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 9686/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9687/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9688/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 9689/15000, steps: 336, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9690/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9691/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9692/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9693/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9694/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9695/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9696/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 9697/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9698/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 9699/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9700/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9701/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9702/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9703/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9704/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9705/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9706/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9707/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9708/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 9709/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9710/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9711/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9712/15000, steps: 330, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9713/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9714/15000, steps: 482, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9715/15000, steps: 176, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 9716/15000, steps: 61, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9717/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9718/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9719/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9720/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9721/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9722/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9723/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9724/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9725/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9726/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 9727/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9728/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9729/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9730/15000, steps: 571, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9731/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 9732/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9733/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9734/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9735/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 9736/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9737/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 9738/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9739/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9740/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9741/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9742/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9743/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9744/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9745/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9746/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9747/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9748/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9749/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 9750/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 9751/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9752/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9753/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 9754/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9755/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9756/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9757/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9758/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9759/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9760/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9761/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9762/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9763/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9764/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9765/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9766/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9767/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9768/15000, steps: 61, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 9769/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9770/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 9771/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9772/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 9773/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9774/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9775/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9776/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 9777/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9778/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9779/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9780/15000, steps: 200, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9781/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9782/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9783/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9784/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9785/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9786/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9787/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9788/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9789/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9790/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9791/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9792/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9793/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9794/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 9795/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9796/15000, steps: 538, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9797/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 9798/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9799/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9800/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9801/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9802/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9803/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9804/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9805/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9806/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9807/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9808/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9809/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 9810/15000, steps: 289, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9811/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9812/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9813/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9814/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 9815/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9816/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9817/15000, steps: 568, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9818/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9819/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 9820/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9821/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9822/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9823/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 9824/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 9825/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9826/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9827/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 9828/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9829/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9830/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9831/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9832/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9833/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9834/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 9835/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9836/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9837/15000, steps: 591, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9838/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9839/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9840/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9841/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9842/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9843/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 9844/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 9845/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9846/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9847/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9848/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9849/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9850/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 9851/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9852/15000, steps: 217, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 9853/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9854/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9855/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9856/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9857/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9858/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 9859/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9860/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9861/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9862/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9863/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9864/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 9865/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9866/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9867/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9868/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9869/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 9870/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9871/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9872/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9873/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9874/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9875/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 9876/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 9877/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9878/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 9879/15000, steps: 545, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9880/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9881/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9882/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9883/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9884/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9885/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 9886/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9887/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 9888/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9889/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9890/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 9891/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9892/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 9893/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 9894/15000, steps: 489, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9895/15000, steps: 252, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9896/15000, steps: 246, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 9897/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9898/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 9899/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 9900/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 9901/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9902/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 9903/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9904/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9905/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9906/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9907/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9908/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9909/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9910/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9911/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9912/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9913/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 9914/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9915/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9916/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 9917/15000, steps: 246, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 9918/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 9919/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 9920/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 9921/15000, steps: 152, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9922/15000, steps: 99, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9923/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 9924/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 9925/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9926/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 9927/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9928/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 9929/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9930/15000, steps: 520, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9931/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9932/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9933/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9934/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 9935/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9936/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 9937/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 9938/15000, steps: 402, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 9939/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9940/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 9941/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9942/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 9943/15000, steps: 301, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9944/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9945/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 9946/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 9947/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 9948/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 9949/15000, steps: 163, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 9950/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9951/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 9952/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 9953/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 9954/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 9955/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 9956/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 9957/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 9958/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 9959/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 9960/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 9961/15000, steps: 376, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 9962/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 9963/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 9964/15000, steps: 419, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9965/15000, steps: 269, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 9966/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9967/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 9968/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 9969/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9970/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9971/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 9972/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 9973/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9974/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 9975/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 9976/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 9977/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 9978/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 9979/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 9980/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 9981/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9982/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9983/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9984/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9985/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 9986/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9987/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 9988/15000, steps: 561, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 9989/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 9990/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 9991/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 9992/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 9993/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 9994/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9995/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 9996/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 9997/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 9998/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 9999/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10000/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10001/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10002/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10003/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10004/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10005/15000, steps: 494, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10006/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10007/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10008/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10009/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10010/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10011/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10012/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10013/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10014/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10015/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10016/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10017/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10018/15000, steps: 384, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10019/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10020/15000, steps: 73, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10021/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10022/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10023/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10024/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10025/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 10026/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10027/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10028/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10029/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10030/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10031/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10032/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10033/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10034/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10035/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10036/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10037/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 10038/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10039/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10040/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10041/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10042/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 10043/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 10044/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10045/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10046/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10047/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10048/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 10049/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10050/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10051/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10052/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 10053/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 10054/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10055/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10056/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 10057/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10058/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10059/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10060/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10061/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10062/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10063/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10064/15000, steps: 524, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10065/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10066/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10067/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10068/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10069/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10070/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10071/15000, steps: 248, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10072/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10073/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10074/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10075/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10076/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10077/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10078/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10079/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10080/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10081/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10082/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10083/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10084/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10085/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10086/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10087/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10088/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 10089/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10090/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10091/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10092/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10093/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10094/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10095/15000, steps: 263, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10096/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10097/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10098/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10099/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10100/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10101/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10102/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10103/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10104/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10105/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10106/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10107/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10108/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10109/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10110/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10111/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10112/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10113/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10114/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10115/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10116/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10117/15000, steps: 405, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 10118/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10119/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10120/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10121/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10122/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10123/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10124/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10125/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10126/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10127/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 10128/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10129/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10130/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10131/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10132/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10133/15000, steps: 134, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10134/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10135/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10136/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10137/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10138/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10139/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10140/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10141/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10142/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10143/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10144/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 10145/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10146/15000, steps: 169, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10147/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10148/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10149/15000, steps: 585, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10150/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10151/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 10152/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10153/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10154/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10155/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10156/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10157/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10158/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10159/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10160/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10161/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10162/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10163/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10164/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10165/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10166/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 10167/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10168/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10169/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10170/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10171/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10172/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10173/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10174/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10175/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10176/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 10177/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10178/15000, steps: 489, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10179/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10180/15000, steps: 478, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10181/15000, steps: 301, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10182/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10183/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10184/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10185/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10186/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10187/15000, steps: 188, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10188/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 10189/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 10190/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10191/15000, steps: 278, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 10192/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10193/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10194/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10195/15000, steps: 419, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10196/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10197/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 10198/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10199/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10200/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10201/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10202/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10203/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10204/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10205/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10206/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10207/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10208/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10209/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10210/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10211/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10212/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 10213/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10214/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10215/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10216/15000, steps: 81, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10217/15000, steps: 120, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10218/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10219/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10220/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10221/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10222/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10223/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10224/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10225/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10226/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 10227/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10228/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10229/15000, steps: 288, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10230/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10231/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10232/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10233/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10234/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10235/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10236/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10237/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10238/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10239/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10240/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10241/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 10242/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10243/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10244/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10245/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10246/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10247/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10248/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10249/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10250/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10251/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 10252/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 10253/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10254/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10255/15000, steps: 267, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10256/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10257/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10258/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10259/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10260/15000, steps: 461, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10261/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10262/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10263/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10264/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10265/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10266/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10267/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10268/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10269/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10270/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10271/15000, steps: 236, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10272/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10273/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10274/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10275/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10276/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10277/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10278/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10279/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10280/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10281/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10282/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10283/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10284/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10285/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10286/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10287/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10288/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10289/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10290/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10291/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 10292/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10293/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10294/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10295/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10296/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10297/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10298/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10299/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10300/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10301/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10302/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10303/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10304/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10305/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10306/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10307/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10308/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10309/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10310/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10311/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10312/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 10313/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10314/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10315/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10316/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 10317/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10318/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10319/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10320/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10321/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10322/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 10323/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10324/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10325/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10326/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10327/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10328/15000, steps: 110, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10329/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10330/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10331/15000, steps: 463, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10332/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10333/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10334/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10335/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10336/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10337/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10338/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10339/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10340/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10341/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 10342/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10343/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10344/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10345/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10346/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10347/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10348/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10349/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10350/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10351/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10352/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10353/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10354/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 10355/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 10356/15000, steps: 434, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10357/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10358/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 10359/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10360/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10361/15000, steps: 225, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10362/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10363/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10364/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10365/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10366/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10367/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10368/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10369/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10370/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10371/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10372/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10373/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10374/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 10375/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10376/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 10377/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10378/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10379/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10380/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 10381/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10382/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10383/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 10384/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10385/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10386/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10387/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10388/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10389/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 10390/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10391/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10392/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10393/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10394/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10395/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 10396/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10397/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10398/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10399/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10400/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10401/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10402/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10403/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 10404/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10405/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10406/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10407/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10408/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10409/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10410/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10411/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10412/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10413/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10414/15000, steps: 228, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10415/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10416/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10417/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10418/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10419/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10420/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10421/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10422/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 10423/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10424/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 10425/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10426/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10427/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 10428/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10429/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10430/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 10431/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10432/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 10433/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10434/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10435/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10436/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10437/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10438/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10439/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10440/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10441/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10442/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10443/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10444/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10445/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10446/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10447/15000, steps: 312, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10448/15000, steps: 507, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10449/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10450/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10451/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10452/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10453/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10454/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10455/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10456/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10457/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10458/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10459/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10460/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10461/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10462/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10463/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10464/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10465/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 10466/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10467/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10468/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10469/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10470/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 10471/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10472/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10473/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10474/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10475/15000, steps: 251, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10476/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 10477/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10478/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10479/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10480/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10481/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10482/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 10483/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10484/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 10485/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10486/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10487/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10488/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10489/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 10490/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10491/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10492/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 10493/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10494/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10495/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10496/15000, steps: 275, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10497/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10498/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 10499/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10500/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10501/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10502/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10503/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10504/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10505/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 10506/15000, steps: 457, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10507/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10508/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10509/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10510/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 10511/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10512/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10513/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10514/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10515/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10516/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10517/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10518/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10519/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10520/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 10521/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10522/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10523/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10524/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10525/15000, steps: 542, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 10526/15000, steps: 52, e: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10527/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10528/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10529/15000, steps: 258, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10530/15000, steps: 273, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10531/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10532/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 10533/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10534/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10535/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10536/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10537/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10538/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10539/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10540/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10541/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10542/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10543/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10544/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10545/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10546/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10547/15000, steps: 534, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10548/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10549/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10550/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10551/15000, steps: 481, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10552/15000, steps: 78, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10553/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10554/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 10555/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10556/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10557/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10558/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10559/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 10560/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10561/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10562/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10563/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10564/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10565/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10566/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10567/15000, steps: 587, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10568/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10569/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10570/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10571/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 10572/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10573/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10574/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10575/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10576/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10577/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 10578/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10579/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 10580/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10581/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10582/15000, steps: 260, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10583/15000, steps: 120, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10584/15000, steps: 449, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10585/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10586/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10587/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10588/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10589/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10590/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10591/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10592/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10593/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 10594/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 10595/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 10596/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10597/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10598/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10599/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10600/15000, steps: 261, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10601/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10602/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10603/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 10604/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10605/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10606/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10607/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10608/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10609/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 10610/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10611/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10612/15000, steps: 242, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10613/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10614/15000, steps: 369, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10615/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10616/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10617/15000, steps: 441, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 10618/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10619/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 10620/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10621/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10622/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10623/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10624/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10625/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10626/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10627/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10628/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10629/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 10630/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10631/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 10632/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 10633/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10634/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10635/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10636/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10637/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10638/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10639/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10640/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10641/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10642/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10643/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10644/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 10645/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10646/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10647/15000, steps: 156, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 10648/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10649/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10650/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10651/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10652/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10653/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10654/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10655/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10656/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 10657/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10658/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10659/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10660/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10661/15000, steps: 149, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10662/15000, steps: 75, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10663/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10664/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10665/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10666/15000, steps: 183, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10667/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10668/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10669/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10670/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10671/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10672/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10673/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 10674/15000, steps: 558, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10675/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10676/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10677/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10678/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 10679/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10680/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10681/15000, steps: 92, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10682/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10683/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10684/15000, steps: 80, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10685/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10686/15000, steps: 99, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10687/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10688/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10689/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10690/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10691/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 10692/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10693/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10694/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10695/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10696/15000, steps: 446, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10697/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10698/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10699/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10700/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10701/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10702/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10703/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10704/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10705/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 10706/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10707/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10708/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10709/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 10710/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10711/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10712/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10713/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10714/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10715/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10716/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10717/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10718/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 10719/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 10720/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10721/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10722/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10723/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 10724/15000, steps: 237, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10725/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10726/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10727/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10728/15000, steps: 174, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10729/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10730/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10731/15000, steps: 92, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10732/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10733/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10734/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10735/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10736/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10737/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10738/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10739/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10740/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10741/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10742/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10743/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10744/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 10745/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 10746/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10747/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10748/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10749/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10750/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10751/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10752/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 10753/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10754/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10755/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10756/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10757/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10758/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10759/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10760/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10761/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10762/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10763/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10764/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10765/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 10766/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10767/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 10768/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10769/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10770/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10771/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 10772/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10773/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 10774/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10775/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10776/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10777/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10778/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10779/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10780/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10781/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10782/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10783/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10784/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10785/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10786/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10787/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10788/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10789/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 10790/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10791/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10792/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10793/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10794/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10795/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 10796/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 10797/15000, steps: 470, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10798/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 10799/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10800/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10801/15000, steps: 137, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 10802/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 10803/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 10804/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10805/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 10806/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10807/15000, steps: 559, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10808/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10809/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10810/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10811/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10812/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10813/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10814/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10815/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10816/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10817/15000, steps: 577, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10818/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10819/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10820/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 10821/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10822/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10823/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10824/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10825/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10826/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10827/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10828/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10829/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 10830/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10831/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10832/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10833/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10834/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10835/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10836/15000, steps: 279, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10837/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10838/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 10839/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 10840/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10841/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10842/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 10843/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10844/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10845/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10846/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10847/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10848/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 10849/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 10850/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10851/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10852/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10853/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 10854/15000, steps: 410, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10855/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10856/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10857/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10858/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10859/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 10860/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10861/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10862/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10863/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 10864/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 10865/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10866/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 10867/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10868/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 10869/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 10870/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10871/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 10872/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10873/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10874/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10875/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10876/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 10877/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 10878/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10879/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10880/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10881/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10882/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 10883/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 10884/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 10885/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10886/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10887/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10888/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10889/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 10890/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10891/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 10892/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 10893/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10894/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 10895/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10896/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 10897/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10898/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10899/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10900/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 10901/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10902/15000, steps: 352, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10903/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 10904/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10905/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 10906/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10907/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 10908/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10909/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10910/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10911/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10912/15000, steps: 533, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 10913/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10914/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10915/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10916/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10917/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 10918/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10919/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 10920/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 10921/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 10922/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10923/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10924/15000, steps: 81, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 10925/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10926/15000, steps: 256, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 10927/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 10928/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10929/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 10930/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10931/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 10932/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 10933/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10934/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10935/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 10936/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 10937/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 10938/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 10939/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 10940/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10941/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10942/15000, steps: 101, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10943/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10944/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10945/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 10946/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 10947/15000, steps: 578, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10948/15000, steps: 592, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 10949/15000, steps: 322, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 10950/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 10951/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 10952/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10953/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10954/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 10955/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 10956/15000, steps: 135, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 10957/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 10958/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 10959/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 10960/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10961/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 10962/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10963/15000, steps: 194, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10964/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10965/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 10966/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10967/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 10968/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 10969/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 10970/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10971/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 10972/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 10973/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 10974/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 10975/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 10976/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 10977/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 10978/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 10979/15000, steps: 518, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 10980/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10981/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 10982/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 10983/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 10984/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 10985/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 10986/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10987/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 10988/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 10989/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 10990/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 10991/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 10992/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 10993/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 10994/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 10995/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 10996/15000, steps: 200, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 10997/15000, steps: 89, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 10998/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 10999/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11000/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11001/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11002/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11003/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 11004/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11005/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11006/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 11007/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11008/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11009/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11010/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 11011/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11012/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11013/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11014/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11015/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11016/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11017/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11018/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11019/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11020/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11021/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11022/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11023/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11024/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11025/15000, steps: 345, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11026/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 11027/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11028/15000, steps: 167, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 11029/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11030/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11031/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11032/15000, steps: 577, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11033/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11034/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11035/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11036/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11037/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11038/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11039/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11040/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11041/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11042/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11043/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11044/15000, steps: 316, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11045/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11046/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11047/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11048/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11049/15000, steps: 171, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 11050/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11051/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11052/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11053/15000, steps: 237, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11054/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 11055/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11056/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 11057/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11058/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11059/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11060/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11061/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11062/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11063/15000, steps: 554, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11064/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11065/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11066/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11067/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11068/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11069/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11070/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11071/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11072/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11073/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11074/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11075/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11076/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 11077/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11078/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11079/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11080/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11081/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11082/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11083/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11084/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11085/15000, steps: 491, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11086/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 11087/15000, steps: 307, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11088/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11089/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11090/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11091/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11092/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11093/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11094/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11095/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11096/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11097/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 11098/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11099/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11100/15000, steps: 170, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11101/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11102/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11103/15000, steps: 141, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11104/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11105/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 11106/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11107/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11108/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11109/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11110/15000, steps: 312, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11111/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11112/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11113/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11114/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11115/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 11116/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11117/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 11118/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11119/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11120/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11121/15000, steps: 182, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11122/15000, steps: 81, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11123/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11124/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11125/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11126/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11127/15000, steps: 240, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11128/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11129/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 11130/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11131/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 11132/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11133/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11134/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11135/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11136/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 11137/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11138/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11139/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11140/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11141/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11142/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11143/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11144/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11145/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11146/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11147/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11148/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 11149/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11150/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11151/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 11152/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11153/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11154/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11155/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11156/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11157/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 11158/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11159/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11160/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11161/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11162/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11163/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11164/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11165/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 11166/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11167/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11168/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11169/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11170/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11171/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11172/15000, steps: 410, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11173/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11174/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 11175/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11176/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11177/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11178/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11179/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11180/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11181/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11182/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11183/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11184/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11185/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11186/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 11187/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11188/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11189/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11190/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11191/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11192/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11193/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11194/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11195/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11196/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11197/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11198/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11199/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 11200/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11201/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11202/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11203/15000, steps: 472, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11204/15000, steps: 131, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11205/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11206/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11207/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11208/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11209/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11210/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11211/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11212/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11213/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11214/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11215/15000, steps: 371, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11216/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11217/15000, steps: 189, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11218/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11219/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11220/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11221/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11222/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11223/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11224/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11225/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 11226/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11227/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 11228/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11229/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 11230/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11231/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 11232/15000, steps: 466, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11233/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11234/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11235/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11236/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11237/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11238/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11239/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11240/15000, steps: 576, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11241/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11242/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 11243/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11244/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11245/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11246/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11247/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11248/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11249/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11250/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11251/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11252/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11253/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11254/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11255/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11256/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11257/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11258/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 11259/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11260/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11261/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 11262/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11263/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11264/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11265/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11266/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11267/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11268/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11269/15000, steps: 340, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11270/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11271/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11272/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11273/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11274/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11275/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11276/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11277/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11278/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11279/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 11280/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11281/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11282/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11283/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 11284/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11285/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11286/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11287/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11288/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11289/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11290/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11291/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11292/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11293/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11294/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11295/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11296/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11297/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 11298/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 11299/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11300/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11301/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11302/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11303/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11304/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11305/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11306/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11307/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11308/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 11309/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11310/15000, steps: 510, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11311/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11312/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11313/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11314/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11315/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 11316/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11317/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11318/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11319/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11320/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11321/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11322/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 11323/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 11324/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11325/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11326/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11327/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11328/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11329/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11330/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11331/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11332/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11333/15000, steps: 248, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11334/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11335/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11336/15000, steps: 90, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11337/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11338/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11339/15000, steps: 59, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11340/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11341/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11342/15000, steps: 152, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11343/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 11344/15000, steps: 358, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11345/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 11346/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11347/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11348/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11349/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 11350/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11351/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11352/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11353/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11354/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 11355/15000, steps: 222, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11356/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11357/15000, steps: 143, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11358/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 11359/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11360/15000, steps: 582, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11361/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11362/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 11363/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11364/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11365/15000, steps: 91, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11366/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11367/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 11368/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11369/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11370/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 11371/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11372/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11373/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 11374/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 11375/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11376/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11377/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11378/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11379/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11380/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11381/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11382/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 11383/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11384/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11385/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11386/15000, steps: 329, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11387/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11388/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11389/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11390/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11391/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11392/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11393/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11394/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 11395/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11396/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11397/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11398/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11399/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11400/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 11401/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 11402/15000, steps: 549, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11403/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11404/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11405/15000, steps: 236, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11406/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 11407/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11408/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11409/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11410/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11411/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 11412/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11413/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11414/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11415/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11416/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11417/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11418/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11419/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11420/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11421/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11422/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11423/15000, steps: 382, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11424/15000, steps: 407, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11425/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11426/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11427/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11428/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11429/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11430/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11431/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11432/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11433/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11434/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11435/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11436/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11437/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11438/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11439/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11440/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11441/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11442/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11443/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11444/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11445/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11446/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11447/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11448/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 11449/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 11450/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 11451/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11452/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11453/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11454/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11455/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11456/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11457/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11458/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11459/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 11460/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11461/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11462/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11463/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11464/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 11465/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11466/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 11467/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 11468/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11469/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11470/15000, steps: 187, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11471/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11472/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11473/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11474/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 11475/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11476/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11477/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11478/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11479/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11480/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 11481/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11482/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11483/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11484/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11485/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11486/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11487/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11488/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11489/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11490/15000, steps: 138, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 11491/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11492/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 11493/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11494/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11495/15000, steps: 223, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11496/15000, steps: 463, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11497/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11498/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11499/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11500/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11501/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11502/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11503/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11504/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11505/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11506/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11507/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11508/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11509/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 11510/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11511/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11512/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11513/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11514/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 11515/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11516/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11517/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11518/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11519/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11520/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 11521/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11522/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11523/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11524/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11525/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11526/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11527/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11528/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11529/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 11530/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11531/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 11532/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11533/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11534/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11535/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11536/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11537/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11538/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11539/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11540/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11541/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11542/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11543/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11544/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11545/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 11546/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11547/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11548/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11549/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11550/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11551/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11552/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11553/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11554/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11555/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11556/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11557/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11558/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11559/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11560/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11561/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11562/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11563/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 11564/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11565/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11566/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11567/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11568/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11569/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11570/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 11571/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11572/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 11573/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11574/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11575/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11576/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11577/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11578/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11579/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11580/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 11581/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11582/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 11583/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11584/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11585/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 11586/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11587/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11588/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11589/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11590/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11591/15000, steps: 366, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 11592/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11593/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11594/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 11595/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11596/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11597/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11598/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11599/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11600/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11601/15000, steps: 403, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11602/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11603/15000, steps: 311, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11604/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11605/15000, steps: 157, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 11606/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11607/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11608/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11609/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11610/15000, steps: 303, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11611/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11612/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 11613/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11614/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11615/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 11616/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 11617/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 11618/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11619/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11620/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11621/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11622/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11623/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11624/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11625/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11626/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11627/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11628/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11629/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11630/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11631/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11632/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 11633/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11634/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11635/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11636/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11637/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11638/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11639/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11640/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11641/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11642/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11643/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11644/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11645/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11646/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11647/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11648/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11649/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11650/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11651/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11652/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11653/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11654/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11655/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11656/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11657/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11658/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11659/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11660/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11661/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11662/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 11663/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11664/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11665/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11666/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 11667/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11668/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11669/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11670/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11671/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11672/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11673/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11674/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 11675/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11676/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11677/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11678/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11679/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11680/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11681/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11682/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11683/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11684/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11685/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 11686/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 11687/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11688/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11689/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11690/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11691/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11692/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11693/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11694/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11695/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11696/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11697/15000, steps: 172, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11698/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11699/15000, steps: 291, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11700/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11701/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11702/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11703/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11704/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11705/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11706/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11707/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 11708/15000, steps: 64, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11709/15000, steps: 78, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11710/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11711/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11712/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11713/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11714/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11715/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11716/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11717/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11718/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11719/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11720/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11721/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11722/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 11723/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11724/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11725/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11726/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 11727/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11728/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 11729/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11730/15000, steps: 336, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11731/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11732/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 11733/15000, steps: 521, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11734/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11735/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 11736/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11737/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11738/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11739/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11740/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11741/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11742/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11743/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11744/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11745/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11746/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11747/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11748/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11749/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11750/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 11751/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 11752/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11753/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11754/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11755/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11756/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11757/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11758/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11759/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11760/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11761/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11762/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11763/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11764/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11765/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11766/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11767/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11768/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11769/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11770/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11771/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11772/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11773/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11774/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11775/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 11776/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11777/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11778/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11779/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11780/15000, steps: 95, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11781/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11782/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11783/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 11784/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11785/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 11786/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11787/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11788/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 11789/15000, steps: 159, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11790/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11791/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 11792/15000, steps: 514, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11793/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 11794/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11795/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11796/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 11797/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 11798/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11799/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11800/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11801/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11802/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11803/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11804/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11805/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11806/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11807/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11808/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11809/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11810/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11811/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11812/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11813/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11814/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11815/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11816/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 11817/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11818/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11819/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11820/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 11821/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 11822/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 11823/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11824/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11825/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11826/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11827/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 11828/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11829/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 11830/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11831/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11832/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11833/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11834/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 11835/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11836/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11837/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11838/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 11839/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11840/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11841/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11842/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 11843/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11844/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 11845/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11846/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11847/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 11848/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11849/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11850/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11851/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11852/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 11853/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11854/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11855/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11856/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11857/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 11858/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 11859/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11860/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11861/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11862/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11863/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 11864/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11865/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11866/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 11867/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11868/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11869/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 11870/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11871/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 11872/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11873/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 11874/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11875/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 11876/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11877/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11878/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 11879/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11880/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11881/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11882/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 11883/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11884/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11885/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11886/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 11887/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 11888/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11889/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11890/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11891/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11892/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11893/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11894/15000, steps: 66, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 11895/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11896/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 11897/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11898/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11899/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11900/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11901/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 11902/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11903/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11904/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 11905/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11906/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11907/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 11908/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11909/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 11910/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 11911/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11912/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11913/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11914/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11915/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11916/15000, steps: 107, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11917/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 11918/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11919/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11920/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11921/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11922/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11923/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11924/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11925/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 11926/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 11927/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11928/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 11929/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11930/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 11931/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11932/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11933/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 11934/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 11935/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11936/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 11937/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 11938/15000, steps: 240, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11939/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 11940/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 11941/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 11942/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 11943/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11944/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11945/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 11946/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11947/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11948/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 11949/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11950/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11951/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 11952/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 11953/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 11954/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 11955/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11956/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 11957/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 11958/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 11959/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 11960/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11961/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 11962/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 11963/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 11964/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 11965/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 11966/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 11967/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 11968/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11969/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 11970/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 11971/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 11972/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 11973/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 11974/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 11975/15000, steps: 374, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 11976/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 11977/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 11978/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 11979/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 11980/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 11981/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 11982/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 11983/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 11984/15000, steps: 435, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 11985/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11986/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 11987/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 11988/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 11989/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 11990/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 11991/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 11992/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 11993/15000, steps: 259, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 11994/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 11995/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 11996/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 11997/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 11998/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 11999/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12000/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12001/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12002/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12003/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 12004/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12005/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12006/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12007/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12008/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12009/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12010/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12011/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12012/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12013/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12014/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12015/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12016/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 12017/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12018/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12019/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12020/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12021/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12022/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 12023/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12024/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12025/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 12026/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12027/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12028/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12029/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12030/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12031/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12032/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12033/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12034/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12035/15000, steps: 360, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12036/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12037/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12038/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12039/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 12040/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12041/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12042/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12043/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12044/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 12045/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12046/15000, steps: 130, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12047/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12048/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12049/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12050/15000, steps: 282, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12051/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12052/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 12053/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12054/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 12055/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12056/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12057/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 12058/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 12059/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12060/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12061/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 12062/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12063/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12064/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12065/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12066/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12067/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12068/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12069/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12070/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12071/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 12072/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12073/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12074/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 12075/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12076/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12077/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12078/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12079/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12080/15000, steps: 133, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12081/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12082/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12083/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 12084/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12085/15000, steps: 208, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12086/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12087/15000, steps: 495, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12088/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12089/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12090/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12091/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12092/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12093/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 12094/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12095/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12096/15000, steps: 353, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12097/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12098/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12099/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 12100/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12101/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12102/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12103/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12104/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 12105/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12106/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12107/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 12108/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 12109/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 12110/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12111/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12112/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12113/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12114/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12115/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12116/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12117/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12118/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12119/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12120/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12121/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12122/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12123/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12124/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12125/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12126/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12127/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12128/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12129/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12130/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 12131/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 12132/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12133/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12134/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12135/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 12136/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12137/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12138/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12139/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12140/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12141/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12142/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12143/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12144/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12145/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12146/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12147/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12148/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12149/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12150/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12151/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12152/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12153/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12154/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12155/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12156/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12157/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 12158/15000, steps: 272, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12159/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12160/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12161/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12162/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12163/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12164/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12165/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12166/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12167/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12168/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12169/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12170/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12171/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12172/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12173/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 12174/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12175/15000, steps: 91, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12176/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12177/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12178/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 12179/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12180/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12181/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12182/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12183/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12184/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12185/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12186/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12187/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12188/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12189/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12190/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 12191/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12192/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12193/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12194/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12195/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12196/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12197/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12198/15000, steps: 360, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12199/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12200/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12201/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12202/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12203/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 12204/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12205/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12206/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12207/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12208/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 12209/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12210/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12211/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12212/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12213/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12214/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 12215/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12216/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12217/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12218/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12219/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 12220/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12221/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 12222/15000, steps: 308, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12223/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12224/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12225/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12226/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12227/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12228/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12229/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12230/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 12231/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12232/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12233/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12234/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12235/15000, steps: 354, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12236/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12237/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12238/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12239/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12240/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12241/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12242/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 12243/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12244/15000, steps: 147, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12245/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12246/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 12247/15000, steps: 189, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 12248/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12249/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 12250/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12251/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12252/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12253/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12254/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12255/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12256/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 12257/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 12258/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12259/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12260/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12261/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12262/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 12263/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12264/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 12265/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12266/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12267/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12268/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 12269/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12270/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12271/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12272/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12273/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12274/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 12275/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12276/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 12277/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12278/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12279/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12280/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12281/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12282/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12283/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12284/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12285/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12286/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12287/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12288/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12289/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12290/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 12291/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12292/15000, steps: 320, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12293/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12294/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12295/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12296/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12297/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12298/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12299/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12300/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12301/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12302/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12303/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12304/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12305/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12306/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12307/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12308/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12309/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12310/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12311/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12312/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12313/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12314/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12315/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12316/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12317/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12318/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12319/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12320/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12321/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12322/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12323/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12324/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 12325/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12326/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12327/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 12328/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12329/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12330/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12331/15000, steps: 408, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12332/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12333/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12334/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12335/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12336/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12337/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12338/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12339/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12340/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12341/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12342/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12343/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12344/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12345/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12346/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12347/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12348/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12349/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 12350/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12351/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12352/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12353/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12354/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12355/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12356/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12357/15000, steps: 142, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12358/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12359/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12360/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 12361/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12362/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12363/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12364/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12365/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12366/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12367/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12368/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12369/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12370/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12371/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12372/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12373/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12374/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12375/15000, steps: 194, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12376/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12377/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12378/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12379/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12380/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12381/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12382/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 12383/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12384/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12385/15000, steps: 298, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12386/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12387/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 12388/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12389/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12390/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12391/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12392/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12393/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12394/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12395/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 12396/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12397/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12398/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12399/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12400/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12401/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12402/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12403/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12404/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12405/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12406/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12407/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12408/15000, steps: 111, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 12409/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12410/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12411/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 12412/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 12413/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 12414/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12415/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12416/15000, steps: 178, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12417/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12418/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12419/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12420/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12421/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12422/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12423/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12424/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12425/15000, steps: 577, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12426/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12427/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12428/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12429/15000, steps: 447, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12430/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12431/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12432/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12433/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12434/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12435/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12436/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12437/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12438/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12439/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12440/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12441/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12442/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12443/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12444/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12445/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12446/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12447/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 12448/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12449/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12450/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12451/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12452/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12453/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12454/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 12455/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12456/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12457/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12458/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12459/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12460/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12461/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12462/15000, steps: 154, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12463/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12464/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12465/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12466/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12467/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 12468/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12469/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12470/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12471/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12472/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12473/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 12474/15000, steps: 124, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12475/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12476/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12477/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12478/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 12479/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12480/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12481/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12482/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12483/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12484/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12485/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12486/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 12487/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12488/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12489/15000, steps: 421, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12490/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12491/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12492/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12493/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12494/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12495/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12496/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12497/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12498/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12499/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12500/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12501/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12502/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 12503/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12504/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 12505/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 12506/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12507/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 12508/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 12509/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12510/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12511/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 12512/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 12513/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12514/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12515/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12516/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12517/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 12518/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12519/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12520/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12521/15000, steps: 418, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12522/15000, steps: 575, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12523/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12524/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12525/15000, steps: 426, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 12526/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12527/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12528/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12529/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12530/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12531/15000, steps: 125, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12532/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12533/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12534/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12535/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 12536/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12537/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 12538/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12539/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12540/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12541/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12542/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 12543/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12544/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12545/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12546/15000, steps: 103, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12547/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12548/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12549/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12550/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12551/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12552/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12553/15000, steps: 230, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12554/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12555/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12556/15000, steps: 279, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12557/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12558/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12559/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 12560/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12561/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12562/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12563/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12564/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12565/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 12566/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12567/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12568/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12569/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12570/15000, steps: 472, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12571/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12572/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12573/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12574/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 12575/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12576/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12577/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12578/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12579/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12580/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12581/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12582/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12583/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12584/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12585/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12586/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12587/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12588/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12589/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12590/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12591/15000, steps: 433, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12592/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12593/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12594/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12595/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12596/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12597/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 12598/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12599/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12600/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12601/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12602/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12603/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12604/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12605/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12606/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12607/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12608/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12609/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12610/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12611/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12612/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 12613/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12614/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 12615/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12616/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12617/15000, steps: 334, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12618/15000, steps: 166, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12619/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12620/15000, steps: 524, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12621/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12622/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 12623/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12624/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12625/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12626/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12627/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12628/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12629/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12630/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12631/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12632/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12633/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12634/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12635/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12636/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12637/15000, steps: 255, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12638/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12639/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12640/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12641/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12642/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12643/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12644/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12645/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12646/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12647/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12648/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 12649/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12650/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12651/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12652/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12653/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12654/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12655/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12656/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12657/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 12658/15000, steps: 442, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12659/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12660/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12661/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12662/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12663/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12664/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12665/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12666/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 12667/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12668/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12669/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12670/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12671/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12672/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 12673/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12674/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 12675/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12676/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12677/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 12678/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 12679/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12680/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12681/15000, steps: 526, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12682/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12683/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12684/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12685/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12686/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12687/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12688/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12689/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12690/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12691/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12692/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12693/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12694/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12695/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12696/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12697/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12698/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12699/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12700/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12701/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12702/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12703/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12704/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12705/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12706/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12707/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12708/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 12709/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12710/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12711/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12712/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 12713/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12714/15000, steps: 455, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12715/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12716/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12717/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12718/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12719/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 12720/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12721/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12722/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12723/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12724/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12725/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12726/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12727/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12728/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12729/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12730/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12731/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12732/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12733/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12734/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12735/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12736/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12737/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12738/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 12739/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12740/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12741/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12742/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 12743/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12744/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12745/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12746/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 12747/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 12748/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12749/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12750/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12751/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12752/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12753/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12754/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12755/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12756/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12757/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 12758/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12759/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12760/15000, steps: 522, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 12761/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 12762/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12763/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12764/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12765/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12766/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12767/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12768/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12769/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12770/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12771/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12772/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12773/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12774/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12775/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12776/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12777/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12778/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12779/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 12780/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12781/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12782/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12783/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12784/15000, steps: 194, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12785/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12786/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12787/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12788/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12789/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12790/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12791/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12792/15000, steps: 120, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 12793/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 12794/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 12795/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12796/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12797/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12798/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12799/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12800/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 12801/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 12802/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12803/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12804/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12805/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12806/15000, steps: 141, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12807/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12808/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 12809/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12810/15000, steps: 489, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12811/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12812/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12813/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 12814/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12815/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 12816/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 12817/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12818/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12819/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12820/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12821/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12822/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12823/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 12824/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12825/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 12826/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12827/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 12828/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12829/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12830/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12831/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12832/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12833/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12834/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12835/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12836/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12837/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 12838/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12839/15000, steps: 321, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 12840/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12841/15000, steps: 419, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12842/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 12843/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12844/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 12845/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12846/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12847/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12848/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12849/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12850/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12851/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 12852/15000, steps: 360, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 12853/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 12854/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 12855/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 12856/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 12857/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12858/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 12859/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12860/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 12861/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12862/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12863/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 12864/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 12865/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12866/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12867/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 12868/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12869/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12870/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12871/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12872/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12873/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12874/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12875/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12876/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12877/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 12878/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12879/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12880/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 12881/15000, steps: 358, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 12882/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12883/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12884/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12885/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12886/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 12887/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12888/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 12889/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12890/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12891/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12892/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12893/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12894/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 12895/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 12896/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12897/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12898/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12899/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12900/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12901/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 12902/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12903/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 12904/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12905/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 12906/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 12907/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 5, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12908/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12909/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 12910/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12911/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 12912/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 12913/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 12914/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12915/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 12916/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12917/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12918/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12919/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 12920/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12921/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12922/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 12923/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 12924/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12925/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 12926/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 12927/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 12928/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 12929/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12930/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12931/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 12932/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 12933/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12934/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12935/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 12936/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 12937/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12938/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12939/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 12940/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12941/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 12942/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 12943/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 12944/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12945/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 12946/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 12947/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12948/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12949/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12950/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 12951/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12952/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12953/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 12954/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 12955/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 12956/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12957/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 12958/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 12959/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 12960/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 12961/15000, steps: 160, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 12962/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 12963/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12964/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 12965/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 12966/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12967/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12968/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 12969/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 12970/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12971/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12972/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 12973/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 12974/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 12975/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 12976/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 12977/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 12978/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 12979/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 12980/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 12981/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 12982/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 12983/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 12984/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12985/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 12986/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 12987/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 12988/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 12989/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 12990/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 12991/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 12992/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 12993/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 12994/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 12995/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 12996/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 12997/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 12998/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 12999/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13000/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13001/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13002/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13003/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13004/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13005/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13006/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 13007/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13008/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13009/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13010/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13011/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13012/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13013/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13014/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13015/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13016/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13017/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13018/15000, steps: 396, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13019/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13020/15000, steps: 243, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13021/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 13022/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 13023/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13024/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13025/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13026/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13027/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13028/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 13029/15000, steps: 564, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13030/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13031/15000, steps: 204, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13032/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13033/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13034/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13035/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13036/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 13037/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 13038/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13039/15000, steps: 154, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13040/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 13041/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13042/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13043/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13044/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 13045/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13046/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13047/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13048/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 13049/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13050/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13051/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 13052/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13053/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13054/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 13055/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13056/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13057/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13058/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13059/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13060/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13061/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13062/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13063/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 13064/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13065/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13066/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13067/15000, steps: 525, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13068/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 13069/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13070/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 13071/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13072/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13073/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13074/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13075/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13076/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13077/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13078/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13079/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 13080/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13081/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13082/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13083/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13084/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13085/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13086/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 13087/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13088/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13089/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13090/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13091/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 13092/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 13093/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 13094/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 13095/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13096/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 13097/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 13098/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 13099/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13100/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13101/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13102/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 13103/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13104/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13105/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13106/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13107/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13108/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13109/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 13110/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13111/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13112/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13113/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13114/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13115/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13116/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13117/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13118/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13119/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13120/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13121/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13122/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13123/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13124/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13125/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13126/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13127/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 13128/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13129/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 13130/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13131/15000, steps: 393, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13132/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13133/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13134/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13135/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13136/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13137/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13138/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13139/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 13140/15000, steps: 96, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13141/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13142/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13143/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13144/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13145/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13146/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 13147/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13148/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13149/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13150/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13151/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 13152/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13153/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13154/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13155/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13156/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13157/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13158/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13159/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 13160/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 13161/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13162/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13163/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13164/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13165/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13166/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13167/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13168/15000, steps: 543, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13169/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13170/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13171/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13172/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 13173/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 13174/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 13175/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13176/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13177/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13178/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13179/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13180/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13181/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13182/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13183/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13184/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13185/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 13186/15000, steps: 172, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13187/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13188/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13189/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13190/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13191/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13192/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13193/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 13194/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13195/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13196/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13197/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13198/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13199/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13200/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13201/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13202/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13203/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13204/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13205/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13206/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13207/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 13208/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13209/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13210/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13211/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13212/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13213/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13214/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13215/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 13216/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13217/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13218/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 13219/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 13220/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13221/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13222/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13223/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13224/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13225/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13226/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 13227/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13228/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13229/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 13230/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13231/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13232/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 13233/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13234/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13235/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13236/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13237/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 13238/15000, steps: 465, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 13239/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 13240/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13241/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13242/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13243/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13244/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13245/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13246/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13247/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13248/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13249/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13250/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13251/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13252/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13253/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13254/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13255/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13256/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13257/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13258/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13259/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13260/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13261/15000, steps: 79, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13262/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13263/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13264/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13265/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13266/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13267/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13268/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13269/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13270/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13271/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13272/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13273/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 13274/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13275/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13276/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13277/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13278/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13279/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 13280/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13281/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13282/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13283/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 13284/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13285/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13286/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13287/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13288/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13289/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13290/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13291/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13292/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13293/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13294/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13295/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13296/15000, steps: 585, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13297/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13298/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13299/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13300/15000, steps: 452, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13301/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13302/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13303/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13304/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13305/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13306/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13307/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13308/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13309/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13310/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13311/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13312/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13313/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 13314/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13315/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13316/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13317/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 13318/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13319/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 13320/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 13321/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13322/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13323/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13324/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13325/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13326/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13327/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13328/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13329/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13330/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13331/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13332/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13333/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13334/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13335/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13336/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13337/15000, steps: 165, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13338/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13339/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13340/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 13341/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13342/15000, steps: 234, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13343/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 13344/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13345/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13346/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 13347/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13348/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13349/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13350/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13351/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13352/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 13353/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13354/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13355/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13356/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13357/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13358/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13359/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13360/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13361/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13362/15000, steps: 374, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13363/15000, steps: 392, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13364/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 13365/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13366/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13367/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13368/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13369/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 13370/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13371/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 13372/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13373/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13374/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13375/15000, steps: 151, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13376/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13377/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13378/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13379/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13380/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 13381/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13382/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13383/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 13384/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13385/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13386/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13387/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13388/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13389/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 13390/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13391/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13392/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13393/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13394/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13395/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13396/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13397/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13398/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 13399/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13400/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 13401/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13402/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13403/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13404/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13405/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13406/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13407/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13408/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13409/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13410/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13411/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13412/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 13413/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13414/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13415/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 13416/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13417/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13418/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13419/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 13420/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13421/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13422/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13423/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13424/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13425/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 13426/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13427/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13428/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13429/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13430/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13431/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 13432/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13433/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13434/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 13435/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13436/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 13437/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13438/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13439/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13440/15000, steps: 328, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13441/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13442/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13443/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13444/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13445/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13446/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 13447/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13448/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13449/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13450/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13451/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13452/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13453/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13454/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13455/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13456/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13457/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13458/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13459/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13460/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13461/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 13462/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13463/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13464/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13465/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 13466/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13467/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13468/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13469/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 13470/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 13471/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13472/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13473/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13474/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13475/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13476/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13477/15000, steps: 366, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 13478/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13479/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13480/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13481/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13482/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13483/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 13484/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13485/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13486/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13487/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 13488/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13489/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13490/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13491/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13492/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13493/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13494/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13495/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13496/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13497/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13498/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13499/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13500/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 13501/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 13502/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13503/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13504/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13505/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13506/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13507/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13508/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13509/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13510/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13511/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13512/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13513/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13514/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 13515/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13516/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13517/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13518/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13519/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13520/15000, steps: 72, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 8, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13521/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 13522/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13523/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13524/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13525/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13526/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13527/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13528/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13529/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 13530/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13531/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13532/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13533/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13534/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13535/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 13536/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13537/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 13538/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13539/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 13540/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 13541/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13542/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13543/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13544/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13545/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13546/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13547/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13548/15000, steps: 486, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13549/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13550/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13551/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13552/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13553/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13554/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 13555/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13556/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13557/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13558/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 13559/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13560/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13561/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13562/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13563/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13564/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13565/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13566/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13567/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13568/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 13569/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13570/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13571/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13572/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 13573/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13574/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 13575/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13576/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13577/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 13578/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13579/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13580/15000, steps: 388, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13581/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13582/15000, steps: 412, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13583/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13584/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13585/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13586/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13587/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 13588/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13589/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 13590/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13591/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13592/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 13593/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13594/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13595/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13596/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13597/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13598/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13599/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13600/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13601/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13602/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13603/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 13604/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 13605/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13606/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13607/15000, steps: 290, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 13608/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13609/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13610/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13611/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13612/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13613/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13614/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13615/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 13616/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13617/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13618/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13619/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 13620/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13621/15000, steps: 299, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 13622/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13623/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13624/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13625/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13626/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13627/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13628/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13629/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 13630/15000, steps: 265, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 13631/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13632/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13633/15000, steps: 64, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13634/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13635/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13636/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 13637/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 13638/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13639/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13640/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13641/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13642/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13643/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13644/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13645/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13646/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 13647/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13648/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13649/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13650/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13651/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13652/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13653/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13654/15000, steps: 352, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 13655/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13656/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13657/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 13658/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13659/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13660/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13661/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13662/15000, steps: 265, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13663/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13664/15000, steps: 68, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13665/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13666/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 13667/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13668/15000, steps: 441, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13669/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 13670/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13671/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13672/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13673/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13674/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13675/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 13676/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 13677/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13678/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13679/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13680/15000, steps: 412, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13681/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 13682/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 13683/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13684/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13685/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13686/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13687/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13688/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13689/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13690/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 13691/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 13692/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13693/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13694/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13695/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13696/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13697/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13698/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13699/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13700/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13701/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13702/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13703/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13704/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13705/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 13706/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13707/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13708/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13709/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13710/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13711/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13712/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13713/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 13714/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13715/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13716/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13717/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13718/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13719/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13720/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 13721/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13722/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 13723/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13724/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13725/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 13726/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13727/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13728/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13729/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 13730/15000, steps: 264, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13731/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13732/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13733/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13734/15000, steps: 505, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13735/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13736/15000, steps: 87, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13737/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13738/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13739/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13740/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13741/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 13742/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13743/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13744/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13745/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 13746/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13747/15000, steps: 287, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 13748/15000, steps: 505, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 13749/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13750/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 13751/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 13752/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13753/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13754/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13755/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13756/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13757/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13758/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13759/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13760/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 13761/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13762/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 13763/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13764/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13765/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13766/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13767/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13768/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13769/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13770/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13771/15000, steps: 380, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13772/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13773/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13774/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13775/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13776/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13777/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 13778/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13779/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 13780/15000, steps: 581, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13781/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 13782/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 13783/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 13784/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 13785/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 13786/15000, steps: 510, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13787/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13788/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 13789/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13790/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 13791/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13792/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13793/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13794/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13795/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13796/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13797/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 13798/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13799/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13800/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 13801/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13802/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13803/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13804/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13805/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 13806/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13807/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13808/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13809/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 13810/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13811/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 13812/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 13813/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13814/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 13815/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13816/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 13817/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13818/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13819/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 13820/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13821/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13822/15000, steps: 473, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13823/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13824/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13825/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13826/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 13827/15000, steps: 580, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 13828/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13829/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13830/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 13831/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13832/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13833/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13834/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13835/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13836/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13837/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 13838/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13839/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13840/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13841/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13842/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13843/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13844/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 13845/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 13846/15000, steps: 208, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13847/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 13848/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13849/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13850/15000, steps: 286, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13851/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13852/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13853/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13854/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13855/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13856/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13857/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13858/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13859/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 13860/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13861/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13862/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13863/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13864/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13865/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13866/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13867/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 13868/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13869/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13870/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13871/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13872/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 13873/15000, steps: 511, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13874/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13875/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 13876/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 13877/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 13878/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13879/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 13880/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13881/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 13882/15000, steps: 156, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13883/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13884/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13885/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13886/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13887/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 13888/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13889/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 13890/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13891/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13892/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13893/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 13894/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13895/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 13896/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 13897/15000, steps: 296, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13898/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 13899/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13900/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 13901/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 13902/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13903/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13904/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13905/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 13906/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13907/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13908/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13909/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 13910/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13911/15000, steps: 63, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 13912/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13913/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13914/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 13915/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13916/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 13917/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13918/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13919/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 13920/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13921/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13922/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 13923/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 13924/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13925/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 13926/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 13927/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13928/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 13929/15000, steps: 360, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 13930/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 13931/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13932/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13933/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 13934/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13935/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 13936/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13937/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13938/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 13939/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13940/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 13941/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13942/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 13943/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 13944/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 13945/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 13946/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 13947/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 13948/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 13949/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 13950/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 13951/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13952/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 13953/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 13954/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 13955/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 13956/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 13957/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 13958/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13959/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 13960/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 13961/15000, steps: 55, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 13962/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 13963/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13964/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 13965/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13966/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13967/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 13968/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 13969/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 13970/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13971/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 13972/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 13973/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 13974/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 13975/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 13976/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 13977/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 13978/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13979/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13980/15000, steps: 408, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 13981/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 13982/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 13983/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 13984/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 13985/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13986/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 13987/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 13988/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 13989/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 13990/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 13991/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 13992/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 13993/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 13994/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 13995/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 13996/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 13997/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 13998/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 13999/15000, steps: 110, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14000/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14001/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14002/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14003/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14004/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14005/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14006/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14007/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14008/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14009/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14010/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14011/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14012/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14013/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14014/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14015/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14016/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14017/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14018/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 14019/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14020/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14021/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14022/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14023/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14024/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14025/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14026/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14027/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14028/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14029/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14030/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14031/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 14032/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14033/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14034/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14035/15000, steps: 186, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14036/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14037/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14038/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14039/15000, steps: 302, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14040/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14041/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14042/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14043/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14044/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14045/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14046/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14047/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14048/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 14049/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14050/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14051/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 14052/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14053/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14054/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14055/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14056/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14057/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14058/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 14059/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 14060/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14061/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14062/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 14063/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14064/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14065/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14066/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14067/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14068/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14069/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14070/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14071/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14072/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14073/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14074/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14075/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 14076/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14077/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14078/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14079/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14080/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14081/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14082/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 14083/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14084/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14085/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14086/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14087/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14088/15000, steps: 590, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14089/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14090/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14091/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 14092/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14093/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14094/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14095/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 14096/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14097/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 14098/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 14099/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14100/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14101/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14102/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14103/15000, steps: 544, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 14104/15000, steps: 201, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14105/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14106/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14107/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14108/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14109/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14110/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14111/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14112/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14113/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 14114/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14115/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14116/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14117/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14118/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14119/15000, steps: 346, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14120/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 14121/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14122/15000, steps: 80, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14123/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 14124/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14125/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14126/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14127/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14128/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 14129/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14130/15000, steps: 190, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 14131/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14132/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14133/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14134/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14135/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14136/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14137/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14138/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14139/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14140/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14141/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14142/15000, steps: 415, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14143/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14144/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 14145/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14146/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14147/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14148/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14149/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14150/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14151/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14152/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14153/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14154/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14155/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14156/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14157/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14158/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14159/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14160/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14161/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14162/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14163/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14164/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 14165/15000, steps: 215, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14166/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14167/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14168/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14169/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14170/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14171/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14172/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14173/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14174/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14175/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 14176/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14177/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14178/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14179/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14180/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14181/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14182/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14183/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14184/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14185/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 14186/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 14187/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14188/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14189/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14190/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14191/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14192/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14193/15000, steps: 312, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14194/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14195/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14196/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14197/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 14198/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14199/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14200/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 14201/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 14202/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14203/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14204/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14205/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 14206/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14207/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14208/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14209/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 5, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14210/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14211/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14212/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14213/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 14214/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14215/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14216/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14217/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14218/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 14219/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14220/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14221/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14222/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14223/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14224/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14225/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14226/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14227/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14228/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 14229/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14230/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14231/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14232/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 14233/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14234/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14235/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14236/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14237/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14238/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 14239/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14240/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14241/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14242/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 14243/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14244/15000, steps: 112, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 14245/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14246/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14247/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14248/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14249/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 14250/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14251/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14252/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14253/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14254/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14255/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14256/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14257/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14258/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 130.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14259/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14260/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 14261/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14262/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14263/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14264/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14265/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14266/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14267/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 14268/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14269/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 14270/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 14271/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14272/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14273/15000, steps: 203, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14274/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14275/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14276/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14277/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 14278/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14279/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14280/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14281/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 14282/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14283/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14284/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14285/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14286/15000, steps: 517, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 2, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14287/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14288/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14289/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14290/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 14291/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14292/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14293/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14294/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14295/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14296/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14297/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14298/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 14299/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14300/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14301/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14302/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14303/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 14304/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14305/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14306/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14307/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14308/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14309/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14310/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14311/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 14312/15000, steps: 567, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14313/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14314/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14315/15000, steps: 324, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14316/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14317/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14318/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14319/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 14320/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14321/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14322/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 14323/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 14324/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14325/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14326/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14327/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14328/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 14329/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 14330/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14331/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14332/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14333/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 14334/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14335/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14336/15000, steps: 330, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14337/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 14338/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14339/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14340/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14341/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14342/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14343/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14344/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14345/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14346/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14347/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14348/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 14349/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14350/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14351/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 14352/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14353/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14354/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14355/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14356/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14357/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14358/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14359/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14360/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14361/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14362/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 0, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14363/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14364/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14365/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14366/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14367/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 14368/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 14369/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14370/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14371/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14372/15000, steps: 581, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14373/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14374/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 14375/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14376/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 14377/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14378/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14379/15000, steps: 565, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14380/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 14381/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14382/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14383/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 14384/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14385/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 14386/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14387/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14388/15000, steps: 258, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14389/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14390/15000, steps: 282, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14391/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14392/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14393/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14394/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14395/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14396/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14397/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 14398/15000, steps: 118, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14399/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14400/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14401/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14402/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14403/15000, steps: 107, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14404/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14405/15000, steps: 114, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 14406/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14407/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14408/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14409/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14410/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14411/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14412/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14413/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14414/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14415/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14416/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14417/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14418/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14419/15000, steps: 210, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14420/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14421/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14422/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 14423/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14424/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14425/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14426/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14427/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14428/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14429/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14430/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14431/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14432/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14433/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14434/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14435/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14436/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14437/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 14438/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14439/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 3, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14440/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14441/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14442/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14443/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14444/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14445/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14446/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14447/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14448/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14449/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14450/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14451/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 14452/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 14453/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14454/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14455/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14456/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14457/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14458/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 14459/15000, steps: 128, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14460/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14461/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14462/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14463/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14464/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14465/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 14466/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14467/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14468/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 14469/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14470/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14471/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14472/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14473/15000, steps: 549, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14474/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14475/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14476/15000, steps: 310, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 14477/15000, steps: 150, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14478/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 1)\n",
      "episode: 14479/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 14480/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14481/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14482/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14483/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 14484/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14485/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14486/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14487/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14488/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14489/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 14490/15000, steps: 181, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14491/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 14492/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14493/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 14494/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14495/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14496/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14497/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14498/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14499/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14500/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14501/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14502/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14503/15000, steps: 591, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14504/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 14505/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 14506/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14507/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14508/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14509/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14510/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14511/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 14512/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14513/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14514/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14515/15000, steps: 89, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14516/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14517/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14518/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14519/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14520/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14521/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14522/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14523/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14524/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14525/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14526/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14527/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14528/15000, steps: 89, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 14529/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14530/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14531/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14532/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14533/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14534/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14535/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 14536/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14537/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14538/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14539/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14540/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14541/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14542/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14543/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14544/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14545/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14546/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14547/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14548/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 14549/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 14550/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14551/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14552/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14553/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14554/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14555/15000, steps: 119, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14556/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14557/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14558/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14559/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 14560/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14561/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 14562/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14563/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14564/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14565/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14566/15000, steps: 255, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14567/15000, steps: 291, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14568/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14569/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14570/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14571/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14572/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14573/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14574/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14575/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 14576/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14577/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14578/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14579/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14580/15000, steps: 496, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14581/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14582/15000, steps: 538, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14583/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14584/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14585/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14586/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14587/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14588/15000, steps: 405, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 14589/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14590/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14591/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14592/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14593/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14594/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14595/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 14596/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14597/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 14598/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14599/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14600/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14601/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14602/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14603/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14604/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 14605/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14606/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14607/15000, steps: 462, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14608/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 14609/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14610/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14611/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14612/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14613/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14614/15000, steps: 515, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14615/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14616/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14617/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 14618/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 14619/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 14620/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14621/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14622/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14623/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14624/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14625/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14626/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14627/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14628/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14629/15000, steps: 157, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14630/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14631/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14632/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14633/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14634/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14635/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14636/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14637/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14638/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 14639/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 14640/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 14641/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 14642/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14643/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14644/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 350.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14645/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14646/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14647/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14648/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 14649/15000, steps: 272, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 14650/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14651/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14652/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14653/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14654/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14655/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14656/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14657/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14658/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14659/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14660/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14661/15000, steps: 149, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14662/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14663/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 14664/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14665/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14666/15000, steps: 433, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14667/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 14668/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14669/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 20.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14670/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14671/15000, steps: 91, e: 0.001\n",
      "accumulated_rewards_per_episode: 50.0\n",
      "START state: (0, 0, 9, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14672/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14673/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14674/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 14675/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 170.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14676/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14677/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14678/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14679/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 14680/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14681/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 0, 3)\n",
      "episode: 14682/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14683/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14684/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14685/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14686/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14687/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14688/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14689/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14690/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14691/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14692/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14693/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14694/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 14695/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14696/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14697/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14698/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14699/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14700/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14701/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14702/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14703/15000, steps: 172, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14704/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14705/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14706/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 70.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 14707/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14708/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14709/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 5)\n",
      "episode: 14710/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14711/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14712/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14713/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14714/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 30.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14715/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14716/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14717/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14718/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14719/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 0, 9)\n",
      "episode: 14720/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14721/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14722/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14723/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14724/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14725/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14726/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14727/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14728/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14729/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14730/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14731/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 14732/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14733/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14734/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14735/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 14736/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14737/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14738/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14739/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14740/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14741/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14742/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14743/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14744/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14745/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14746/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14747/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14748/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 4, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14749/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14750/15000, steps: 404, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14751/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14752/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14753/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 14754/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14755/15000, steps: 187, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 14756/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14757/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14758/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14759/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14760/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14761/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14762/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 14763/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14764/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14765/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14766/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 14767/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14768/15000, steps: 322, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14769/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14770/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 14771/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 14772/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 14773/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14774/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14775/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14776/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14777/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14778/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14779/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14780/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14781/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14782/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14783/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 10.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 14784/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 14785/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14786/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14787/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 14788/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14789/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14790/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14791/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14792/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 14793/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14794/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14795/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14796/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14797/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14798/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14799/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14800/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 14801/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 1)\n",
      "episode: 14802/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14803/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14804/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14805/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14806/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 0, 4)\n",
      "episode: 14807/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14808/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14809/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 160.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14810/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14811/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14812/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14813/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14814/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 8, 2)\n",
      "episode: 14815/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14816/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 9)\n",
      "episode: 14817/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14818/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14819/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14820/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14821/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14822/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14823/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 14824/15000, steps: 132, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14825/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14826/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 9)\n",
      "episode: 14827/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14828/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14829/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14830/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14831/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14832/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14833/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14834/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 14835/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14836/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14837/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14838/15000, steps: 551, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14839/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 14840/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 4)\n",
      "episode: 14841/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14842/15000, steps: 441, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14843/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14844/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14845/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 2, 6)\n",
      "episode: 14846/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 14847/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14848/15000, steps: 176, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 5)\n",
      "episode: 14849/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14850/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14851/15000, steps: 205, e: 0.001\n",
      "accumulated_rewards_per_episode: 90.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14852/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 7)\n",
      "episode: 14853/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 2, 8)\n",
      "episode: 14854/15000, steps: 580, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14855/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 5)\n",
      "episode: 14856/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14857/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14858/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14859/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14860/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 14861/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 1, 3)\n",
      "episode: 14862/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14863/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14864/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14865/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14866/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14867/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14868/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 0)\n",
      "episode: 14869/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 0)\n",
      "episode: 14870/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14871/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14872/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14873/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14874/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 4)\n",
      "episode: 14875/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14876/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 3, 6)\n",
      "episode: 14877/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14878/15000, steps: 425, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14879/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 14880/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14881/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 6)\n",
      "episode: 14882/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14883/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14884/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 1)\n",
      "episode: 14885/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 2, 7)\n",
      "episode: 14886/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14887/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 14888/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14889/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 1, 9)\n",
      "episode: 14890/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14891/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 4)\n",
      "episode: 14892/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14893/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14894/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14895/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 2, 0)\n",
      "episode: 14896/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14897/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 9)\n",
      "episode: 14898/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 0)\n",
      "episode: 14899/15000, steps: 91, e: 0.001\n",
      "accumulated_rewards_per_episode: 40.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14900/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 7, 3)\n",
      "episode: 14901/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14902/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14903/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 8)\n",
      "episode: 14904/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 14905/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14906/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14907/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14908/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 150.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14909/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 6)\n",
      "episode: 14910/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14911/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 0)\n",
      "episode: 14912/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 9)\n",
      "episode: 14913/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 14914/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14915/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14916/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14917/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 110.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14918/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14919/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 1, 7)\n",
      "episode: 14920/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14921/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 60.0\n",
      "START state: (0, 0, 7, 6)\n",
      "episode: 14922/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14923/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 0, 6)\n",
      "episode: 14924/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14925/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 6, 3)\n",
      "episode: 14926/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 6, 8)\n",
      "episode: 14927/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 8, 1)\n",
      "episode: 14928/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 3)\n",
      "episode: 14929/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 8, 6)\n",
      "episode: 14930/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14931/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 7, 1)\n",
      "episode: 14932/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14933/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 220.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 14934/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14935/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 2, 1)\n",
      "episode: 14936/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 8)\n",
      "episode: 14937/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14938/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14939/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 4, 7)\n",
      "episode: 14940/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14941/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14942/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 0, 8)\n",
      "episode: 14943/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 3, 2)\n",
      "episode: 14944/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 6, 2)\n",
      "episode: 14945/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 14946/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 260.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14947/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 9)\n",
      "episode: 14948/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 7, 8)\n",
      "episode: 14949/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 190.0\n",
      "START state: (0, 0, 6, 7)\n",
      "episode: 14950/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 8, 3)\n",
      "episode: 14951/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 14952/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 7)\n",
      "episode: 14953/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 14954/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 100.0\n",
      "START state: (0, 0, 9, 0)\n",
      "episode: 14955/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 4, 9)\n",
      "episode: 14956/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 0, 7)\n",
      "episode: 14957/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14958/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14959/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14960/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 4)\n",
      "episode: 14961/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 8, 7)\n",
      "episode: 14962/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 5)\n",
      "episode: 14963/15000, steps: 509, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 8, 5)\n",
      "episode: 14964/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14965/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 7, 4)\n",
      "episode: 14966/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 80.0\n",
      "START state: (0, 0, 0, 2)\n",
      "episode: 14967/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 5, 7)\n",
      "episode: 14968/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 2, 2)\n",
      "episode: 14969/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 270.0\n",
      "START state: (0, 0, 3, 8)\n",
      "episode: 14970/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14971/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 5, 6)\n",
      "episode: 14972/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 140.0\n",
      "START state: (0, 0, 2, 5)\n",
      "episode: 14973/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14974/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 7)\n",
      "episode: 14975/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 300.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14976/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 310.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14977/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 4, 2)\n",
      "episode: 14978/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 1, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 14979/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 120.0\n",
      "START state: (0, 0, 9, 5)\n",
      "episode: 14980/15000, steps: 578, e: 0.001\n",
      "accumulated_rewards_per_episode: 200.0\n",
      "START state: (0, 0, 6, 6)\n",
      "episode: 14981/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 230.0\n",
      "START state: (0, 0, 3, 5)\n",
      "episode: 14982/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 250.0\n",
      "START state: (0, 0, 9, 3)\n",
      "episode: 14983/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 1, 2)\n",
      "episode: 14984/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 180.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 14985/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 3, 1)\n",
      "episode: 14986/15000, steps: 489, e: 0.001\n",
      "accumulated_rewards_per_episode: 240.0\n",
      "START state: (0, 0, 9, 4)\n",
      "episode: 14987/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "START state: (0, 0, 2, 3)\n",
      "episode: 14988/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 6, 0)\n",
      "episode: 14989/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 7, 2)\n",
      "episode: 14990/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 7, 0)\n",
      "episode: 14991/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 320.0\n",
      "START state: (0, 0, 9, 2)\n",
      "episode: 14992/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 8, 4)\n",
      "episode: 14993/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 290.0\n",
      "START state: (0, 0, 5, 2)\n",
      "episode: 14994/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 4, 3)\n",
      "episode: 14995/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 330.0\n",
      "START state: (0, 0, 5, 8)\n",
      "episode: 14996/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 210.0\n",
      "START state: (0, 0, 4, 6)\n",
      "episode: 14997/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 340.0\n",
      "START state: (0, 0, 3, 3)\n",
      "episode: 14998/15000, steps: 52, e: 0.001\n",
      "accumulated_rewards_per_episode: 0.0\n",
      "START state: (0, 0, 6, 1)\n",
      "episode: 14999/15000, steps: 600, e: 0.001\n",
      "accumulated_rewards_per_episode: 280.0\n",
      "Finished in 34904.3457 second(s)\n"
     ]
    }
   ],
   "source": [
    " t2, epsilon, average_accumulated_rewards = csrl.train_DRQN(EPISODES=15000, num_steps=600, batch_size=32, weights_update=50, state_sequence_size=5, label_sequence_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b66d29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEKCAYAAABzHwA5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABfAklEQVR4nO2dd5wURfbAv29ZsiAgBoIoKKBgjoBpzYo5A2bxPO/0Tk89hZ+nZ+LEdOZwCJ4RUc+sGNE1oQJmAVEkSVAUBEkCC+/3R3UzPb3dM92TZ7e+n09/uru6uurt7Ey/rlev3hNVxWKxWCyWcqOi2AJYLBaLxZIJVoFZLBaLpSyxCsxisVgsZYlVYBaLxWIpS6wCs1gsFktZYhWYxWKxWMqSymILkA0VFRXatGnTYothsVgsZcXy5ctVVct+AFPWCqxp06YsW7as2GJYLBZLWSEiK4rbPxsCK1RZKkID4DRgDfCoKmujtlP2GthisVgsZcdLQFfneAhwCXARcEucRqScI3E0b95c7QjMYrFY4iEiy1W1efH651egjSoqwmygD7AUmKhKu6jtlLUJ0WKxWCxlyRqgkQjdgMWqzBKhAlgvTiNWgVksFoul0LwCPAlsAIxyynoAc+I0Upg5MJEHEJmPyNch1wWROxCZisiXiOxUELksFovFUgzOBl4GRgDXO2VtgaviNFKYOTCRvTH2zYdR3Sbgel/gL0BfYHfgdlR3T9esnQOzWCyW+BR7DiwhBxXAxqrMy+T+wozAVN8FFqaocRRGuSmqHwGtEIk8kWexWCyW8kGEViKMBH4HpjplR4pwXZx2SsWNvgPwg+d8tlOWH776CgYPhkWL8taFxWKpPyxZAiNHFluKsuI+YDGwGbDKKfsQOClOI6WiwCSgLNC2KSLniMgEEZlQU1OTWW/TpsHQofDtt5ndb7FY6h2//w4zZgRf++Mf4eST4ZNPCipSObM/8FfHdKgAqvwMbBSnkVJRYLOBTT3nHYG5QRVVdZiq7qKqu1RWZuhEucUWZj9tWmb3WyyWeseAAdC5MwS9N89xfOeWLInW1lNPwRtv5E62MmQxxmljHSJ0gnhzYaWiwF4ATnO8EXsBi1HNaFIvEp07m71VYBaLJSIvvmj2awMCHVU4T9KoPnEnnggHHZRctno1vPxy5vKVGcOBp0XYF6gQoTfwEMa0GJlCudE/jrFvdkdkNiIDETkXkXOdGqOBaZjJvPuBP+dVnubNYZNN4Pvv89qNxVLfWbMGFiwothS5IZVychVYkHKLytVXw+GHw5tvZt5GGXEDZh3Y3UBD4AHgeeD2OI0UZiGzav801xU4ryCyuHTpYkdgFkue+dvf4M47YelS895YzrgKLEiRiTOLn40CmzrV7H/+OfM2ygVVFLjN2TKmVEyIhWeLLewIzGLJMw88YParV0erP2IENGoUPM+USz7/3CgdV2lEwVVOQQosrgkxCPdeCXJpAy67DHr0yLz9YiPCflG2OG3W31BSXbrAo4/CypXQuHGxpbFYSpbnnoP58+Gcc+Lf68YZCHso+7n4YqPsli6FVq3i9bVkiRnx/fvf0LJleD3VhGI97DCYMiVeP6kUmKvkhg6FPn1g773jtxv2Wd14Y/S2SpQRvvMOGA/EBZiQUoJx6OsStcH6OwLr0sV8Y8L8Yi0WCwDHHGPcxLMh6sgkm7mk2283I7ibbkpdzzVrQmYraVI5cTz0EKxYYZaZ7rNPvHbTKbByR5XO7obxdbgTaK1Ke6A1cIdTHpn6OwLzutJ3715cWSyWOo6qMQvOmmXeHcPIRoFFVZK3x3ITCO9n9WqYPds4NbtyjxoFbduG3xuFuqrAfPwNaK/KagBVloswGLN86vqUd3qo3yMwsI4cFksBUIVBg8x74+zZwdevvTbhsZiNM0QmrFplRk1Ll6av6yqwCy80j5H58xMKDGBu4ArW6O2WAiI0EWGcCF+IMFGEq53yNiK8IcJ3zr61557BIkwVYYoIB6fpYhmwm69sV2B5HDnrrwLbZBNo2tQ6clhKnh13hN38P/UyQzXhHh7kZffVV3Dllcn1g3jhBTNCCVKC6e5NxQMPmHmrq69OX7dlS/jii8RC5F9/TVZgmVJiJsSVwH6qbA/sABwiQi9gEDBGla7AGOccEXoA/YCewCHAPSI0SNH+FcCrIowU4QYnLuKrwD/iCFl/FZiIdaW3lAWffw7jx+ev/SefhAsuyF/7kKxUvA/oBQvMNb+XYtgI7H5nhmToUDjjjORr2Tz4VznR+H7/vfa1mTNrLzp+9dVEf6rZK7AVK+CZZ8yx/+9Yu9YoyUKiiqrijkcbOptiAq8/5JQ/BBztHB8FjFJlpSrTMWt6Q1+7VHkEk3lkMtAS+Abo5ZRHpv4qMLCu9JY6x4IFcM898UYhJ50Ed9yRP5nAyOOX6bvvzHzRXXfVrj9sWGpT3N13G4eJXOFVRn6uvDI47JN7z/PPZz9qeued2u26DB4Mbdpk134miNBAhM+B+cAbqnyMJ/WJs3djF8YOyK7KJGAIcA1wnXMei7J24mjTpg3V1dWZN3DCCcaekU0bFkveqQLgnHOmMWDArJQ1Bw3alo8/3oCGDSfQtWuECR1P+0G/pRdeaAd0D70ete333/+ApUu3B9ZjwoTxLFq0jA8/3ADYlsceW0CjRjOAndfdddVV8MgjSxk+fEJSawsWbIM3hJ5XpunTNwM6M2PGTKqrpwdKs2xZA2CvpLLq6mpef70b0J7Zs+fwwgvTWbSoIZ06rQBg/vzuQHJ2p2nTvmfFik2A5gwaBPvsMx/3Wf7zzz8DG9aSL+hzca9/+WUbYDsAvv76a9q0+WVdzYcf3h1oGvg3Z0GliHg/3GGqOsxbQZU1wA4itAKeFaF2LscEkQOyA4jQErgLE32+EqgRYRQmwO/iiH8DTgqu8tyaNWumWXHnnebFcN687NqxWPLEO++4YxezpWO33Uy9jz6K3keqtkWi9V1Tk7rtH39U3WYbc/zFF+bac8+Z8yOOUB03LvnvBNVNNqnd3hFHhH8e111nyv7v/8Ll/Oij2v0sX544/tOfVDfbLLnts86qfc8NN6huvXXi/MQTE8fHHZf+M/NfHz06UfbMM8l1O3eO9x2IArBMYzxrQf8JegnoFNB2Tlk70CnO8WDQwZ76r4H2TtHeg6BPg3YDbezsnwJ9KI5c9duEaD0RLSVO3LVEfh5/3ESbeOWVzO7XCKbI6dOhshIeeyxaO66JbM0as28QMtUfNA+WylQXRdYgvPNvqmbOy0vY/JZXFu9xJnIEfT5hjB6diH6fL0TY0Bl5IUJT4ADMPNULwOlOtdMx8QtxyvuJ0FiEzkBXYFyKLg4BTlXlWzXzZt8CZzrlkanfCsymVbHUUVThgw9MCpCuXaFv3/z1NXGi2Y8alVoeP14FFnTdLZs/HzbbDCYFzJCkCgXVqxc88UT49agEKRSR5HKvknOdMeKQSun5+z/sMNhjj/h9xKQd8LYIXwLjMXNgLwFDgQNF+A440DlHlYmY4LyTMN6E56kxQYbxO66dNUFbjPdjZOq3Att8c/PtsI4cljrIb78Vpp8ogWxdAxgYhXT11ekVmNveCy+YBdC33FK7Tteu8P77yXK4fPwx9OsX/e9w5fQTNgLzlr/1VnCdqN6jqUZgQQrUP0rMNap8qcqOqmynyjaqXOOUL1Blf1W6OvuFnnuGqLKFKt1VSTfmHw68IcK5IhwqwrnAa8CwNPclUdZOHFnTuDF07BgvoqfFUgaI5H9h7Lx5xlMwlQefi/famWfCDz/An52kSRUVqUdgLmGmtSlTYM89U9+bivbtE8dRzZaTJ5u1ay4//RTc9m67Jcvy1VfBcRrjmBDrCEMwUTcGAO2d4xsxaVUiU78VGEC3bsaf12KpQ6jCwoXp68Vl4UI46igTB3vzzU3Z6NGJPlPJ4153FzK7a6/C5sBUo62BEjERNNz23Pv8LFkSHBXfDTgc9DeowqJFte8Zl2p2JwXbbRdc7pXLL2Oh14AVAjXpVB4gpsLyYxVYt25mplu13rz6WOou7ld48mQ466zka7n4iu+5p2nbVV7ePqMqMHexcJQ5sGuvNS71qRCBFi1q3+unZcv0Ee6HD08+f/DB4Lm9DTZI3U5cjj8++diV//XX8/MiUmxE6A98rspkEbphgviuAf6syjdR26nfc2BgFNiiRXUnbazFAnwT8AjI1qS4YIFRXn7iKjCXKHNg//tfcj9RFbC3vX794JNPzHHQaCoVr76avv2oBClCd44srL13343fT5lwHaybP7sF4yjyLnBPnEaiKzCR9RDpiMh6cTooebp1M/tM8ipYLDkm6EEf937v3os3IWMmwXJdE52fuHNgLnHd6MOUl7/8/vtNdAyXJ56AXXYJly2MVJ9R3M9v0SLoH5CXfliIy8I//2n2YYlA64CxaENVfhKhCbAncDkmIscOcRpJrcBEtkHkTkSmAYuBWcBiRL5H5C5Ets1I9FKia1ezt/NglhLggAMyi6unCo88kshknMop4uqrjdJYsSJeH2EP7XyNwKIqcv/D/JdfTJCdbFmTwgk8rgJz5wmjcs01po8wBZaL4MFF5mcRtgQOBcarshJoQnBEj1DCPwaRx4GRwDzgFIyPfiNnfyowB3gMkRSrP8qAzTc3qzDtCMxSJFauNF55EO6OnY6RI+G00+DTT8PruArhHsdIc/nlydcPPzx1H2EP9Cg5vFKNCKMqsDAT4vTp4f1mQy5HYGFJ39Mp/TAFlm8P0wJwLfAJJkuzm4J0f+CLOI2k0uMjUd0O1X+hOhbVX1GtcfZjUb0e1e2ARzMSv1Ro2NBE5LAKzJJjfvgBbrgh/cPmjDOgU6dwE10Ufvkl+TxMIVRXJ7wAb701+frLL6fuo9AjsKhK4p5YsybRyeUILBNSjcAKnS8t16jyIGaxdEdV3FDJH2NSskQm3AtR9cWIkrwUp8OSpFs3q8AsOeeoo+Czz4xXmRv0xcuaNSbKuTtfE/awyoQwBbbvvpm3ma0CCysTCW476kO6Mk++1JdcYlLNBBFXgWQyYkqlwMoREcRxn0eECkw0DvcY4Jewe8OI/q8XOQgzwZbsxKF6ZVD1sqJbNxgzxnxj6oBx2VI4amrMXJLfjRvMuiNIdp5YtAhaOzlsb7rJpMpwyWZi3n9vpiMav4xe0jkUxB2BefNfBcnmrz9sGOy0U+16+frJ3ntv+LVUo7Mgwj6bVP/ztWvj91PiLMbk/gKogVrR6sUpS5UIM4lo/3qRuzCmwp2BTT1bx6gdlTTdupmnUL4jZFrqHP37B0dWCOLOO01eJzf0ZqEjmEUZBQwbZmT8739rZxlyHUT8ZKrAvERRYBA8x1eMd86wzyKXlLuZMICenuPOQBff5pZFJuoIrD+wA6o/pK1Zjnhd6TfdtLiyWMqCCRPMaMC7Tikdrqlw+vREIgQv3gf2n/+cfm5n+XKjDLdJlaUppP0w3Hkw/yJoMNEugghz4tgtNB9vbbmCRhpRzW5hbvj5JCiwcBjLlsWr71IHHDWSUE0kvFRlJhizIsYx8BfXvBiHqO8uC4BFcRsvG+xaMEsMxo6FXXeFm2+Od5/7gA8bMXgfWKnMVy4DBsC225oHZBQTYpQHYqrRzB/+EFweNgLzBrLNZAQWdQRSDAUWZ3S03nrh0URSfSb3318n1nsFIkIrER7BzIP9BKwQ4RERYuWejqrAbsG4zPdGpEvSVhdo3x6aNbMKzBIJ1wT45Zfx7nNHGWFKIq7JyE1DvzIgAUWmc2CpFJg3eG1QX3GdOFzuuCO6CTGIYiiwQnDRRakV2NNPF06WPPBfTJrpHTB+FTsCjclTMF/3fdC/UiTWhFvJImKD+loi485/pHtw+h/A3nVPUeqnI8rcU9z2M11Ena79Xr1Sh3HKxlmhro5SIPXf9tRTcNxxhZMlx+wLtFPFXU4/WYQzMFHpIxPt66paEbKVv/Jy6drVjsAskXAVWFz37TgmxKBzP+7DbezY2nH2MjUhBsU6TIfXyzKMdDEI66DDQt4p889sCrC5r6yTUx6ZeD9BkU5AB2B2nXPo6NbN+PWuWgWNGhVbGksGXHQR3H574VyP0z1A/G/PfgUWdt0lqpfikUfWLgtSJm7+rVRk42zglT/uwzWsfhSlW+YPcgDWXx8WL65dnmoEVuZOHmOA1515sB8wXu2nAI+IsM6FSDW1STGqG307RN4BpgLPAN8j8i4i7dPc6W3jEESmIDIVkUEB19dH5EVEvkBkIiJnRm47F2y1lXny2ezMZcutt4Y/zJ54Aq67Lrf9eSNnXHopPPdc6vrpRmB+xXvXXanbi2s6C4qGngv8JsRly5JzbEUh7P8WN15juTFyJPz97+F5wlJl1S5zxd0bo096Ayc6+++BPphQhadiFFpKolq878XEqGqNajugNfAZcF+ku0UaAHdjAjf2APoj0sNX6zxgEqrbA1XALYgUbijUwxEnk1dQS8nTrx9ccUVu2nIfHF4FctNNcMwxMGIEzJ+fXF/VJCV0vfLCFNhf/5p8fvvttetENfHl4+28YcPgcvfzGD8ebrzReN3ttVe8tsNGza7DTCrKeQ5sxQrjzRr2/3rqqfB7y3kEpsq+Ebb90rUTVYHtCVyM6jKn92XApRhtGYXdgKmoTkN1FTAKOMr/NwEtEBGMV8pCzGrtwrDVVuaXYBVYnSYXP/ogBeZy9tlw4om16w8cmDgPc+KIMkLyKrBCm5fCFJi3r8suM/svYoVkzW40URemrjP5f5WjAhNhd995U9/5MXHai6rAfsWMnLx0J/rasA6Ad85stlPm5S5ga4wXylfABagWbpDcrBlstplVYHUcNxtwNqR72L7zTnKep7Vrk0dl2USO8D60Cj3yiKLAMqXMzWFZk2msxDLkDd+5P/zRQ3Eai+rEcSPwJiIjgJnAZsCZQFSjTNBPzf8vOxj4HNgP2AJ4A5H3UE2yAovIOcA5AI1y7WzRo0dmbliWsiEXP/pUIzCXP/4RttwyuM9sFJgb4PWppzJfd5Up6UyI2VDHYv7FZuzY+PeU4wiM2rog3XlKorrR3w+chAn5cYSz749qSD7RWszGeJm4dKS2v/+ZwDOoKqpTgenAVrVF0WGquouq7lKZ6zDUPXqYXOz1/ddUhynkaCHIOy8X/OtfcPLJtVOoZMKee0avG/Zzy8Vn+tFH2bdR38inAhNhUxHeFmGyCBNFuMApv0qEOSJ87mx9PfcMFmGqCFNEODhM7JjnKYmuAVTfAjJMt8d4oCsinTFDxn7AAF+dWZiEZu8hsjHGRBlhCjeH9OhhwhpMn554fbbUKfI9Bxalz2xkUIWZM+P3GcYHH0TvO2zuLhcKOl85veoyeTYh1gAXq/KpCC2AT0TWmf9uVSUpkJoIPTDP9Z5Ae+BNEbqpktfRQLgCE7kmUgtR0qmo1iByPvAaJnLHA6hORORc5/p9mAydDyLyFWYYeRmqOXjHjMHWW5v9pElWgZUxquHKpRjzNTvskDsZUmXpzVUfYeRzBGaJTz4VmCrzgHnO8RIRJlPbb8HLUcAoVVYC00WYinHe+9BXr7kIszzn63vOBWgWR85UI7DchmVXHQ2M9pXd5zmeCxyU0z7j4iqwyZODV4dayoK1a3MXrimsfUg/AgvLsOxN5BiXqArsvmgLXGIR9pn6065YCkOhXhxE2BwTq/BjYA/gfBFOAyZgRmm/YpSb1xAc5KgHpHeNj0OqjMyFXUicAW3atKE617+e2283CZ7sr7IMqQLg7bffobLS/+s21957733WWy+71RnffbcpsAU//vgjsElovR9CYtWMHz+BRYuWMnduN4y1JTpffz2JuXM3BDaMdV8uWLNmGdC8VvkttxRcFAuwYMFCqqtjRpROUCkiEzznwzTAp0GE9YCngQtV+U2EezHWMnX2twBnEc1RD1XeyVTgIFKZEDdHdYZzHB51XrWw81QeFi5cSFVVVW4bHTLEBG7z5oKwlBV77bUPjRsHX+vTZ0/axErYUJsPHaNIu3bhyisVO++8CzvtBI8/Hv/eHj16xF5jlStatWoeaf7NUhhatWqTzfOvRlV3SVVBhIYY5fWYKs8AqPKT5/r9wEvOaRRHvZyTygvRmzxhKvCds/dudS98u+tKX6aLLCypnUijmF2+/BKuvTb8elwnjlwS1YSYD6xzbmmRZy9EAUYAk1X5t6e8nafaMcDXzvELQD8RGovQGegKjMufhIZUJsQWnuMiJO0uEj16mEBuP/xgFjZbyo5U7x5RfvS77mrmry6/PHjNltv+f/+bmXzZPHjWri1MOvsgrAIrLfI8B7YHJh7hVyJ87pT9H9BfhB0w5sEZwB8dWSaK8CQwCePBeF6+PRAhbjR6F2NSXINq3TMouDERJ0+2CqxMyVaBhTlfRGk/CiNHws47Z3ZvuszG+cQqsPzStm28tX159kJ8n+B5rdEBZe49Q4AheRMqgKjR6B9HpI9zfCYwEZiEyMCU95UjrifixInFlcOSMdkqsHR1s31w/Pvf6euEUUwFZq3q+SWuSbpcly+IcJAII0X4QoRpzn6kCAfGbSuqaXB/jMskwEXAARgf/9ppUcqdtm1hk03g66/T17WUJHHnwETgn/+M3n4xH+TFfGjZEVh+ifv5lqMCE+FvmHiHU4FrMGEBr8b4UzzkRvyISlQTYiNUVyHSAWiD6geONBvH6axs2G47M5NvKUsyGYFdcw1cfXW0usVWYMV6cE2fXpx+6wsLF8arf2Ds8UpJ8HdgX1W+8ZU/I8LjwNtAQCKhYKKOwD5HZDAmeO/LAI4yS5FurYzZdltjQizWbLklK1IpmJoakyjSnefKJCBurhRYpguZLRaAAw4otgQZ0Zxw9/ofiRmJI6oCGwhsCzQlEYG+N/BYnM7Khu22MzERv6t7qwTqA66CWbrUrEl/5ZXEtREj4C9/MQkoIfU7SpiyKKYpzSqw8mDzzfPfR5km8nwaeFGE/UXYUIRGIrQVYX/gWeB/cRqLGo3+e1QHoHo6qj85Zf9D9bKYwpcHbn5va0YsG7xBaV0FM2UKLFli3OFdFi0y+19/NftM1lQV24RoKQ1SOSm7yuVvf8tf/2WqwM4FxmLmwX4CVjj7hzChqP4Up7Ho67tEzkLkDUQmOvuBTvbkusfWW5vAb1aBlQ3e+Rn/QuOgh75blkqBBd3Xrl1xQycVcw7MkswJJ4Rfc797+XxCZpNXrlioskqVwap0BNpgcktuoEpHpzzNIpZkojlxiNyIiTZ8GyahZSfgEkzKk0vjdFgWNG4MW21lFVgZ4X2o+xWYV0nddltyffdaUKT1IEXx449ZiZnUdlisxFRccw3Mnp0bGSzZ0bSpyaf2/vu1r7nKJZ8KrNyHD6osAhZl00ZUL8QzgJ1QTfx0RF4GPqUuKjAwZsQ4yZIsRcWrbPxzVKm85+IqsFwxfDiMDl0SGo5VXqWDCPTpE6zAXOWSzSipRQtjAk/Vf11ChMbAclVC8h7UJurHu8TZ/GV10wsRjAKbNSsxaWIpabzKxp+uJEgR3Xqr2bsKLCxVyNKlieC9uSTooWfJL5065bY9r3IaPDj4Wl1TMtkiQqewDWNOjPWJRVVgtwHPIHIgIlsjchDwFHArIl3WbXUJ15Hjq69S17OUHNXVcMklqRUYmMzG6UZg/fqZt2zX6SNXlOP8Rbnz5JOZ3xsU+ksk8d1q0aL2tXxTpnOhM4Dpzt6/fUNACpZURP0Z3Q7si8moPBF4FROd4w7qamR664lYVnh/zH/4g3G0SKfAFi1Kr8DcrDorV+ZMVMAqsCD23huGDjWBcHJFByel4kkn1VYycQhSSBUVie+WfwRfiIwFZRraax7QB2gYsMX+D0V1o6+IsEW2W5YFHTpA69ZWgZUJYSGiwq6BWQPmzpely+Cca4VjTUu1EYHLLsssGbobwtTPNdeYfUVF/P/hcccly+bHOwLzX3dfjOyLSi0mADuqssa/YaLY58WEGIxI26zuL2VEzCjMmhDLgiAl5b6hplqQnOpNWTV/b9LFyulVymTjen7DDcHlrgIJUmDff5+6Te+oPK4CcxfI+8u7d0/dZxzK1IT4RzDJMf2oslI1nk5KXVlkoe98jK9G0bIxFwQ3JqKNYlqWuA+RVCOwdArMOwJr1Sp3sj38cO7ayjePPFKYfsIU2F13pb83bKTjHVn768RRlFttFdxnuhGYn1xm0y5HE6IqP6oyP1ftpdN2DX3nO/rO67YhZKedTHLLb78ttiSWNAQpKfe9I0yBrV6d2i399dcTAVZFYPHi7GQsV5o2LUw/7v/Lrwxatqxdt63P9hOmjLzl2Yyibw8IL5tqBObG2vSXN26cvq+oyTDKdASWU9IpsHQfUd3+CF3Xo08/La4c9RxVYyL66afUdfykM9NdfTUcfrg5dh80b7+duH7iianbry8UKqa1+9D3j5SC5iePPz75PGgE5l3/p5pc56OPghXaqFHBQXKDlHiq+a0wBRaFLl2gZ8/09erzd9LFTjGmYuutoUkT+OSTYktSr/nmGxg0KHlS3U/Qj/nnn8OvQbKyEoH114f99gtuc/ny6PJmwxVXpK9TaAo1Xxf20A9SYG3apK8jkuzI41U4u+8erFx23x3OOiu4raAyd57Mq+Q7dzYROiA7J450ys8qsPQKrAkiD6/boLnvPMKAuIyprITtt7cjsCLjvv1Onhx8feXK4IfsMceYfdQf+m++Zfne+wbWvdzjkQkagYV5bWaD+z/0P7g33TT5fJNN4Mork8uCFEUqBRbUT1hZqrp+BXb22TBtmnkZStVeKqLeU44KTITrROgjkpvpp3ShpIb4zv+V5rzusdNO8OijZsbU+sQWBfeHGjYKatIk2v2pSPfQ+Pjj9G3kglJ0r+/YsXZZPuT0j8AGDoRGjWq7yM+ZU/unGPTTrKhIljPXCqyiIlmB1dTkNwLH4MHw738n1iSWowIDlgE3AF1FGAOMBl5VZUEmjaVWYKpXp7xeH9h5Z7j3XuNz27VrsaWpF5x8sll0et995jzMU3Dt2tzlXUpnJitHj69cETQnlO0D+pln4Nhjk8v8I7CePU06Ev/IOExZBcnoHYFlI3OUEViYGdPlwgsz7x/gX/8yXoxuDM1yVGCqXA9cL0Ir4GDgMOBmEaZjlNloVSKbvMKHFCLbR2ohar1yxXXksPNgBWPkSPjPfxLnYT/UZcsyi+gexO+/p75eqJUUpTgCCyJbY8QGG9Qu8y/+DfPwCyLdaCrIhJiunXQejP4RWBDePt34m9nw0EOJ43JUYC6qLFLlCVVOA9oDF2K83v8jwhwRTorSTqp/6d2IjEakPyLtk66ItEOkHyKjgTsz+xPKhB49jB3DzoMVjbARWC6VSrpQUfV5BBZEtoo2aLTiNyHGWUSeyRxYECJwxBFm1DlkSHJ5UN10CizXLyRt2xoX+0MOMQ4n+UKETUV4W4TJIkwU4QKnvI0Ib4jwnbNv7blnsAhTRZgiwsFR+1JFVRmnyj9V2RWzXCvSAzf8X6q6J3APcDIwFZEliMxFZAkm7mE/4C5U944qaFnSqJFZ0GxHYEXD+yCrqYF994X33sutUkmnwOwILJlsR2BB92fjep5LBbbeevDGG8ad3VseVLehs1LWb4LOtbnSS8+e8Mor6ed+s6QGuFiVrYFewHki9AAGAWNU6QqMcc5xrvUDegKHAPeIRE+L4kWV+arRYuum/peqvoTq4cD6wO7A8cBuQCtUj0Y1g4xGZchOO5kRWDmP2csYrwKbOdNEmz/jjMIGSKlrwVjWWy+7+/MxAvMrgTgmxLB5sS22MMd9+tSuk+3PuaIiocDyPQIrdOQWVea5c1GqLAEmAx0wiY1dQ+ZDwNHO8VHAKCcc1HRMgPfd8i1n1GC+q1GdhOpYVCejWqCljSXCzjub0OWpMiNa8oZ/BAbmAVhIpVKod5dCjcD8kSziEldO79wNBCucbObAwkZgO+8MU6YYB4pM59JS1T3+eOOlef750ds7+eTE8W4RH/GnnhpdrlwjwuYYs97HwMaqzAOj5ICNnGodAO+M9GynLK9EzcicPSKHYNKyNACGozo0oE4VJvdYQ+AXVPdJ1WSbNm2orq7OsaABdOwIN99sXIBmzcp/f/WYOXOaYgb7rPvfTpvWHNiVNWtq+PDDT4HdWLlyGe+99yXQu0iS5ofp06cDnfPez7JlvwPRbFDm/1CVVLZ2bQ1RHx/nnPM9rVvPA/ZcV/bZZxOAXZLqrV6tVFe/ww8/dAE6MXXqNKqrZ/H77xVAYqYi8ZtPyPTZZ58AyUm7xo79gPXXN1px7lxYurQBsNe6Nn76qTH+78+HH45lgw1WeUqqPH2a406dljFrVnOmTPmWKVPm8sgjMG+e2Vx++mlrYGNmzJgGdEmS++yz4bHHTFs33FDNoEHb8vHHCa+W335bRHX156xZsyfuZ5yH51yliEzwnA9T1WH+SiKsBzwNXKjKbykUfNCVyK99IjQF1qiyKm3lpB6cGbS8btBA4XuFLgqNFL5Q6OGr00phkkIn53yjdO02a9ZMC8LKlaqNG6tefHFh+qvH9OrlhtBNlH3+uTlv0kT1vPPMcc+eqtOnJ+rWle2aawrTz4YbRq+rWrusZcvo9w8dqrpoUXKZ+z8N6uuyy8zxv/5lzpcvr13HL9OECbXb+uWX5O/Wb78ltzFzZu175s5NvifoM/jLX8z+1lvDv8f9+yf+dr/c/nYPOyxZhj33NOXrrx98by4Alqmmfr6CNgR9DfQiT9kU0HbOcTvQKc7xYNDBnnqvgfZO0fbNoLs5x4eBrgBdDnpEOrm8W6FW5u4GTEV1GqqrgFEYm6mXAcAzqJohjmrOIhZnTaNGZh7so4+KLUm9ZPhws//9d7j7bnM8bVr2Mfrq87r0IDf2OMQx36pGCw/l4nW88J6nIsrasFz9v/0mziDimiKDSNV+vnEiZYwAJqvyb8+lF4DTnePTgec95f1EaCxCZ6ArMC5FFycDbsjiK4FTgCOJGRwj3r9UZFNEesW6xxDFPtoNaI1INSKfIHJaBv3kj169jCeiTeRUcILSaaxYkYgUnymlqMAKNQc2Zky83FR+J4I4Lw9BCsz72Z97bvC1dA/w999P7sNPqj7D7omimDNRTs2bJ5d757787fnP/XEfC8QewKnAfiJ87mx9gaHAgSJ8BxzonKPKROBJYBLwKnCemiSVYTRTZbkIGwBdVHlalTeBzeIIGe0nLNIJkQ+Ab4A3nbLjERkesZ8o9tFKjBH7MMwK7SsQ6VZbFDlHRCaIyISaQoXJBrPo4vffbYbmPON9qPztb8nzCn6ynY4s5Ncnn8TNU7bpptC+vZmLAdh///T39PK9tjb0J1pKQfv2qUdggwcnX4u6DmyPPRLHQYonnWIIwv9+esstJgJGEFFGSGEK7N13jV9YlHtzmUMsKqq8r4qosp0qOzjbaFUWqLK/Kl2d/ULPPUNU2UKV7qq8kqaLb0U4GTgfeANAhLbAijhyRn0H/Q/wMtACcP/Fb2A0cBRmA96QnB2BuQF1XkV1Gaq/AO8C2/sbUtVhqrqLqu5SWVk4H5R1v2BrRiwYt90GkyaFXz/hhIKJkhPc1C2pyGQElu0b+g47wD33BF+78Uaz945e3nzTBNRNxQ03wFtvwfPPw2mnpR4N+c2JRx5p9n37mn2UzySTEVgQ/peaiy4KV7BRcCO8NGuWXN64cSLgbzpFWKh8bAXmz8B5wL6Am4PhYOD1OI1EVWC7AUNRXYs7clJdjFkfFoXxQFdEOiPSCLPg7QVfneeBvRCpRKQZxhUtJP54EejUyfxqCxXV1QIUdx4gCmHu03722SfxYE5FIUyIYWGRgmjXrvY9+++fXs4NNjALzo880tRNNR/lv7bbbub/7kZxi/KZtG9fuyydAgtKlBlUFkaUOTBXcR12WPR2w9qqS6gyXpU+qlSp8r1T9pgqsRYMRB3C/ARsCSRSE4v0AKIZcVRrEDkfeA3jRv8AqhMROde5fh+qkxF5FfgSWItxtY+Ql7RAiBgzoh2B5RX/Q6HUQzhFnUcbNgwmTsyvLJmiGv53xHGkSEUqE2KctsPMxiLm/fLHHxNl6Zw4Wrc2c4GuCXXqVNh44+iyRHm52nhj+O677IJO1xUFJsJ+6WuBKm9FbTOqArsZeAmR64FKRPoD/4czgRcJE7VjtK/sPt/5TcBNkdssNL16GZvIggXZu3FZIlHqI7CoCizVQ8i7KNtfb++9zXxJLokzAttpp+B7pk7Nrs9UI7BU9/pzg3k55xy45prwPoP+bm8CUzdqR67Zcst49euKwgpghO+8A8aitwDYAOMrMRt34VwEokbieAC4FDgB4014GnAFqo9F7ahO4EbPHJfKO9SSS0p9BOaOJHr71lOffnqyv0+qh1KqOIz5eJgFPdiDlMjKlYnU9v57sn2x8I7A4iiwVHWuuioRTzHovlx9lvlWMDvsUJh+Co0qnd0NuB8TCL61Ku2B1sAdTnlkonoh7o7qc6j2RbUnqoei+hwieY91VVLssov5tVkzYsEotRGY35vMnWD3OzU8+KAxUbmIhP8tDRoYj8Bddgl/6N52Gxx9dPD9qT6jxp6c6aNGhdcLUiKNGtWWIyrplEecEVjU/rzBdYP6zBVx2o3y/fXW+egjuOmm6PeWMX8DBqmyHMDZDwYuitNI1K/OGyHlr8bprOxp0QK22QY+/LDYktRZSn0OrEeP5PM+fUyW3OEBC0q8D7p0D6P774fx48Pb6NkzswWv3jxnu+5aW64gWcF4GnrJVsk0agQjPAakOHNgmcYwzPcIJlcLmV1uuskYebwvDpm2VQYso3aw312BkLzrwaT+WopUINIAEETEOXe3rpiQ+/WLPn3Ma1JdC09eouR6/idb/C7fDRqY9WpBruxxFFjQPf7zbB9kqWTwjyD9a8Ny8RA966zEcaZzYHHq5Guher4UylZbBZfX0ZHYFcCrIowU4QYRRmIGRP+I00i6f3ENsApo5hyv9myTMPnC6hd77QVLlhRndWE95KYSc+nxLz1M9ZD0K7BU4ZOikKsHctAD+KCD4t+TDXHmwKJQaiMwN+K8f240DnV05OXyGGYENhloiQmS0UuVR+I0ku6r0xnYgoRniLt1BlqielU8mesAe5lo1iU3NKgjlPrbpl+BpXrI+BXY4YfDZZelbj+svTBHi1zgfubuVzus/1yS6xFYNvc9/LBxp4/bbqrv6iGHmOtxPRD9bUDyPGZdQEyiy2XANFWuVeVPqlyjSoqwBcGkS2g5E9UZqG7mHLvbLFRjhfyoM2y6qVnU8d57xZakTvHJJ2ZgW+pko8AaNICh0ReeAIkFus2bJ7fnNcdFVfr+enEiVURVBu3bm7VP6RbuevvKhXLMZgR26qnJ7vSlwoMPwvff147iUe44MRK/xbjOZ0X0WEwiRwL7AG3xxjZULa2gu4Vgr73g1VeDo5RaYrN8ufHAO/jg0v84/WbAIOUx2Ykfk8nfsvfeyef33AMHHmiWIHrb22orE5Hfm/Y+HekWJaeSN0xJtmiR/OKxzTbw2mvpZakLJsRcEfbZNm4c7/9bZjwGvCTC7RgL37pPIc5C5qhu9P/ExEOswKwFW4CJW7Uosrh1ib32gp9/hm+/TV/XkhZ37c7YscGeeKVElHksdzI+EyeOXr1M7lSXFi1MLEEwysHFdRuPQzoFlokiOeec+Pf4+4rab6rsxYVUYOXWbonyJ8zar6uA4ZhFziOc48hE/cqeBRyI6t+AVc7+CGDzOJ3VGdzXZDsPlhPcB2s5mhBTkYkCg+T1Y14GDcq+be+9/vtSBQXO9dxk3BHY55/D6ynCvBbj4V/q87WljHdRs2+LNeaMqsBaeeISrkKkIarjMCbF+ke3brDRRnYeLEfk+0HQwZ95LgsyVWBxOC3EKJ9p/MB0TgBuW8NqJZRPEPV/dPDB0epVVBhT6yOPRPtbtt8+Eb09iEIqsChOHJbCEPXn+D0iPVGdiMmi+SdEfgV+zZ9oJYwI7LmnVWA5It8PgjhK54ADai/i9RJlDswl01FSFHlFElFAevRInRttwQKzIHyuP4GRj7CRH0ST/8cfzXtdFCoqjKl1q61y8/+PqsDOOsvMKRaiL0s4IrTEmA9r+VWo0ilqO1FHYP8g4TEyCPgrJujuxVE7qnPstRfMmAE//JC2qiU1+V4THkeBBaXl8HLiidHbClNg3kzCmSJivP1eey11iCgwHowtWgTL5ZctLt57N944+sM9Vwu04947YgT065d5f17sCCwr7gF2Aq4B2gB/wWQ3uTVOI1GD+Y5G9V3neByqW6K6CbVzetUf9nGsp9XVRRWjLpBvBRYne3C6+Ri/u3UmDzFvJuFMcR/cBx0UbFpzA8J6SedGn4qwv7NUQn0Vw4SYK+qpIjwIOE6V54E1zv4kiJcPLDMHVpHGiPwFmJbR/XWB7bc3KVVS2ZsskSilEVg6L8NM58ByvRg13UP0vfeMgSDuvU2aBJeH5ckqlYdvXXDiqGemyQpgsXO8VIRWwDxM3snIpP45inTHuDXuAHyHSaPSHRP2fg712YRYUWFex8eMsevBMmTlSvNgz/dbfJwRWLp/Y5xwUN62unaNfl8U0o0U11vPbF6iPHAnTQpOvOkPMOtSKiFBy3kEVk/5AjP/NQZ4D7gbWIo3aXIE0o3A7gCmAicCE4HngauB01HdDdUnYwpdtzjgAJgzB6ZMKbYkZcerr5q3/XHjSsuEmI5CeCFm23ZY/L3u3eG88+C558Lv7dzZhLyKSn0zIXrnEi1Z8QdghnP8V2AF0AozSIpMup/jzsCRqK5E5F3gN2AzVGfHErWu4obsHjMmPJS0JZBXXjH7sWPjPTAzYXmsBA2pydQLMVfstZcxD4a1/emn4dEbKirgrrsS526IIn+OszjUJwU2aZKZNbj9dnNeKubTckQ1Mf2kys/A2Zm0k24E1gjVlU4vy4DFVnl56NIFNtssXiRQC6++Ck88YY4rKvI/AvOnBsmGTE2IucLNRxbW9o47pl4v5eXss+Ff/4LBg6PV//hjePbZ5LJSUWCFYOutzTKB+uDEIcIDIswX4WtP2VUizBHhc2fr67k2WISpIkwRIe1qQBE+E+FWEY4WIcUS+tSkG4E1RuQaz3lT3zmoXplp52WPiDEjPv20eQpnmy+jnnDooYnjior8PwRvuinx1pwpTZvCmWfGe3i5dYPmq7bYwgRqjUtYOKgRI2DChHhtNWwYXXlBcCinjh3j9ZkvytmJ4+ab4bffEo7NJcKDwF3Aw77yW1W52VsgQg+gH9ATaA+8KUI3J2hvGJcAewMXAiNFmAq8A7yjyv+iCpluBDYS2NSzjfKdl8jXt4jsvz8sWmRsN5bY5HIE5o7q/OTCiePss+Huu83x228bJ1SIZkIManPs2OgyeQlTYGedZQL/Fpo4CjCfFFKBuSZvN91JtvTsCR98kJ0pN9eo8i6wMGL1o4BRqqxUZTrGbyJF5EpQZYwq/1SlCqNLXsTMf4X8ioNJPQJTPTNOY/USd2HQmDGJnO2WyORSgeUzcrd3cF1VFe5W7iXVCMyNWBHXOzFdQN5CE8epBUx+rKlTcy9HIT+PXr1K0+xXIM4X4TRgAnCxKr8CHYCPPHVmO2WhiHAIxgtxH4wC+xAYjBmFRSbm16+0aNOmDdWlsJD47rvNL7kUZCkLqtYdffvtFCoqlgC7ZN3qe+99jlnxkYz5jlTVKg9i3ry5GCtIMnPm/EB1dcLmd8QRLfnoo21ZvfpjqqtrnNIqT3+walUFxkqylurq2oGfR41qzHrr1VBdHaTBk9tymTu3O9COb7+dQnX1vEh/U36oAox8hxzSncpKpbo6vQf0bbc1YOnSSqqrV4a2F499AOHdd9+hUSNXq2TaVr2iUkS8RudhqpoiGiYA9wLXYlKfXAvcggn0HvT6kE7Fjwa+B64HHlalJk39YFS1bLdmzZppSXDZZaqVlaqLFhVbkrLAvL+abdgw1XHjkssy3V54weybNEmUfftt7T5TbWefHVx+ySXR/y6X5csT8mT6Gfk580xTPnx4/DZzSZh8hW5PxNy3cmX+ZKuLAMs0zfMVdHPQr9NdAx0MOthz7TXQ3mna3tO57xXQuaCvg14Oulc6ubxbnpKU1zMOOwxqamxUjgzIpRPHQQfB6afDM8+Y8wsuSJjoVq6EIUPStxEmSya5stx7+vSJf28YWiImxFdeifZ55ptifw71CRHaeU6PgXUeii8A/URoLEJnoCswLlVbqryvyvWqHIoxm4wHLgWq48hU1ibEkqF3b+O7PHo0HHdcsaUpK3I5B9a4sUnDDiZgrndKslGjaE6iZ50FDzxQuzwTB9PGjY1nYLdu8e8N4+yzzd/oj8lYaA45JHdODACbbAInn5z5/VaR5RYRHsfYYtuKMBv4J1Alwg4Y8+AM4I8AqkwU4UlgElADnKepPRAR4Rin/X2AbsAnGK/HPM2BmbBS2wPJAWpUA37u9YzKSpMIafRo8wqfixzp9QSR/KwDCwqY645e0t136aVw443J5Zn+S3feObP7wthjj7rpQDAvy+k8q8Byiyr9A4pHpKg/BIgzJr8Ao6wuAj5UZUU8CQ3RFJjI/wFXYuJXeeMaKGAVGEDfvvDkkyZ17E47FVuaskGk9uLYYhNkRsxlUkxL7rEKrLxQjehVlYaoI7ALgd1Q/TIXndZJ3NW5o0dbBRaDBx8sPedNrwLbcku4/no49tj09334IbRrl75eFGbONGE2Lalp2hSWLaubo9K6jAiNMYOi/sAGqqwvwkFAN1XuSn13gqiGkRXAN/HFrEdstJGZdBk9utiSlBXpsgRHJcoDLOpDzqvAKirg+OOjmRB79TKRxXJBp07hQXktCT7+2ITDirsezVJ0bgO2AU4m4XI/EfhTnEaiKrArgDsRaYdIRdJmSdC3L3z0Efz8c7ElKUnef7+2qScXpp+ZM+PfU1UVfq0+xfcrd3r2LJ1oIJZYHA0MUOVDYC2AKnNIswDaT1QF9CAm/P1sYLWz1Tj7aIgcgsgURKYiMihFvV0RWYPI8ZHbLhWOPNK85r/4YrElKUnuuKN2WS5MP506xas/aJAJBxWGVWAWS95ZhW8KS4QNgQVxGomqwDo7WxfP5p6nR6QBJmHZoUAPoD8iPULq3QC8FlGu0mLHHY0NyV2IZElipT/4AuEeiK1b577/TEyIlvIj19mvLXnhKeAhZ92Yu8bsLky83chEU2CqM1GdCfyA0Zw/eMqisBswFdVpqK5yhDwqoN5fgKeB+RHbLS1EzGz/G2+Y8NKWJFatql0WpizyGdcwndmylIKqWuIzYwZ8/XXaapbi8n+YtWRfYRJZfgfMwyRMjkw0BSbSEpGHgd+BOcAKRB5CJGLmITpglJ9L7WCPIh0wq7vvi9hmaXLsseZJ7WZstKwjaAQ2fXpw3WJmpvnnP+GUU4rXvyU7NtnEzI1ZShdVVqlyoSrrARsDLVS5ELOoOTJRTYh3AM0xXiNNgW2BZk55FKIEe7wNuAzVNCu45RwRmSAiE2pqMov/mFd69zahyq0ZMYlVq1LPO/nJhwKLakJs3hwuvzz3/VssFhChpQg7i9AW1mVk3k6EZzAhpSIT1fn0EKALqu4i5m8RORMTTTgKszEh8106An4H6l2AUY59py3QF5EaVJ/zVlITMXkYQPPmzUtv9UeDBnDUUfDYY/D779CkSbElKgmuuCJe/XyOwKJ4PtqFsRZL7hHhMMwUUnNglQinYFI2nAoMB7aM017UEdjvwIa+srZAgFEokPFAV0Q6I9IIk73zhaQaqp1R3RzVzYH/AX/2K6+y4dhjzerKN94otiQlw3ffxatvk1tbLHWS64CLMQrsYuAhjG7ZQpW/O670kYmqwIYDbyByLiKHInIuxlMwXf4Yg2oNcL5zz2TgSVQnOu2dG0fgsmDffaFVK3jqqWJLUrYUQoHdcEP++7BYLEl0VmWYE/vwPqAhMFA1cvbnJKKaEIdgTH4DMNn+5gI3EicOoupoTBIzb1mww4bqGZHbLUUaNTJR6Z94AlasMPFu6jlx13sVYg7s0kvhssty34/FYgll3aBJlTUiLFVNiq8bi2gKzGQgewAbuDc6AwbAiBHw0ktwwgnFlqboPPdcvPrFngOzWCx5oZkI3vTkLXznqLJ31MbCFZjIqag+4hyfFVrPplMJZp99TGTXkSOtAssAvwKbORO++MIEO7FYLGXLQN95aIqWKKQagfUHHnGOTw2pY9OphNGgAfTrB3ffDYsWmTkxS2T8CqxTJ1iesaHBYCOWWyzFRZWHctleuBOHal/P8b4hW5HzwpY4AwaYBVB2TVgt0i0UzkdO0E02MfsoKU+ssrNYSp+okTg+CymfkEth6hw772wSSo0cWWxJCsbatSY1/AcfpK7XuHFq35Z8zFMNHAijRsGfYiVssFgspUrU99zai8tEhKjBfOsrIuZp/tZbMGtWsaUpCIsWGX19+OGp661alZ9RVioqKuCkk6L1ax09LJbSJ/VPWeRhJwZio3XHibJ3MAnILKk4/XRjj3rwwWJLEompU+GXXzK/f8UKs0+3cmDVqtRKIp0C+fOfzb5Ro+iypeL00238Q4ul3EjnRv99yLECH2BC4ltS0bkz7L8//Pe/8I9/FH7YEZOuXWH99c1IKhOWLTP7Zs1S1wuKTO8lSIG581Ldu5ssvPfck7uPs0zeLyyWskWEa6LUU+XKqG2mVmCqVzs9f4RqeeboKgXOPhv69zemxAMOKLY0aVm8OPl84UJ49FEzh5Qu1YirwNKNwMLygHn5+9/hppuCr7mKy5r6LJaywRsPtwlwHCbM4EygEybt1tNxGoyaD+w1RBohsi0i+yKy37rNkp6jjzYZGkdkteQh56jC9xHCMd92G1xwATzt+2qtWAHbbAPvepYhujEPN9oofd/pTIg33li7zE+2I7Bvv4Vp07Jrw2Kpa4jwgAjzRfjaU9ZGhDdE+M7Zt/ZcGyzCVBGmiHBwUJuqnOlumAwl/VXZQ5UBquyJiZEbi6heiHtitOQ7wBuYYLuvYWIkWtLRpImZYHn2WTOcKRH+/W/jJPnFF6nrzXfSi7qjK5dvvoGJE+Evf0mUzXFCcW6ZJqZ0uqzH6UZW7v3ZRuzo2tVYeS0WSxIPYrKQeBkEjFGlKzDGOUeEHhjl09O55x4R0v0yDwWe85U9D/StXTWcqO+vtwI3otoGWOLsrwXuidNZvWbgQJPR8eGHiy3JOt5/3+zTjUBcl3i/UmnY0OxXr06UuSnaRGDs2HB9ne06K1dxbb11du2E4c7hbbFFftq3WEoZVd6FWgF2j4J1C5EfAo72lI9SZaUq04GpGHNgKqYC5/nK/kz0FF1A9GC+3YDbfWVDgenAzXE6zCVt2rShurq6WN3H5957zRO+RGResKAnsCFffjmR1q1/dkqrANZ9rmvXwtdfm7Jvv51CdfW8dffPmtUU2J3ffltOdfU4p04noAvz5s1hjz06sOWWS7j//k/Wtevyyy8LWLt2ffxfwWOOmc2zz3bk55/nU109KUmeWbOaAbuxfPlyJkwYx9Chbdhqq9+ors5PYtMhQzZgu+0WUV0dYcLOYikvKiV5He8wJ9diKjZWZR6AKvNEcCcKOgAfeerNdspScTbwrAiXAnOc+jXAsVH/AIiuwBYDLYFFwDxEegALgPXidJZrFi5cSFVVVTFFiMe8eSY6x+jRcOihxZZmXWSKrbfuSVVV8qjI/Vy9o6uttupOVVX3defu/FllZbN19d35sHbtzPd36tQWgf+j1q03CDT/nXhiR559FjbeeCOqqhITaVVVVUyebI6bNTP95ftfX05fLYslJjWqukuO2goy+Ke0sajymQhdgV6YDCfzgA9VWZ3qPj9RTYjPkLBNjgDeBj7ButHH47jjjNa4885iSwIkHCDc+aTVAV8d71xV2LxUkAnxyy8TZUEu+WvXZu5BaD0PLZai8JMI7QCcvTM7zmySPQw7YlJuRcYxWTYSIY2fczJRvRAvRHWkc3wLxv3xD8A5cTqr9zRqBOeeC6+8Ej9FcR7wK7CgtVled3e/4ghSfK4C+/DDRFn//rXbDZoDGz3axiC0WEqYF4DTnePTMU4Xbnk/ERqL0BnoCoxL1ZAI2wLfAveTiEi/DzGDw6dXYCINEPkekcbrylTfR/UVVNP4kllq8cc/Gu+Hu+8utiTrFJirpOKOwFIpMO89QVN+fi/Ejz4yVlVXgQWNslq0MPvtt699zWKx5A4RHgc+BLqLMFuEgRi/hwNF+A440DlHlYnAk8Ak4FXgPFXSTRzfC1ypylawzmz4DrBnHDnTKzDVNcAazMIzS7ZssgmceKJZE/brr0UVxT8C8ysfCB6B/fqr8S507/OaCIMUWFC7qnDddYnz3XevfZ+fjh3hvffgAZvAx2LJK6r0V6WdKg1V6ajKCFUWqLK/Kl2d/UJP/SGqbKFKd1VeidBFT+BR93anjWVArPT1UefAbgOeRGQfRLZApMu6zRKfv/8dli41XolFxK/AgtZmBZW1aQMbbBB8LY4CO//84PJU7Lln+jBVFoul5JkB7OwtEGE3jHt9ZKIqsLswQ8a3ge+cTqY6x5a4bL+9sZfddlsi+m0BePRRo1DcgZ9fgQUpD+8IbODA5JGRV4EdfzxsvnmwAgvC7euss8zH4C+3jhoWS53mCuBlEa7GOG8MxjgF/iNOI1GdOCpCtizjINRjBg2Cn382QX4LxK23mr3r/u66sccdgQVde/ppmDkTfvopuU5YqCf33hEjTJgqP1aBWSx1F1VewkTj2BAz97UZcKwqr8dpJ+o6MINIJ8yCs9mo/hDrXksye+0FvXubaLXnnAOV8f4VmeB24Y6S/E4cQcoqVdDdoPr+5NNhCiyqt+FTTwU7l1gslvJFhBNUeQoTfcNbfrwq/4vaTtRYiO0QeQdjNnwG+B6RdxFpH0NmixcRMwqbMQMee6wgXYYpsKAR2GGH1S7zs9NO6ftMZ0IMK3fvO/74YDd8i8VS1oRFNk8XDSSJqHNg9wJfAK1RbQe0Bj4D7ovTmcXHEUfAjjvC1VenT5CVA1yToTuqSjUHNno0/PZb+qC76QhTYGHt2jkwi6XuIkIXEboAFSJ0ds+d7QDg9zjtRbVb7Qm0Q9UYc1SXIeLGsLJkiojxJT/sMOMbfu65ee3OVWCPPAIbb2wSQkL4HNjvv0fL25WKuCMwi8VSp5mKcZsXagfu/RG4Kk5jUUdgvwI9fGXdMbERLdlw6KHQpw9ce23ePRJdE+KIEclR3FPNgWU7AvOnYHEJS4zZqJHZW1d5i6XuoUqFKg2A95xj79ZeNT8mxBuBNxEZisifEBmKyQt2Y5r7LOkQgSFDYO5cuC+/FtkwP5EwN3qR7EdgLn19WX4efTS43vHHw5VX1k5mabFY6g6q7JOLdqKZEFXvR+R7YACwHSZQY39U38qFEPWeqirYf3/417/Mwqj1189LN2HJH10FFpS7K9sRmItXEW67bSISvp/KSjMlaLFY6i4iVGI8EPcB2uKJaK/K3lHbiZ6QXfUtVM9Gta+zt8orl9x0EyxYYEyJeeD66+Hll4OvuUrKP0rK5QjMO7oLc623WCz1hluBPwLvYiJyPA1sBMTSK+EjMJFrIrWgemWcDi0h7LijGX3dcYcJ+Nu1a+wmPvzQODPuEzA4/7//C79vzRr4z3+M/vSimpwWJRuqquB1Z4miVWAWS73nWKC3KrNEuFqV20V4DfgPMRw5UpkQN01xzcX6kuWSIUPgySfh4ovhhRdi396nj9nH9fAbOzZ4dPbLL7lbg+U6Z4BVYBaLhWaAGwxjhQjNVPlGhB3jNBKuwFTPzEK42ogcAtwONACGozrUd/1k4DLnbCnwJ1S/yKkMpc7GG8M//gGXXWaGKwcdVJBu54aknuvh9zvNAq/SCpuLs1gs9YbJwK6YvGETgKtE+I2YS7OivwuLdEXkckTudvbRbVwiDYC7MbGvegD9EfE/HqcD+6C6HXAtMVdk1xkuuAC23BLOO69ggX6DnDdyjVeB2RGYxVLvuQBwQ39fBOwEHEHMJMlRQ0kNwETe2A5YBmwLfOqUR8GEyVedhuoqYBRwVFIN1bGougmyPsKkpa5/NG5sJqSmTs2ZQ4d/bstPIWINNmiQcOO3Csxiqd+oMl6VT53j71Q5QJXdVXkvTjtRI3FcB/RF9d11JSJ7AY8AIyPc34GEvRNgNrB7SF2AgRCcFE1EzsHR0o28Eyt1if32gzPPNIuhTjop6xTEbdumvp4rV/lUVFTAW2/B3ntbE6LFUh8RYb8o9VSjeyJGVWAtMOmlvXwEhMRTqEVQQKFgVwORfTEKLDC1tKoOwzEvNm/evO46kdx8s/GsOPts+Oijsn/qV1SY4L89e5oVAxaLpd4RFsDXiwKREyVHNeb8G/gXIk0AEGkKDHHKozCbZK/GjpjF0MmIbAcMB45CNY3hq47Tpo1xqZ8wAYYOTV/fQ00NnHYaTJoUrX6uR2BBET8aNDDho77+GnZPNfa2WCx1ElU6R9giKy+IrsD+DFwI/IbIT8Bi4G/AnxCZtW4LZzzQFZHOiDQC+gHJfuIm19gzwKmofhvnj6iznHgi9OsHV10F48enrLp0aeL4iy9MwN5TTonWTa4WK7sEDRbtvJfFYsk1UU2IER+FIajWIHI+8BrGjf4BVCcicq5z/T7gSmAD4B4nhHkNqrtk1W+5IwL33gsffGC00aefhkbBveiixLGbmSXqFGE6J4+4BCkrq8AsFouLCD8QMo2kSqeo7USNhfhOiBQN16VYSd/GaGC0r+w+z/HZwNmR2qpPtGplhlP77gsXXgj33x9YzVscV4HlmqARWJlP4Vks9Q4RZgBLgDVAjSq7iNAGeALYHJgBnKjKr2FtpMA/KGqHca0fFaeRqG70byDSzle2HWYBmiXf7LMPDB4Mw4fDgw/y/ffwXgpn03wosDhJo+1oy2KpM+yryg6quNawQcAYVboCY5zz2Kjyjm8bBRwDxAqgEfVR8ynwBSInIiKIDAKqMZmaLYXg6qtNxPpzz2XLLY07ehjLl5t9LhVYq1bR6wYpMJvA0mKpExwFPOQcPwQcncO2VwKd49wQ1YR4GSIvAQ9jcoDNBXZDdWpcCS0ZUlkJjz/O+G3OhPmmaOVKs+7Zj+vQkUsF1qRJ6uvPPAPHHmuOrQKzWOoECrwuggL/cZJNbqzKPABV5omwUSYNi+APFt8M6EvI+t8wojpxgNGMLYFpmPVfaR5p+adNmzZUV1cXW4yC8umFV4ATWf72279gt91c83PVujqffDIF6M7ixfOprp6UdC1TJk36FBPtJZhFi8ZjQpvBmjWrgGTt+c03k6mu/ilrOSwWS06oFBHvFNAwZ42tlz1UmesoqTdE+CaH/fuDxS/DLMt6JE4j0RSYyFOY8FEHozoBkfOAdxG5HtWiLUtduHAhVVVVxeq+KHhHMpddtj26Zm2tIU+HDt2d/UZUVWX0glSL3r3DlRfALrvsuu64SZNGLF6cfH277bamqmrrnMhisViypkbTeHmrmrW6qswX4VlMSMCfRGjnjL7asc4eFA/VeHNdYUSdA/sZ2BHVCU7vdwO9gONzIYQlNZ9+CmPGmOOaGt/Fyy+vVd9rQnzwwdzIkM6L0KtYg+o2bJgbOSwWS/4RobkILdxj4CDga8z63dOdaqcDz2fRx+YiHCnCAO8Wp42oc2B/Dij7FpE+cTqzZMbOO5v9zJnw88++i0OH8tHy7YBE4q4lS8x+2TITUrEQbLJJ4lgCAodZBWaxlBUbA886v+VKYKQqr4owHnhShIHALOCETBoXYTBm7e9EwJt2Q4kWX3edYKl6uQPVv3rOB6LqjWf1JHBc1M4s2bHZZgGFxx1H7zuSs076zXe5wF1T/ac/mfM99jDrq12aNk0cN2tW+/6g8FIWi6U0UWUaUCuKuCoLgP1z0MXFwM6qRAx4F0w6E+IZvnP/fNeB2XRuyR4d+Xitsl8zWVboISxWoXeqbbfdwq8FjbbsCMxisXhYgFkInRXpFJjfGBQUVd5SRN6orq0ZPnTyBrgLmuPQvHlilOVFJFlJNW5slqW5eK9dfLHZD/BYs7PMCGOxWOoWFwLDRNhFhE7eLU4j6RSYf/WOXc1TYswJSMA914nz7y5ojkNlZYCjiIM/q/KbbyZGVhUV0L69OT7rLOPU8dhjZq+aPEdmsVjqPY0wjiHjMCMxd5sep5F0MxOVTn4uCTm3Ee6KzMgU053V1UrcQXPDhtEVGCS8D0Vg8mRYsaL2fRaLxeLjHsyK1lEkO3HEIp0Cmw884Dlf4DvPaA2AJTVr1pgoG0HOEH7efDP82qpV8S2+DRrEU2AXXAC33GLua9nSbBaLxZKGSuC/qmSVzCm1CVF1c1Q7p9wsOefMM0OzpuSMm28OLo9qQnRd5W+6ydS30eYtFksMbgYGiWTnV2HjhueJU0+FP/whdZ1ff4VZnjSgixZBhw4mewrAJ5/kR7Y2bYyjxRZbwPXXJ18LU2BduiQrKVeBiVjlZbFYYvNX4CpgqQizvFucRuzqnDzx6KNmH5K+C4Du3c3CZHce6auvEg4YALvskjptShxO5lEec1LwTHAioE11QjEPHpyoFzQHtu22sN56ySMw12HDYrFYMiC7JMkOVoEVETeqxsyZ8MMPwXNeAZGiMmLtDjvB5+a486SXofNhgfWC5sBcBetVYP365UYui8VS/1AlOElyTKwJMUveesuY0dxYhZmw+eaw117B67befTezNo86Kvl81RY9EieHHw79+8P82j44lZW1TYKrnZzb7gLnF1+EFi0yk8tisVhEuCZsi9OOVWBZ4i7mPfhgs58/3zg2ZMI7Md5Jwrz9+jjRKddfP7l85Uqzb9xYTXLMZ56BrbeGu+9Oqnfwwcaz0BtD0b23UyczGjv88OhyWiwWSwCb+rZdgUuALeI0YhVYjlizxri09+4Nl15a+/qsWfDddzB2rFlgvGxZ7Treuah0DB0aXH6CE1rT78W4ciX89BP8+KPAlVfC55/DdtvB+eevqzP1O+WGG0xcwxtvTL7XYrFYcoUqZ/q2Q4FjgRAf6GCsAovJjz8aD8Nly+D335OvHXggTJuWXPbXv5rIFJttBt26mSC455yTfZT4PfYw+4ED4bXXzPEpp8COO5rjk09Orr94MWy0EbRq5RRsvbWxfz73HFs2nMF6LGGLgVVUvjMGVJPMiP6/02KxWPLA68DRcW6wThwxufZa42G4xx7BMQP93Hln7bJPPsk+YsV225k5MzeUU02NcbIQMQqncePk+gsWBDQiAkcdxZQlq9H/DIOh38EBB0Dv3lT87SpMpBczqrRYLJZcIUIXX1EzYADwQ5x27AgsJk2amL2bNDIT5swxnofZ4o3w3qBBYm2Wq7zmzYNJk4zM16SYGq1o3JAGfz3PDB/vuQfmzEFOTOQqfWL4kuyFtVgslgRTge+c/VTgI2AvEskyIyGq5Ruft3nz5rosaDIpT/zyC2y4YcG6C2ToUNhvP9h11zx2sno1i+5/itbnDaAli1m8Xkc47TRjC91pp+CMlRaLpWwQkeWqmud4P/nHjsBicNFFxZYAjj8+z8oLoGFDWp47gBNOgJfumwPHHgsjRpiV1T16wJAhMD1W0GiLxWJZhwg7iLCpr2xTkdpJNFNhFVhEpkwxDhzFws14HCXAby6oqIAnn4S9/tgDHnrI2COHDTOeIP/4h4kttfPOxiX/008Tq50tFoslPY8C/mSGjYBH4jRiTYhpePRRuOyy5BBPmfLII8aD0YubL8uNcnHAAbUjzNfUQNu2JlbiggUmlmFRmTkTRo2CF14w2TNVoWNH6NsX9t0XqqpsAjCLpYQptglRhN9UqbWaNaw8tB2rwIKZNcu4vmfL0KEwaJA5XrUKZsww7vQu3nxaYNzz3TVcI0eaVCUTJpjoF1deCePHm2gZJcP8+TB6tFFmY8bAb7+Z8q23Nopsjz2MzXPLLZNjUVkslqJRAgpsEnCKKp96ynYCRqqyVeR2rAJLsHYt/OUvMGCACXR7xhnp76mpga5dk6eEvBHd166Fc8811jdVo8A6O0loJk40U0oA339v+jz4YONmv3ZtAea6ck1NDXz2GVRXw9tvm0jErrtmq1ZmDm3XXY0jSM+eRqk19FsRLBZLvikBBfYH4ErgRuB7TASOS4AhqgyL3I5VYAmGD0+fAsXL6tVGWXXubBTTpEnGTHjsscZpb/Jko7TWrjWROho2NAF8N9rIBMN9/PGciV6a1NSYD2HcODN0HD8evvwyod0bNjQh+Xv2NFvXrmZurUsX2GAD6+1oseSJYiswIwMnAAMxoaR+AIar8r9YbRRMgYkcAtwONACGozrUd12c632B5cAZqH7qb8ZLpgrsuefgmGPM8eOPm5HPFVfEbmad+a9XL/j4YzM11KmTKVu40Dh99OhR+75PPjHlrmNGvWLFCqPUJk5M3vxejS1amDeDLl1MtOP27ZO3du1MHavkLJbYRFFgIiQ9s1UJCWBXPAqjwEQaAN8CBwKzgfFAf1Qneer0Bf6CUWC7A7ejunuqZjNVYC+/nFlA2jPOMNM4DzxglNaHH5ryuXPh+eejReawhLBsmVFi06Yl9u42Y4YJIOmneXOjyNq2NSO2sK1VK6PsWrY0+6ZNreKz1GvSKTARAp/ZqkwKuyde/9wBjFJlrKesD3CiKhdGbqdACqw3cBWqBzvnJmyt6vWeOv8BqlF93DmfAlShOi+s2UwV2NixiViCQVx4Iey5J/z3v8b69fPPJnbhAw+Y64sXm+gW/nBNljyhCkuWmDeFefPM3t3mzTOumd4t3XeiosIoMv+23npGuTVpUnsfVNa0qTGDerfKyujn1qnFUiQiKLDewFWqHOycDwZQ5fqwe+L1z89AB1VWecoaAz+oslHUdgrlz9aB5BhXszGjrHR1OgChCixTUuWy+vXXRMDb444z+xdegEMOSdTxpyqx5BkRM3pq2RK2iuCgtHKlUWQLF5r9r78aBRi0/fZb4nj+fBNI8vffjanT3a9Zk5+/q6LCKLSKitSbSLTrInZkWZ8YODCf0RWiPLOzQam9DrlBQFlKCqXAgn5V/qFflDqIyDnAOQAdOnSguro6tjBr1sBJJ3Xh6KPnsskmiVDrqibLiJ+WLc2ozVKmtGrlCcOfIWvXms31yvGeeze3rnvsvx60ufVcUpUFXS9jRyxLFrRqZTx+M6NSRCZ4zoepqtf7L9LzOAveA64T4VJV1opQAVzllEemUApsNiSFDekI+JcGR6mD8yEPA2NCrKqqykggk4iyU0b3WiwWS5lTo6q7pLge6XmcBRcALwHzRJiJeRjPA46M00ihFNh4oCsinYE5QD9M6HwvLwDnIzIKM1RdnGr+y2KxWCx5YzzQVYRUz+yMUWW2s3B5d4xy/AEYF7edwigw1RpEzgdew9g5H0B1IiLnOtfvA0ZjPBCnYtzos0z5aLFYLJZMUKVGhKRntioTc9zHWuBDABG2BW4ATgbaR23DLmS2WCyWekaJLGTeEDOqOx3YHngfuEuVp6K2UUpR9SwWi8VShxGhIWae6wzgYIzF7XFgM+AEVebHac8uRLFYLBZLofgJ+A8wBeilSg9VroXEerA4WAVmsVgslkLxJdAK47yxqwits2nMKjCLxWKxFARVqjCR51/HRJ//UYQXgebUTnCZFqvALBaLxVIwVJmpyrWqdAX2x6z/Wgt8IcKNcdoqay9EEVkLrMjw9kqgJofi5AMrY/aUunxQ+jKWunxgZYxLU1UtmQGMCE2AY4DTVDk08n3lrMCyQUQmpFmJXnSsjNlT6vJB6ctY6vKBlbG+UjIa2GKxWCyWOFgFZrFYLJaypD4rsGHpqxQdK2P2lLp8UPoylrp8YGWsl9TbOTCLxWKxlDf1eQRmsVgsljKmXiowETlERKaIyFQRGVTAfjcVkbdFZLKITBSRC5zyNiLyhoh85+xbe+4Z7Mg5RUQO9pTvLCJfOdfuEMldKl4RaSAin4nISyUqXysR+Z+IfON8lr1LUMa/Of/jr0XkcRFpUmwZReQBEZkvIl97ynImk4g0FpEnnPKPRWTzHMh3k/N//lJEnhWRVsWSL0xGz7VLRERFpG0xZaxXqGq92jCpAb4HugCNgC+AHgXqux2wk3PcAvgW6AHcCAxyygcBNzjHPRz5GgOdHbkbONfGAb0xmVNfAQ7NoZwXASOBl5zzUpPvIeBs57gRJjRNyciIScc+HbPWBuBJTPDSosoI7A3sBHztKcuZTMCfgfuc437AEzmQ7yCg0jm+oZjyhcnolG+KST0yE2hbTBnr01Z0AQr+B5svzWue88HA4CLJ8jxwICawZTunrB0wJUg25wfS26nzjae8P/CfHMnUERgD7EdCgZWSfC0xykF85aUkYwdMgr42mMWrLzkP4qLLCGxOsoLImUxuHee4EvjF/3+KK5/v2jHAY8WUL0xG4H+YlCAzSCiwoslYX7b6aEJ0Hy4us52yguKYBnYEPgY2Vif7tLPfyKkWJmsH59hfngtuAy7FhHZxKSX5ugA/A/91zJzDRaR5KcmoqnOAm4FZmDA5i1X19VKS0UMuZVp3j6rWAIuBDXIo61mY0UpJySciRwJzVPUL36WSkbGuUh8VWNAcQkFdMUVkPeBp4EJV/S1V1YAyTVGerVyHA/NV9ZOot4TIkc/PuBJjwrlXVXcElmFMX2EUXEZnHukojNmoPdBcRE5JdUuILMX8rmYiUz4/08sxYZgeS9NXQeUTkWbA5cCVQZdD+ivKZ1gXqY8KbDbGXu3SEZhbqM5FpCFGeT2mqs84xT+JSDvnejtYl9QtTNbZzrG/PFv2AI4UkRnAKGA/EXm0hORz+5ytqh875//DKLRSkvEAYLqq/qyqq4FngD4lJqNLLmVad4+IVALrAwuzFVBETgcOB05Wx7ZWQvJtgXlR+cL53XQEPhWRTUpIxjpLfVRg44GuItJZRBphJkpfKETHjqfRCGCyqv7bc+kFTFptnP3znvJ+jmdSZ6ArMM4x9SwRkV5Om6d57skYVR2sqh1VdXPM5/KWqp5SKvI5Mv4I/CAi3Z2i/YFJpSQjxnTYS0SaOW3vD0wuMRldcimTt63jMd+fbEc4hwCXAUeq6nKf3EWXT1W/UtWNVHVz53czG+Oo9WOpyFinKfYkXDE2oC/GA/B74PIC9rsnxhzwJfC5s/XF2LjHAN85+zaeey535JyCxwMN2AX42rl2Fzme6AWqSDhxlJR8wA7ABOdzfA5oXYIyXg1847T/CMYTragyYlK3zwNWYx60A3MpE9AEeAqTJn4c0CUH8k3FzAm5v5f7iiVfmIy+6zNwnDiKJWN92mwkDovFYrGUJfXRhGixWCyWOoBVYBaLxWIpS6wCs1gsFktZYhWYxWKxWMoSq8AsFovFUpZYBWaxBCAirzgLaHPZ5lXOwvCo9f8oIrdFqPeMs17KYqlXVBZbAIslnzjRETYG1niKH1TV81Pdp6qH5lOudDiL7P8B9IpQfShwL/BqXoWyWEoMq8As9YEjVPXNYgsRk6MwEcvnpKuoquNEpKWI7KKqEwogm8VSElgToqVeIiJniMgHInKniCwWkzRxf8/1ahE52zneUkTecer9IiJPeOr1EZHxzrXxItLHc62zc98SEXkDaOuToZeIjBWRRSLyhYhUeS4fCrzjqdtERB4VkQVO/fEisrGnfjVwWG4+HYulPLAKzFKf2R2YhlEs/wSeEZE2AfWuBV7HhKzqCNwJJpsx8DJwByYk07+Bl0XETX8xEvjEaf9aEjHuEJEOzr3XYfKGXQI8LSIbOlW2xYQfcjkdE9h1U6evc4EVnuuTMfmoLJZ6g1VglvrAc86oxd3+4JTPB25T1dWq+gRGYQSNYlYDmwHtVfV3VX3fKT8M+E5VH1HVGlV9HBP/8AgR6QTsClyhqitV9V3gRU+bpwCjVXW0qq5V1Tcw8R37OtdbAUt8MmwAbKmqa1T1E01OxbPEucdiqTdYBWapDxytqq082/1O+RxNDgY6E5O/y8+lmDxN40Rkooic5ZS3d+7xMhOTlLA98KuqLvNdc9kMOMGrWDHBnts5138FWnjqP4LJ1jtKROaKyI1iUvO4tAAWhX0AFktdxCowS32mg5POwqUTAfm2VPVHVf2DqrYH/gjcIyJbOnU381XvBMzBRCxvLSZbtPeayw/AIz7F2lxVhzrXvwS6eWRYrapXq2oPTG6xwzFpOFy2BvwZgS2WOo1VYJb6zEbAX0WkoYicgFECo/2VROQEEXETEP6KSYmzxqnbTUQGiEiliJwE9MCkoZmJMQleLSKNRGRP4AhPs49iTI0Hi0gDx0mjytPPaGAfjwz7isi2ItIA+A1jUvQuDdgHeCXbD8RiKSesArPUB14UkaWe7Vmn/GNMksFfgCHA8aq6IOD+XYGPRWQpJuHgBao63al7OHAxsABjajxcVX9x7huAcRRZiHESedhtUFV/wLjK/x/wM2ZE9ncSv8kXga1ExDVpboLJPv0bxmHjHYwSRER2BZap6rhMPyCLpRyx+cAs9RIROQM4W1X3LLYsYYjIOUAPVb0wTb2ngRGqWmv0aLHUZexCZoulRFHVYRHrHZdvWSyWUsSaEC0Wi8VSllgTosVisVjKEjsCs1gsFktZYhWYxWKxWMoSq8AsFovFUpZYBWaxWCyWssQqMIvFYrGUJVaBWSwWi6Us+X8XwtzaTO86egAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, ax1 = plt.subplots() \n",
    "\n",
    "ax1.set_xlabel('Episode(s)', fontsize=12) \n",
    "ax1.set_ylabel('Exploration Rate (Epsilon)', fontsize=12, color = 'red') \n",
    "ax1.plot(t2, epsilon, color = 'red') \n",
    "ax1.tick_params(axis ='y', labelcolor = 'red') \n",
    "# Adding Twin Axes\n",
    "ax2 = ax1.twinx() \n",
    "ax2.set_ylabel('Accumulated Rewards / 10 Episodes', fontsize=12, color = 'blue') \n",
    "ax2.plot(t2, average_accumulated_rewards, color = 'blue') \n",
    "ax2.tick_params(axis ='y', labelcolor = 'blue') \n",
    "# Show plot\n",
    "#plt.title('The Accumulated Rewards of 10 by 10 Grid-world Simulation (label uncertainty Pl = 1.0)', fontsize=14)\n",
    "plt.grid()\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe0f664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START state: (0, 0, 4, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 3, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 3, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 3, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 2, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 2, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 1, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 9)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 8, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 8, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 8, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 3\n",
      "reward10.0\n",
      "state: (0, 0, 8, 0)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 0)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 1)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 2)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 8)\n",
      "action: 0\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 1, 1, 8)\n",
      "action: 2\n",
      "reward10.0\n",
      "state: (0, 1, 1, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 0, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 0, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 1, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 8, 4)\n",
      "episode: 0/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 4, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 3, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 4, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 3, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 2, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 2, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 1, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 2, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 2, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 2, 2)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 2)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 1)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 1)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 1)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 1)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 1\n",
      "reward10.0\n",
      "state: (0, 0, 9, 1)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 9, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 0)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 6, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 2)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 3)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 8)\n",
      "action: 0\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 1, 1, 8)\n",
      "action: 2\n",
      "reward10.0\n",
      "state: (0, 1, 1, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 1, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 1\n",
      "reward10.0\n",
      "state: (0, 0, 9, 1)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 9, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 2)\n",
      "episode: 1/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 3, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 3, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 2, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 2, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 3\n",
      "reward10.0\n",
      "state: (0, 0, 8, 0)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 0)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 2)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 8)\n",
      "action: 0\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 1, 1, 8)\n",
      "action: 2\n",
      "reward10.0\n",
      "state: (0, 1, 1, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 9)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 3)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 2)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 1\n",
      "reward10.0\n",
      "state: (0, 0, 9, 1)\n",
      "episode: 2/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 4, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 3, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 3, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 3, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 3, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 9)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 3\n",
      "reward10.0\n",
      "state: (0, 0, 8, 0)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 0)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 3)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 1, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 1, 7)\n",
      "action: 2\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 1, 1, 8)\n",
      "action: 2\n",
      "reward10.0\n",
      "state: (0, 1, 1, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 9)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 4)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 1\n",
      "reward10.0\n",
      "state: (0, 0, 9, 1)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 9, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "episode: 3/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 4, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 4, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 3, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 3, 9)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 4)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 3)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 3\n",
      "reward10.0\n",
      "state: (0, 0, 8, 0)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 2)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 3)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 8)\n",
      "action: 0\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 1, 1, 8)\n",
      "action: 2\n",
      "reward10.0\n",
      "state: (0, 1, 1, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 2, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 1\n",
      "reward10.0\n",
      "state: (0, 0, 9, 1)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 9, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 0)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "episode: 4/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 5, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 5)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 3\n",
      "reward10.0\n",
      "state: (0, 0, 8, 0)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 8, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 2)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 8)\n",
      "action: 0\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 1, 1, 8)\n",
      "action: 2\n",
      "reward10.0\n",
      "state: (0, 1, 1, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 9)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 1)\n",
      "action: 1\n",
      "reward10.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 1, 8, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 2)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 6, 1)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 1, 7, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 2)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 1, 7)\n",
      "action: 2\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 1, 1, 8)\n",
      "episode: 5/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 3, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 8)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 5, 8)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 4)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 8, 4)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 8, 3)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 8, 2)\n",
      "action: 3\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 0, 8, 1)\n",
      "action: 3\n",
      "reward10.0\n",
      "state: (0, 0, 8, 0)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 1, 8, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 7, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 6, 0)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 5, 0)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 5, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 4, 1)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 1)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 2)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 2)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 3, 4)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 4, 3)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 3, 3)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 1, 2, 3)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 1, 2, 8)\n",
      "action: 0\n",
      "dynamic event occurrenced !\n",
      "reward0.0\n",
      "state: (0, 1, 1, 8)\n",
      "action: 2\n",
      "reward10.0\n",
      "state: (0, 1, 1, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 1, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 2, 9)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 3, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 4, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 9)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 9)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 6, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 8)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 7, 5)\n",
      "episode: 6/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 4, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 5)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 2, 5, 4)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 2, 6, 4)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 5, 4)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 4, 4)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 3, 4)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 2, 4)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 1, 4)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 0, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 1, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 1, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 1, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 1, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "episode: 7/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 4, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 4, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 3, 5)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 2, 4, 5)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 2, 5, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 4, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 3, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 2, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 1, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 1, 4)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 1, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 1, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 2, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 2, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 2, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 1, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 9)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "episode: 8/10, steps: 70, e: 1.0\n",
      "START state: (0, 0, 4, 7)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 4, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 6)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 6, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 0, 5, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 0, 5, 7)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 0, 5, 6)\n",
      "action: 3\n",
      "reward0.0\n",
      "state: (0, 0, 5, 5)\n",
      "action: 1\n",
      "reward0.0\n",
      "state: (0, 2, 6, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 5, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 4, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 3, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 2, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 1, 5)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 5)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 2\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 8)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 6)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "action: 0\n",
      "reward0.0\n",
      "state: (0, 2, 0, 7)\n",
      "episode: 9/10, steps: 70, e: 1.0\n"
     ]
    }
   ],
   "source": [
    "Path = csrl.verify_DRQN(EPISODES=10, num_steps=70, state_sequence_size=7, label_sequence_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7192c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
